title,authors,abstract,published_date,pdf_link
Learning Generalizable Shape Completion with SIM(3) Equivariance,"Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu","3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.",2025-09-30,http://arxiv.org/pdf/2509.26631v1
Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods for Enhanced Facial Expression Classification,"Xinjin Li, Yu Ma, Kaisen Ye, Jinghan Cao, Minghao Zhou, Yeyang Zhou","Facial expression classification remains a challenging task due to the high dimensionality and inherent complexity of facial image data. This paper presents Hy-Facial, a hybrid feature extraction framework that integrates both deep learning and traditional image processing techniques, complemented by a systematic investigation of dimensionality reduction strategies. The proposed method fuses deep features extracted from the Visual Geometry Group 19-layer network (VGG19) with handcrafted local descriptors and the scale-invariant feature transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB) algorithms, to obtain rich and diverse image representations. To mitigate feature redundancy and reduce computational complexity, we conduct a comprehensive evaluation of dimensionality reduction techniques and feature extraction. Among these, UMAP is identified as the most effective, preserving both local and global structures of the high-dimensional feature space. The Hy-Facial pipeline integrated VGG19, SIFT, and ORB for feature extraction, followed by K-means clustering and UMAP for dimensionality reduction, resulting in a classification accuracy of 83. 3\% in the facial expression recognition (FER) dataset. These findings underscore the pivotal role of dimensionality reduction not only as a pre-processing step but as an essential component in improving feature quality and overall classification performance.",2025-09-30,http://arxiv.org/pdf/2509.26614v1
Molecular dynamics insights into the Debye process of 1-propanol,"Marceau Hénot, Jan Philipp Gabriel","The dielectric response of mono-alcohols exhibits a strong Debye peak generally attributed to the dynamics of hydrogen-bonds (HB) supramolecular structures through a mechanism that remains unclear in many aspects. In this letter, we use standard all-atom molecular dynamics simulations to investigate this phenomenon in 1-propanol, a prototypic monoalcohol, over a wide temperature range covering a significant change in dielectric permittivity. We obtained dielectric spectra showing a Debye peak in good agreement with experimental data, which we decomposed into the self and cross parts of the dipolar correlations. The latter extends over a few molecular distances and contributes increasingly to the Debye peak upon cooling. To investigate its physical origin, we analyzed the HB structures by identifying clusters from simulation snapshots. Below 300~K, the dielectric permittivity was shown to arise almost entirely from intra-cluster cross-correlations. Furthermore, by tracking the dipole decorrelation of groups of molecules initially belonging to the same cluster, we found that supramolecular structures play a key role in stabilizing cross-correlation over time scales longer than the relaxation of individual molecules.",2025-09-30,http://arxiv.org/pdf/2509.26588v1
Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark,"Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Yaïr Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng","While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced ""critical point""), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.",2025-09-30,http://arxiv.org/pdf/2509.26574v1
Secure ISAC with Fluid Antenna Systems: Joint Precoding and Port Selection,"Abdelhamid Salem, Hao Xu, Kai-Kit Wong, Chan-Byoung Chae, Yangyang Zhang","This paper presents a novel framework for enhancing physical-layer security in integrated sensing and communication (ISAC) systems by leveraging the reconfigurability of fluid antenna systems (FAS). We propose a joint precoding and port selection (JPPS) strategy that maximizes the sum secrecy rate while simultaneously ensuring reliable radar sensing. The problem is formulated using fractional programming (FP) and solved through an iterative algorithm that integrates FP transformations with successive convex approximation (SCA). To reduce computational complexity, we further develop low-complexity schemes based on zero-forcing (ZF) precoding, combined with greedy port selection and trace-inverse minimization. Simulation results demonstrate substantial improvements in both secrecy performance and sensing accuracy compared to conventional baselines, across a wide range of FAS ports, user loads, and sensing targets. These findings highlight the critical importance of FAS geometry optimization in enabling secure and efficient joint communication-sensing for next-generation wireless networks.",2025-09-30,http://arxiv.org/pdf/2509.26572v1
Electrical Readout of Spin Environments in Diamond for Quantum Sensing,"Olga Rubinas, Michael Petrov, Emilie Bourgeois, Jaroslav Hruby, Akhil Kuriakose, Ottavia Jedrkiewicz, Milos Nesladek","Nitrogen-vacancy (NV) centres in diamond are a key platform for quantum sensing and quantum information, combining long coherence times with controllable spin-spin interactions. Most of current quantum algorithms rely on optical access, which limit device integration and applicability in opaque or miniaturized settings. Here we demonstrate an all-electrical approach, photocurrent double electron-electron resonance (PC-DEER), permitting exploiting local dipolar interactions between individual NV spin qubits or ensembles and nearby paramagnetic defects with sub-confocal resolution. PC-DEER extends photocurrent NV readout from single-spin to spin-bath control and coherent manipulation, enabling characterization of bath-induced noise and effective deployment of noise-reduction protocols. We resolve the signatures of substitutional nitrogen (P1) and NVH centers with reproducible contrast by using electrical signals. Our results establish a scalable, optical-free spin readout strategy that bridges fundamental studies of spin environments with deployable quantum technologies, advancing the integration of diamond-based sensors into solid-state quantum devices.",2025-09-30,http://arxiv.org/pdf/2509.26570v1
DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis,"Firas Ben Hmida, Abderrahmen Amich, Ata Kaboudi, Birhanu Eshete","Deep neural networks (DNNs) are increasingly being deployed in high-stakes applications, from self-driving cars to biometric authentication. However, their unpredictable and unreliable behaviors in real-world settings require new approaches to characterize and ensure their reliability.   This paper introduces DeepProv, a novel and customizable system designed to capture and characterize the runtime behavior of DNNs during inference by using their underlying graph structure. Inspired by system audit provenance graphs, DeepProv models the computational information flow of a DNN's inference process through Inference Provenance Graphs (IPGs). These graphs provide a detailed structural representation of the behavior of DNN, allowing both empirical and structural analysis. DeepProv uses these insights to systematically repair DNNs for specific objectives, such as improving robustness, privacy, or fairness.   We instantiate DeepProv with adversarial robustness as the goal of model repair and conduct extensive case studies to evaluate its effectiveness. Our results demonstrate its effectiveness and scalability across diverse classification tasks, attack scenarios, and model complexities. DeepProv automatically identifies repair actions at the node and edge-level within IPGs, significantly enhancing the robustness of the model. In particular, applying DeepProv repair strategies to just a single layer of a DNN yields an average 55% improvement in adversarial accuracy. Moreover, DeepProv complements existing defenses, achieving substantial gains in adversarial robustness. Beyond robustness, we demonstrate the broader potential of DeepProv as an adaptable system to characterize DNN behavior in other critical areas, such as privacy auditing and fairness analysis.",2025-09-30,http://arxiv.org/pdf/2509.26562v1
HilbertA: Hilbert Attention for Image Generation with Diffusion Models,"Shaoyi Zheng, Wenbo Lu, Yuxuan Xia, Haomin Liu, Shengjie Wang","Designing sparse attention for diffusion transformers requires reconciling two-dimensional spatial locality with GPU efficiency, a trade-off that current methods struggle to achieve. Existing approaches enforce two-dimensional spatial locality but often incur uncoalesced memory access. We present HilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA reorders image tokens along Hilbert curves to achieve a contiguous memory layout while preserving spatial neighborhoods, and employs a sliding schedule across layers to enable long-range information propagation without repeated or uncoalesced memory access. To further enhance cross-tile communication and positional awareness, HilbertA introduces a small central shared region. Implemented in Triton, HilbertA delivers comparable image quality with significant acceleration over prior methods on Flux.1-dev, demonstrating the feasibility of hardware-aligned two-dimensional sparse attention for high-resolution image generation. HilbertA delivers attention speedups of $2.3\times$ when generating $1024\times 1024$ images, and up to $4.17\times$ at $2048\times 2048$, while achieving image quality comparable to or surpassing baselines.",2025-09-30,http://arxiv.org/pdf/2509.26538v1
Optimal Matching Strategies in Two-sided Markets: A Mean Field Approach,"Erhan Bayraktar, Dantong Chu, Bohan Li, Ho Man Tai","This paper develops a mean field game framework for dynamic two-sided matching markets, extending existing matching theory by integrating micro-macro dynamics in two-sided environments. Unlike traditional matching models focusing on static equilibrium or unilateral optimization, our framework simultaneously captures dynamic interactions and strategic behaviors of both market sides, as well as the equilibrium. We model two types of agents who meet each other via Poisson processes and make simultaneous matching decisions to maximize their respective objective functionals, and find the corresponding equilibrium. Our approach formulates the equilibrium as a fully coupled Hamilton-Jacobi-Bellman and Fokker-Planck system with nonlocal structure coupling two distinct populations. The mathematical analysis addresses significant challenges from the dual-layered coupling structure and nonlocal structure. We also provide insights into individual behaviors shaping aggregate patterns in labor markets through numerical experiments.",2025-09-30,http://arxiv.org/pdf/2509.26531v1
Explainable and Resilient ML-Based Physical-Layer Attack Detectors,"Aleksandra Knapińska, Marija Furdek","Detection of emerging attacks on network infrastructure is a critical aspect of security management. To meet the growing scale and complexity of modern threats, machine learning (ML) techniques offer valuable tools for automating the detection of malicious activities. However, as these techniques become more complex, their internal operations grow increasingly opaque. In this context, we address the need for explainable physical-layer attack detection methods. First, we analyze the inner workings of various classifiers trained to alert about physical layer intrusions, examining how the influence of different monitored parameters varies depending on the type of attack being detected. This analysis not only improves the interpretability of the models but also suggests ways to enhance their design for increased speed. In the second part, we evaluate the detectors' resilience to malicious parameter noising. The results highlight a key trade-off between model speed and resilience. This work serves as a design guideline for developing fast and robust detectors trained on available network monitoring data.",2025-09-30,http://arxiv.org/pdf/2509.26530v1
Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization,"Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su","Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.",2025-09-30,http://arxiv.org/pdf/2509.26520v1
Diversity of Cold Worlds: Predicted Near- to Mid-infrared Spectral Signatures of a Cold Brown Dwarf with Potential Auroral Heating,"Genaro Suárez, Jacqueline K. Faherty, Ben Burningham, Caroline V. Morley, Johanna M. Vos, Brianna Lacy, Melanie J. Rowland, Adam C. Schneider, Sherelyn Alejandro Merchan, Daniella C. Bardalez Gagliuffi, Thomas P. Bickle, Eileen C. Gonzales, Rocio Kiman, Austin Rothermich, Niall Whiteford","Recent JWST/NIRSpec observations have revealed strong methane emission at 3.326 microns in the $\approx$482 K brown dwarf CWISEP J193518.59$-$154620.3 (W1935). Atmospheric modeling suggests the presence of a $\approx$300 K thermal inversion in its upper atmosphere, potentially driven by auroral activity. We present an extension of the retrieved spectra of W1935 with and without inversion spanning 1--20 microns, to identify thermal inversion-sensitive spectral features and explore the origin of the object's peculiar characteristics. Our analysis indicates that atmospheric heating contributes approximately 15% to the bolometric luminosity. The model with inversion predicts an additional similar-strength methane emission feature at 7.7 microns and tentative ammonia emission features in the mid-infrared. Wavelengths beyond $\sim$2 microns are significantly influenced by the inversion, except for the 4.1--5.0 microns CO$_2$ and CO features that originate from atmospheric layers deeper than the region where the inversion occurs. W1935 appears as an outlier in Spitzer/IRAC mid-infrared color-magnitude diagrams (CMDs) based on the $m_{\rm Ch1}-m_{\rm Ch2}$ (IRAC 3.6 microns $-$ 4.5 microns) color, but exhibits average behavior in all other combinations that trace clear sequences. This anomaly is likely due to the Ch2 filter probing vertical mixing-sensitive CO$_2$ and CO features that do not correlate with temperature or spectral type. We find that the thermal inversion tends to produce bluer $m_{\rm Ch1}-m_{\rm Ch2}$ colors, so the overluminous and/or redder position of W1935 in diagrams involving this color cannot be explained by the thermal inversion. This analysis provides insights into the intriguing dispersion of cold brown dwarfs in mid-infrared CMDs and sheds light on their spectral diversity.",2025-09-30,http://arxiv.org/pdf/2509.26505v1
Topology-Optimized Dielectric Cavities for Enhanced Excitonic Light Emission from $\rm WSe_{2}$,"Owen Matthiessen, Brandon Triplett, Omer Yesilyurt, Davide Cassara, Karthik Pagadala, Morris M. Yang, Andres E. Llacsahuanga Allcca, Hamza Ather, Colton Fruhling, Abhishek Bharatbhai Solanki, Yong P. Chen, Hadiseh Alaeian, Alexander V. Kildishev, Vladimir M. Shalaev, Federico Capasso, Alexandra Boltasseva, Vahagn Mkhitaryan","Photonic inverse design and, especially, topology optimization, enable dielectric cavities with deeply sub-diffraction mode volumes and high quality factors, thus offering a powerful platform for enhanced light-matter coupling. Here, we design and fabricate arrays of CMOS-compatible silicon cavities on sapphire with extreme subwavelength transverse mode sizes of only 30-40 nm ($\rm V\sim\lambda^3/2500$). These cavities are engineered for deterministic coupling to a monolayer (or few-layer) excitonic material, producing strong near-field localization directly beneath the 2D material. Photoluminescence (PL) measurements show reproducible tenfold enhancements relative to bare silicon, consistent with numerical simulations that account for material absorption and fabrication tolerances. Furthermore, time-resolved PL measurements reveal pronounced lifetime shortening and non-exponential dynamics, indicating cavity-mediated exciton-exciton interactions. The optimized cavity geometry enhances the far-field collection efficiency and supports scalable integration with van der Waals semiconductors. Our results show that the arrays of topology-optimized dielectric cavities are a versatile, scalable platform for controlling excitonic emission and interactions, which creates new opportunities in nonlinear optics, optoelectronics, and quantum photonics.",2025-09-30,http://arxiv.org/pdf/2509.26503v1
Nondestructive characterization of laser-cooled atoms using machine learning,"G. De Sousa, M. Doris, D. D'Amato, B. Egleston, J. P. Zwolak, I. B. Spielman","We develop machine learning techniques for estimating physical properties of laser-cooled potassium-39 atoms in a magneto-optical trap using only the scattered light -- i.e., fluorescence -- that is intrinsic to the cooling process. In-situ snap-shot images of fluorescing atomic ensembles directly reveal the spatial structure of these millimeter-scale objects but contain no obvious information regarding internal properties such as the temperature. We first assembled and labeled a balanced dataset sampling $8\times10^3$ different experimental parameters that includes examples with: large and dense atomic ensembles, a complete absence of atoms, and everything in between. We describe a range of models trained to predict atom number and temperature solely from fluorescence images. These run the gamut from a poorly performing linear regression model based only on integrated fluorescence to deep neural networks that give number and temperature with fractional uncertainties of $0.1$ and $0.2$ respectively.",2025-09-30,http://arxiv.org/pdf/2509.26479v1
ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems,"Junsong Pu, Yichen Li, Zhuangbin Chen, Jinyang Liu, Zhihan Jiang, Jianjun Chen, Rui Shi, Zibin Zheng, Tieying Zhang","Reliability management in cloud service systems is challenging due to the cascading effect of failures. Error wrapping, a practice prevalent in modern microservice development, enriches errors with context at each layer of the function call stack, constructing an error chain that describes a failure from its technical origin to its business impact. However, this also presents a significant traceability problem when recovering the complete error propagation path from the final log message back to its source. Existing approaches are ineffective at addressing this problem. To fill this gap, we present ErrorPrism in this work for automated reconstruction of error propagation paths in production microservice systems. ErrorPrism first performs static analysis on service code repositories to build a function call graph and map log strings to relevant candidate functions. This significantly reduces the path search space for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an iterative backward search to accurately reconstruct the complete, multi-hop error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism achieves 97.0% accuracy in reconstructing paths for 102 real-world errors, outperforming existing static analysis and LLM-based approaches. ErrorPrism provides an effective and practical tool for root cause analysis in industrial microservice systems.",2025-09-30,http://arxiv.org/pdf/2509.26463v1
Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection and Defect Detection,"Yash Kulkarni, Raman Jha, Renu Kachhoria","Ensuring that every vehicle leaving a modern production line is built to the correct \emph{variant} specification and is free from visible defects is an increasingly complex challenge. We present the \textbf{Automated Vehicle Inspection (AVI)} platform, an end-to-end, \emph{multi-view} perception system that couples deep-learning detectors with a semantic rule engine to deliver \emph{variant-aware} quality control in real time. Eleven synchronized cameras capture a full 360{\deg} sweep of each vehicle; task-specific views are then routed to specialised modules: YOLOv8 for part detection, EfficientNet for ICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for scratch-and-dent segmentation. A view-aware fusion layer standardises evidence, while a VIN-conditioned rule engine compares detected features against the expected manifest, producing an interpretable pass/fail report in \(\approx\! 300\,\text{ms}\). On a mixed data set of Original Equipment Manufacturer(OEM) vehicle data sets of four distinct models plus public scratch/dent images, AVI achieves \textbf{ 93 \%} verification accuracy, \textbf{86 \%} defect-detection recall, and sustains \(\mathbf{3.3}\) vehicles/min, surpassing single-view or no segmentation baselines by large margins. To our knowledge, this is the first publicly reported system that unifies multi-camera feature validation with defect detection in a deployable automotive setting in industry.",2025-09-30,http://arxiv.org/pdf/2509.26454v1
Silicon pinhole strip defects and their impact on ATLAS Inner Tracker HV current measurement,"Anthony Affolder, Kirsten Affolder, Emily Duden, Vitaliy Fadeyev, Cole Helling, David Lynn, Forest Martinez-Mckinney, Peter Phillips, Luise Poley, Tate Sakaguchi, Abdullah Sayed, Stefania Stucci, Alex Wang, Marcus Wong","In preparation for the High-Luminsoity LHC (HL-LHC), the ATLAS detector will undergo major detector upgrades, including the replacement of the current Inner Detector with the new all-silicon Inner Tracker (ITk). The ITk consists of a pixel detector close to the beamline surrounded by a large-area strip detector. During detector production, the electrical properties of silicon sensors and readout electronics must be characterized through a series of quality control (QC) and quality assurance tests. These tests ensure any defect is captured at the earliest possible stage. One such defect, called a pinhole, occurs when the strip implant and the metal readout electrode are shorted through the intermediary dielectric layer. Notably, the introduction of pinholes during module assembly and pinhole effects on completed modules, especially on leakage current measurement circuitry, have never been studied. In this paper, we investigate the effect of such connections on the sensor leakage current measurements of completed modules and introduce new ways to locate pinholed strips. With minor modifications to testing procedures, such defects are shown not to impede module testing or performance.",2025-09-30,http://arxiv.org/pdf/2509.26441v1
"The Grammar of FAIR: A Granular Architecture of Semantic Units for FAIR Semantics, Inspired by Biology and Linguistics","Lars Vogt, Barend Mons","The FAIR Principles aim to make data and knowledge Findable, Accessible, Interoperable, and Reusable, yet current digital infrastructures often lack a unifying semantic framework that bridges human cognition and machine-actionability. In this paper, we introduce the Grammar of FAIR: a granular and modular architecture for FAIR semantics built on the concept of semantic units. Semantic units, comprising atomic statement units and composite compound units, implement the principle of semantic modularisation, decomposing data and knowledge into independently identifiable, semantically meaningful, and machine-actionable units. A central metaphor guiding our approach is the analogy between the hierarchy of level of organisation in biological systems and the hierarchy of levels of organisation in information systems: both are structured by granular building blocks that mediate across multiple perspectives while preserving functional unity. Drawing further inspiration from concept formation and natural language grammar, we show how these building blocks map to FAIR Digitial Objects (FDOs), enabling format-agnostic semantic transitivity from natural language token models to schema-based representations. This dual biological-linguistic analogy provides a semantics-first foundation for evolving cross-ecosystem infrastructures, paving the way for the Internet of FAIR Data and Services (IFDS) and a future of modular, AI-ready, and citation-granular scholarly communication.",2025-09-30,http://arxiv.org/pdf/2509.26434v1
Strain-Gradient-Driven Decoupling of Thermal Suppression from Anisotropy in \b{eta}-Ga2O3,"Guangwu Zhang, Xing Xiang, Ziyan Qian, Yixin Xu, Shengying Yue, Hyejin Jang, Lin Yang, Yanguang Zhou, Xinyu Wang, Qiye Zheng","Strain gradients, ubiquitous in flexible devices and epitaxial nanostructures, are a major blind spot for thermal transport in \b{eta}-Ga2O3. We establish that strain gradient unlocks a thermal conductivity (k) suppression mechanism fundamentally more potent than uniform strain: moderate uniaxial gradients (0.6%/nm) suppress k by 32-37% (27-30%) in thin films (nanowires), intensifying to 43.3% with biaxial gradients. This reduction far exceeds that from equivalent uniform strain and surpasses benchmark materials like silicon and BAs. Critically, a surprising decoupling emerges: while 3% uniform strain alters thermal anisotropy by ~25%, strain gradient strongly suppresses k with preserving this ratio. Mechanistically, strain gradients-induced symmetry breaking and enhanced mode coupling anisotropically activate forbidden scattering channels, making gradient-driven scattering dominant over intrinsic phonon scattering below 6.25 THz. These findings redefine non-uniform strain from a parasitic flaw into a powerful design tool for engineering thermal isolation and heat flux in next-generation flexible and high-power \b{eta}-Ga2O3 electronics.",2025-09-30,http://arxiv.org/pdf/2509.26412v1
Method for backtracking the layer thermal conductivities of multilayer thin film structure using coupled Newton Raphson approach and 3-omega approach,"Aigbe Awenlimobor, Jiajun Xu","The thermal conductivity of thin films is commonly estimated using the 3-omega experimental method. When calibrating the test setup, it is customary to use a specimen with a known thermal conductivity for validation. However, when determining the thermal conductivity of samples with unknown values, numerical approximations can provide a means to validate experimental results and ensure the integrity of the setup. A simple analytical or finite element analysis (FEA) method can be used to achieve this. For multilayer systems of unknown layer thermal conductivities, the 3-omega experimental setup only provides information about the overall bulk thermal conductivity of the system. To obtain the individual layer thermal conductivities, a combined experimental and numerical approach can be used. This article presents a novel method for backtracking the layer thermal conductivities of a multilayer thin film structure using a coupled 3-omega experimental and Newton-Raphson numerical approach. The method is validated using high-fidelity data obtained from literature.",2025-09-30,http://arxiv.org/pdf/2509.26408v1
Increase in packing density during multi-layer powder spreading: An experimental and numerical study,"Olivier Gaboriault, Anatolie Timercan, Roger Pelletier, Louis-Philippe Lefebvre, David Melancon, Bruno Blais","A custom apparatus designed to isolate and replicate the spreading process of metal powder in additive manufacturing demonstrates a sudden and unexplained increase in packing density beyond layers 5 to 10. We replicate the experiments that lead to densification with the discrete element method (DEM) using \lethe{}, an open-source software framework. We show that large-scale multi-layer DEM simulations are able to reproduce the densification observed experimentally. Using the Lagrangian simulation results, we highlight significant particle displacement in the powder bed at lower layer number, accompanied by static zones generated by the vertical wall surrounding the powder bed. The amplitude of the densification and the layer number at which it starts to occur is correlated to the distance between those two vertical walls which delimit the powder spreading area. This study addresses the gap between mono-layer powder spreading studies on hard-flat surfaces and the actual metal powder-based additive manufacturing processes by providing a better understanding of how the powder bed behaves during multi-layer spreading.",2025-09-30,http://arxiv.org/pdf/2509.26402v1
Precision measurement and modelling of the threshold-free 210Pb β spectrum,"Shuo Zhang, Hao-Ran Liu, Xavier Mougeot, Paul-Antoine Hervieux, Tao Sun, Wen-Tao Wu, Robin Cantor, Jingkai Xia, Zhi Liu, Cheng Liang, Fu-You Fan, Le Zhang, Mingyu Ge, Xiao-Peng Zhou, Adrien Andoche","Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete \beta decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full \beta decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of \beta particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision \beta decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {\sigma}) enhancement in \beta emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for \beta decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos.",2025-09-30,http://arxiv.org/pdf/2509.26390v1
Microwave-to-Optical Quantum Transduction of Photons for Quantum Interconnects,"Akihiko Sekine, Ryo Murakami, Yoshiyasu Doi","The quantum transduction, or equivalently quantum frequency conversion, is vital for the realization of, e.g., quantum networks, distributed quantum computing, and quantum repeaters. The microwave-to-optical quantum transduction is of particular interest in the field of superconducting quantum computing, since interconnecting dilution refrigerators is considered inevitable for realizing large-scale quantum computers with fault-tolerance. In this review, we overview recent theoretical and experimental studies on the quantum transduction between microwave and optical photons. We describe a generic theory for the quantum transduction employing the input-output formalism, from which the essential quantities characterizing the transduction, i.e., the expressions for the transduction efficiency, the added noise, and the transduction bandwidth are derived. We review the major transduction methods that have been experimentally demonstrated, focusing the transduction via the optomechanical effect, the electro-optic effect, the magneto-optic effect, and the atomic ensembles. We also briefly review the recent experimental progress on the quantum transduction from superconducting qubit to optical photon, which is an important step toward the quantum state transfer between distant superconducting qubits interconnected over optical fibers.",2025-09-30,http://arxiv.org/pdf/2509.26349v1
MoSe2 and WSe2 shell morphology control via temperature optimization during two-step growth of ZnSe-based core-shell nanowires,"Luize Dipane, Liora Kotlara, Viktors Vibornijs, Katrina Laganovska, Aleksejs Zolotarjovs, Eriks Dipans, Jevgenijs Gabrusenoks, Boris Polyakov, Edgars Butanovs","Achieving uniform and controlled transition metal dichalcogenide (TMD) shell growth on nanowires (NWs) remains a key challenge, limiting the development of high-quality core-shell heterostructures for optoelectronic and photocatalytic applications. In this work, the fabrication of ZnSe-MoSe2 and ZnSe-WSe2 core-shell NWs was successfully demonstrated. ZnSe NWs were grown via the vapor-liquid-solid growth mechanism, while TMD (MoSe2 or WSe2) shells were formed through a two-step process of sacrificial oxide layer deposition via magnetron sputtering followed by selenization process in a chemical vapor transport reactor. As-grown nanostructures were characterized using X-ray diffraction, transmission electron microscopy, X-ray photoelectron spectroscopy, Raman spectroscopy and photoluminescence spectroscopy. It was observed that the TMD shell morphology can be controlled through the selenization process temperature optimization, which arises due to different growth mechanisms discussed here. The studied trends could be further extended to other semiconductor NW and TMD core-shell heterostructure growth, offering promising avenues for advanced nanoscale applications.",2025-09-30,http://arxiv.org/pdf/2509.26312v1
Femtosecond Laser Crystallization of Ultrathin a-Ge Films in Multilayer Stacks with Silicon Layers,"Yuzhu Cheng, Alexander V. Bulgakov, Nadezhda M. Bulgakova, Jiří Beránek, Aleksey V. Kacyuba, Vladimir A. Volodin","Ultrashort pulsed laser annealing is an efficient technique for crystallizing amorphous semiconductors with the possibility to obtain polycrystalline films at low temperatures, below the melting point, through non-thermal processes. Here, a multilayer structure consisting of alternating amorphous silicon and germanium films was annealed by mid-infrared (1500 nm) ultrashort (70 fs) laser pulses under single-shot and multi-shot irradiation conditions. We investigate selective crystallization of ultrathin (3.5 nm) a-Ge film, promising for the generation of highly photostable nanodots. Based on Raman spectroscopy analysis, we demonstrate that, in contrast to thicker (above 10 nm) Ge films, explosive stress-induced crystallization is suppressed in such ultrathin systems and proceeds via thermal melting. This is likely due to the islet structure of ultrathin films which results in the formation of nanopores at the Si-Ge interface and reduces stress confinement during ultrashort laser heating.",2025-09-30,http://arxiv.org/pdf/2509.26303v1
A general optimization framework for mapping local transition-state networks,"Qichen Xu, Anna Delin","Understanding how complex systems transition between states requires mapping the energy landscape that governs these changes. Local transition-state networks reveal the barrier architecture that explains observed behaviour and enables mechanism-based prediction across computational chemistry, biology, and physics, yet current practice either prescribes endpoints or randomly samples only a few saddles around an initial guess. We present a general optimization framework that systematically expands local coverage by coupling a multi-objective explorer with a bilayer minimum-mode kernel. The inner layer uses Hessian-vector products to recover the lowest-curvature subspace (smallest k eigenpairs), the outer layer optimizes on a reflected force to reach index-1 saddles, then a two-sided descent certifies connectivity. The GPU-based pipeline is portable across autodiff backends and eigensolvers and, on large atomistic-spin tests, matches explicit-Hessian accuracy while cutting peak memory and wall time by orders of magnitude. Applied to a DFT-parameterized N\'eel-type skyrmionic model, it recovers known routes and reveals previously unreported mechanisms, including meron-antimeron-mediated N\'eel-type skyrmionic duplication, annihilation, and chiral-droplet formation, enabling up to 32 pathways between biskyrmion (Q=2) and biantiskyrmion (Q=-2). The same core transfers to Cartesian atoms, automatically mapping canonical rearrangements of a Ni(111) heptamer, underscoring the framework's generality.",2025-09-30,http://arxiv.org/pdf/2509.26269v1
Observation of non-Hermitian topology in cold Rydberg quantum gases,"Jun Zhang, Ya-Jun Wang, Shi-Yao Shao, Bang Liu, Li-Hua Zhang, Zheng-Yuan Zhang, Xin Liu, Chao Yu, Qing Li, Han-Chao Chen, Yu Ma, Tian-Yu Han, Qi-Feng Wang, Jia-Dou Nan, Yi-Ming Yin, Dong-Yang Zhu, Qiao-Qiao Fang, Dong-Sheng Ding, Bao-Sen Shi","The pursuit of topological phenomena in non-Hermitian systems has unveiled new physics beyond the conventional Hermitian paradigm, yet their realization in interacting many-body platforms remains a critical challenge. Exploring this interplay is essential to understand how strong interactions and dissipation collectively shape topological phases in open quantum systems. Here, we experimentally demonstrate non-Hermitian spectra topology in a dissipative Rydberg atomic gas and characterize parameters-dependent winding numbers. By increasing the interaction strength, the system evolves from Hermitian to non-Hermitian regime, accompanying emergence of trajectory loop in the complex energy plane. As the scanning time is varied, the spectra topology becomes twisted in the complex energy plane manifesting as a topology phase transition with the sign winding number changed. When preparing the system in different initial states, we can access a nontrivial fractional phase within a parameter space that globally possesses an integer winding. Furthermore, by changing the scanning direction, we observe the differentiated loops, revealing the breaking of chirality symmetry. This work establishes cold Rydberg gases as a versatile platform for exploring the rich interplay between non-Hermitian topology, strong interactions, and dissipative quantum dynamics.",2025-09-30,http://arxiv.org/pdf/2509.26256v1
From Shapiro steps to photon-assisted tunneling in microwave-driven atomic-scale Josephson junctions with a single (magnetic) adatom,"Martina Trahms, Bharti Mahendru, Clemens B. Winkelmann, Katharina J. Franke","Ultra-small Josephson junctions are strongly influenced by noise and damping due to energy dissipation into the environment, which are expected to suppress phase coherence. Here, we investigate the coherence properties of atomic-scale Josephson junctions in a scanning tunneling microscope under microwave excitation. Plain Pb-Pb junctions exhibit hysteretic Shapiro steps as signature of a coherent resonant state. With increasing AC amplitude, phase coherence is reduced due to an increase of thermal fluctuations. In the presence of magnetic adatoms the Josephson coupling energy is reduced and quasi-particle tunneling is enhanced. With AC driving we observe a rapid suppression of coherence that we ascribe to photon-assisted quasi-particle tunneling through Yu-Shiba-Rusinov states. Our results highlight the presence of phase coherence and shed light on the origin of the transition to incoherent transport, thereby revealing the importance of controlling dissipation in nanoscale superconducting devices.",2025-09-30,http://arxiv.org/pdf/2509.26228v1
The silence of the weights: an investigation of structural pruning strategies for attention-based audio signal architectures,"Andrea Diecidue, Carlo Alberto Barbano, Piero Fraternali, Mathieu Fontaine, Enzo Tartaglione","Transformer-based models have become the state of the art across multiple domains, from natural language processing to machine listening, thanks to attention mechanisms. However, the attention layers require a large number of parameters and high-end hardware for both training and inference. We propose a novel pruning technique targeted explicitly at the attention mechanism, where we decouple the pruning of the four layers in the attention block, namely: query, keys, values and outputs' projection matrices. We also investigate pruning strategies to prune along the head and channel dimensions, and compare the performance of the Audio Spectrogram Transformer (AST) model under different pruning scenarios. Our results show that even by pruning 50\% of the attention parameters we incur in performance degradation of less than 1\%",2025-09-30,http://arxiv.org/pdf/2509.26207v1
LLM Agents for Knowledge Discovery in Atomic Layer Processing,"Andreas Werbrouck, Marshall B. Lindsay, Matthew Maschmann, Matthias J. Young","Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.",2025-09-30,http://arxiv.org/pdf/2509.26201v1
Improved capabilities of the TurboGAP code for radiation induced cascade simulations: an illustration with silicon,"Uttiyoarnab Saha, Ali Hamedani, Miguel A. Caro, Andrea E. Sand","TurboGAP is a software package designed for efficient molecular dynamics simulations using Gaussian Approximation Potential (GAP) machine-learning interatomic potentials (MLIP). In this work, we enhance the capabilities of TurboGAP for radiation damage simulations by implementing a two-temperature molecular dynamics model, based on electron density-dependent coupling of electronic and atomic subsystems. Additionally, we implement adaptive calculation of the timestep and grouping of atoms for cell-border cooling. Our implementation incorporates electronic stopping power either through a traditional friction-based model or a more realistic first-principles-derived model. By combining the computational efficiency of TurboGAP with the accuracy of GAP MLIP, we perform cascade simulations in silicon with primary knock-on atom (PKA) energies up to 10 keV. Our simulations scale to systems containing up to 1 million atoms. We study the generation and clustering of radiation-induced defects. We also calculate ion-beam mixing and compare our results with the experimental data, discussing how the GAP-MLIP along with the inclusion of a realistic electronic stopping model improves the prediction of experimental mixing values.",2025-09-30,http://arxiv.org/pdf/2509.26199v1
"PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils","Chun-Wun Cheng, Bin Dong, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero","Neural operator models for solving partial differential equations (PDEs) often rely on global mixing mechanisms-such as spectral convolutions or attention-which tend to oversmooth sharp local dynamics and introduce high computational cost. We present FINO, a finite-difference-inspired neural architecture that enforces strict locality while retaining multiscale representational power. FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and evolves states via an explicit, learnable time-stepping scheme. A central Local Operator Block leverage a differential stencil layer, a gating mask, and a linear fuse step to construct adaptive derivative-like local features that propagate forward in time. Embedded in an encoder-decoder with a bottleneck, FINO captures fine-grained local structures while preserving interpretability. We establish (i) a composition error bound linking one-step approximation error to stable long-horizon rollouts under a Lipschitz condition, and (ii) a universal approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six benchmarks and a climate modelling task, FINO achieves up to 44\% lower error and up to around 2\times speedups over state-of-the-art operator-learning baselines, demonstrating that strict locality with learnable time-stepping yields an accurate and scalable foundation for neural PDE solvers.",2025-09-30,http://arxiv.org/pdf/2509.26186v1
Parallax: Efficient LLM Inference Service over Decentralized Environment,"Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan","Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high-bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralized GPU pools. However, heterogeneity in GPU and limited interconnected network bandwidth, along with potentially dynamic availability, make efficient scheduling the central challenge in this scenario. In this paper, we present Parallax, a decentralized LLM serving system that turns a pool of heterogeneous GPUs into an efficient inference platform via a two-phase scheduler. Parallax decomposes planning into (i) model allocation, which places layers of each replica across diverse GPUs to jointly optimize latency and throughput under memory and link-bandwidth constraints, and (ii) request-time GPU pipeline selection, which stitches layers from different replicas into end-to-end execution chains that balance load and adapt to current conditions. We implement Parallax and evaluate it on open-source LLMs deployed over real volunteer nodes. Parallax consistently reduces latency and increases throughput relative to decentralized baselines, demonstrating that principled scheduling can make volunteer compute a practical, affordable substrate for LLM inference.   Github Repo at: https://github.com/GradientHQ/parallax.",2025-09-30,http://arxiv.org/pdf/2509.26182v1
The detection of high X-ray polarization from an accretion disc corona source and its modelling via Monte Carlo radiation transfer simulation,"Ryota Tomaru, Chris Done, Hirokazu Odaka","We report an X-ray polarization degree (PD) of $8.8\pm1.4\%~(1\sigma)$ from the accretion-disc-corona (ADC) neutron-star system 2S 0921-630 (=V395 Car) observed with the Imaging X-ray Polarimetry Explorer (IXPE). The PD increases with energy, while the polarization angle (PA) varies significantly across the band. These trends are consistent with a high-inclination ADC geometry where the vertically extended disc blocks direct sight of the central X-ray source, and the observed X-rays are those scattered in an equatorial disc wind. We also find tentative PD variability in the 2--3 keV band. To interpret the time-averaged polarization, we build spectropolarimetric models by Monte Carlo radiation transfer simulation with column density distribution of thermal-radiative wind launched by X-ray irradiation of the outer disc under an axisymmetric geometry. The model combines boundary-layer emission, its disc reflection, and the disc continuum, each with its intrinsic polarization. Scattering of this composite spectrum in the wind reproduces both the observed PD and its increase with energy. However, the observed PA evolution is not captured, which may indicate departures from axisymmetry--e.g. misalignment between the inner disc (and/or neutron-star spin) and the outer disc/wind, or a weak disc warping.",2025-09-30,http://arxiv.org/pdf/2509.26147v1
Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations,"James Panayis, Matt Field, Vignesh Gopakumar, Andrew Lahiff, Kristian Zarebski, Aby Abraham, Jonathan L. Hodges","There is high demand on fire simulations, in both scale and quantity. We present a multi-pronged approach to improving the time and energy required to meet these demands. We show the ability of a custom machine learning surrogate model to predict the dynamics of heat propagation orders of magnitude faster than state-of-the-art CFD software for this application. We also demonstrate how a guided optimisation procedure can decrease the number of simulations required to meet an objective; using lightweight models to decide which simulations to run, we see a tenfold reduction when locating the most dangerous location for a fire to occur within a building based on the impact of smoke on visibility. Finally we present a framework and product, Simvue, through which we access these tools along with a host of automatic organisational and tracking features which enables future reuse of data and more savings through better management of simulations and combating redundancy.",2025-09-30,http://arxiv.org/pdf/2509.26139v1
Computing Large Deviations of First-Passage-Time Statistics in Open Quantum Systems: Two Methods,"Fei Liu, Jiayin Gu","We propose two methods for computing the large deviations of the first-passage-time statistics in general open quantum systems. The first method determines the region of convergence of the joint Laplace transform and the $z$-transform of the first-passage time distribution by solving an equation of poles with respect to the $z$-transform parameter. The scaled cumulant generating function is then obtained as the logarithm of the boundary values within this region. The theoretical basis lies in the facts that the dynamics of the open quantum systems can be unraveled into a piecewise deterministic process and there exists a tilted Liouville master equation in Hilbert space. The second method uses a simulation-based approach built on the wave function cloning algorithm. To validate both methods, we derive analytical expressions for the scaled cumulant generating functions in field-driven two-level and three-level systems. In addition, we present numerical results alongside cloning simulations for a field-driven system comprising two interacting two-level atoms.",2025-09-30,http://arxiv.org/pdf/2509.26123v1
Confinement of Polariton Condensates in quasi-Flatband BICs in Plasmonic and Dielectric Metasurfaces,"Anton Matthijs Berghuis, Jose Luis Pura, Rafael P. Argante, Shunsuke Murai, José A. Sánchez-Gil, Jaime Gómez Rivas","We investigate exciton-polariton condensation in square arrays, composed of either dielectric silicon (Si) or plasmonic silver (Ag) nanodisks, covered with a dye-doped layer. Both arrays support symmetry-protected bound states in the continuum (BICs) at normal incidence, featuring electric quadrupolar ($( Q_{xy} $)) and magnetic dipolar ($( m_z $)) characters. Due to differences in mode coupling, these BICs are split by $(\sim 10$) meV in the Si array, whereas they remain nearly degenerate in the Ag array. Simulations reveal that interference in the Ag array results in hybrid modes, $((m_z + i\tilde{Q}_{xy})$) and $((m_z - i\tilde{Q}_{xy})$), which are polarized along orthogonal directions. Interestingly, this results in similar lasing thresholds in both Si and Ag arrays, regardless of the inherent non-radiative losses of Ag, and also a confinement of the polariton condensates in the Ag array. While condensation in the Si array occurs in the $( Q_{xy} $) BIC, producing a characteristic donut-shaped far-field emission in k-space, condensation in the Ag array populates the hybrid modes, leading to a double-cross emission pattern extending over a broad range of wave vectors due to the quasi-flatband nature of this mode. As a result, the Ag array also exhibits a strong confinement along the polarization axis in real space. However, for unpolarized emission, there is a similar spatial confinement in both Si and Ag arrays. This control over the confinement of condensates could also be exploited to control interactions. Our results highlight a novel mechanism for condensate confinement with potential applications in quantum computing and polaritonic circuitry.",2025-09-30,http://arxiv.org/pdf/2509.26099v1
The diffusion-driven orthorhombic to tetragonal transition in YBa$_2$Cu$_3$O$_7$ derived with a machine learning interatomic potential,"Davide Gambino, Niccolò Di Eugenio, Jesper Byggmästar, Johan Klarbring, Daniele Torsello, Flyura Djurabekova, Francesco Laviano","Defects in high temperature superconductors such as YBa$_2$Cu$_3$O$_7$ (YBCO) critically influence their superconducting behavior, as they substantially degrade or even suppress superconductivity. With the renewed interest in cuprates for next-generation superconducting magnets operating in radiation-harsh environments such as fusion reactors and particle accelerators, accurate atomistic modeling of defects and their dynamics has become essential. Here, we present a general-purpose machine-learning interatomic potential for YBCO, based on the Atomic Cluster Expansion (ACE) method and trained on Density Functional Theory (DFT) data, with particular emphasis on defects and their diffusion mechanisms. The potential is validated against DFT calculations of ground-state properties, defect formation energies of oxygen Frenkel pairs and diffusion barriers for their formation. Remarkably, the potential captures the diffusion-driven orthorhombic to tetragonal transition at elevated temperatures, a transformation that is difficult to describe with empirical potentials, elucidating how the formation of oxygen Frenkel pairs in the basal plane governs this order-disorder transition. The ACE potential introduced here enables large-scale, predictive atomistic simulations of defect dynamics and transport processes in YBCO, providing a powerful tool to explore its stability, performance, and functionality under realistic operating conditions. Moreover, this work proves that machine learning interatomic potentials are suitable for studies of quaternary oxides with complex chemistry.",2025-09-30,http://arxiv.org/pdf/2509.26095v1
"Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts","Hossein Enshaei, Pariya Jebreili, Sayed Mahmoud Sakahei","Electroencephalogram (EEG) artifact detection in real-world settings faces significant challenges such as computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and trade-offs between accuracy and complexity in deep learning models. We propose a hybrid spectral-temporal framework for real-time detection and classification of ocular (EOG), muscular (EMG), and white noise artifacts in single-channel EEG. This method, in contrast to other approaches, combines time-domain low-pass filtering (targeting low-frequency EOG) and frequency-domain power spectral density (PSD) analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature fusion to minimize redundancy while preserving discriminative information. This feature engineering strategy allows a lightweight multi-layer perceptron (MLP) architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB). Additionally, this framework addresses the unexplored problem of simultaneous multi-source contamination(EMG+EOG+white noise), where it maintains 96% classification accuracy despite overlapping artifacts. With 30-second training times (97% faster than CNNs) and robust performance across SNR levels, this framework bridges the gap between clinical applicability and computational efficiency, which enables real-time use in wearable brain-computer interfaces. This work also challenges the ubiquitous dependence on model depth for EEG artifact detection by demonstrating that domain-informed feature fusion surpasses complex architecture in noisy scenarios.",2025-09-30,http://arxiv.org/pdf/2509.26058v1
Decoding Shake-up Satellites in XPS through Large-Scale ab initio Simulations: Spectral Signatures of Ring Fusion in Porphyrins,"Jannis Kockläuner, Majid Shaker, Maximilian Muth, Simon Steinbach, Christoph Oleszak, Ole Lytken, Hans-Peter Steinrück, Dorothea Golze","In X-ray photoelectron spectroscopy (XPS), shake-up satellites arise when core ionization is accompanied by simultaneous charge-neutral valence excitations. Although these satellites can contain detailed structural information, they are rarely interpreted due to the lack of accurate and scalable theoretical methods. Here, we develop and apply a many-body perturbation theory framework within the $GW$ plus cumulant ($GW+C$) approach that enables accurate predictions of shake-up satellites in large molecular systems. For unfused, mono-fused, and doubly fused porphyrin derivatives with up to 170 atoms, we achieve excellent agreement with experiment, reproducing both main photoionization signals and satellite features within $0.2-0.3$ eV. We show that ring fusion strongly affects satellite features, whereas the N 1s photoionization signals remain unchanged. Our calculations reveal the mechanism behind these changes, identifying the spatial localization of valence excitations as the driving force. This work not only deepens understanding of the shake-up mechanism in porphyrins but also shows how predictive computations can unlock the chemical information encoded in satellites.",2025-09-30,http://arxiv.org/pdf/2509.26057v1
Extreme NiI/FeI abundance ratio in the coma of the interstellar comet 3I/ATLAS,"Damien Hutsemékers, Jean Manfroid, Emmanuël Jehin, Cyrielle Opitom, Michele Bannister, Juan Pablo Carvajal, Rosemary Dorsey, K Aravind, Baltasar Luco, Brian Murphy, Thomas H. Puzia, Rohan Rahatgaonkar","Emission lines of FeI and NiI are commonly found in the coma of solar system comets, even at large heliocentric distances. These atoms are most likely released from the surface of the comet's nucleus or from a short-lived parent. The presence of these lines in cometary spectra is unexpected because the surface blackbody equilibrium temperature is too low to allow the sublimation of refractory minerals containing these metals. These lines were also found in the interstellar comet 2I/Borisov which has a NiI/FeI abundance ratio similar to that observed in solar system comets. On average, this ratio is one order of magnitude higher than the solar Ni/Fe abundance ratio. Here, we report observations of the new interstellar comet 3I/ATLAS, which were carried out with the ESO Very Large Telescope equipped with the UVES spectrograph. Spectra were obtained at six epochs, at heliocentric distances ranging from 3.14 to 2.14 au. NiI was detected at all epochs. FeI was only detected at heliocentric distances smaller than 2.64 au. We estimated the NiI and FeI production rates by comparing the observed line intensities with those produced by a fluorescence model. Comet 3I exhibits a high production rate of NiI atoms as well as a high NiI/FeI ratio, making it exceptional when compared to solar system comets and 2I/Borisov. Additionally, we found that the NiI/FeI ratio decreases rapidly with decreasing heliocentric distance, suggesting that comet 3I could soon become indistinguishable from solar system comets in this respect. We interpreted these observations assuming that the NiI and FeI atoms were released through the sublimation of Ni(CO)$_4$ and Fe(CO)$_5$ carbonyls, which supports the presence of these species in the cometary material.",2025-09-30,http://arxiv.org/pdf/2509.26053v1
Scaling Up Temporal Domain Generalization via Temporal Experts Averaging,"Aoming Liu, Kevin Miller, Venkatesh Saligrama, Kate Saenko, Boqing Gong, Ser-Nam Lim, Bryan A. Plummer","Temporal Domain Generalization (TDG) aims to generalize across temporal distribution shifts, e.g., lexical change over time. Prior work often addresses this by predicting future model weights. However, full model prediction is prohibitively expensive for even reasonably sized models. Thus, recent methods only predict the classifier layer, limiting generalization by failing to adjust other model components. To address this, we propose Temporal Experts Averaging (TEA), a novel and scalable TDG framework that updates the entire model using weight averaging to maximize generalization potential while minimizing computational costs. Our theoretical analysis guides us to two steps that enhance generalization to future domains. First, we create expert models with functional diversity yet parameter similarity by fine-tuning a domain-agnostic base model on individual temporal domains while constraining weight changes. Second, we optimize the bias-variance tradeoff through adaptive averaging coefficients derived from modeling temporal weight trajectories in a principal component subspace. Expert's contributions are based on their projected proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5 models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69% while being up to 60x more efficient.",2025-09-30,http://arxiv.org/pdf/2509.26045v1
Edge-On Disk Study (EODS) III: Molecular Stratification in the Flying Saucer Disk,"A. Dutrey, O. Denis-Alpizar, S. Guilloteau, C. Foucher, S. Gavino, D. Semenov, V. Pietu, E. Chapillon, L. Testi, E. Dartois, E. DiFolco, K. Furuya, U. Gorti, N. Grosso, Th. Henning, J. M. Huré, Á. Kóspál, F. Le Petit, L. Majumdar, R. Meshaka, H. Nomura, N. T. Phuong, M. Ruaud, Y. W. Tang, S. Wolf","Context: Investigating the vertical distribution of molecular content in protoplanetary disks remains difficult in most disks mildly inclined along the line of sight. In contrast, edge-on disks provide a direct (tomographic) view of the 2D molecular brightness. Aims: We study the radial and vertical molecular distribution as well as the gas temperature and density by observing the Keplerian edge-on disk surrounding the Flying Saucer, a Class II object located in Ophiuchus. Methods: We use new and archival ALMA data to perform a tomography of $^{12}$CO, $^{13}$CO, C$^{18}$O, CN, HCN, CS, H$_2$CO, c-C$_3$H$_2$, N$_2$D$^+$, DCN and $^{13}$CS. We analyze molecular tomographies and model data using the radiative transfer code DiskFit. Results: We directly measure the altitude above the mid-plane for each observed species. For the first time, we unambiguously demonstrate the presence of a common molecular layer and measure its thickness: most molecules are located at the same altitude versus radius. Beyond CO, as predicted by chemical models, the CN emission traces the upper boundary of the molecular layer, whereas the deuterated species (DCN and N2D+) resides below one scale-height. Our best fits from DiskFit show that most observed transitions in the molecular layer are thermalized because their excitation temperature is the same, around 17-20 K. Conclusions: These long-integration observations clearly reveal a molecular layer predominantly located around 1-2 scale height, at a temperature above the CO freeze-out temperature. The deuterated molecules are closer to the mid-plane and N2D+ may be a good proxy for the CO snowline. Some molecules, such as CN and H2CO, are likely influenced by the disk environment, at least beyond the mm dust disk radius. The direct observation of the molecular stratification opens the door to detailed chemical modeling in this disk which appears representative of T Tauri disks.",2025-09-30,http://arxiv.org/pdf/2509.26033v1
Muon Outperforms Adam in Tail-End Associative Memory Learning,"Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, Vincent Y. F. Tan","The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.",2025-09-30,http://arxiv.org/pdf/2509.26030v1
Arbitrary Instantaneous Bandwidth Microwave Receiver via Scalable Rydberg Vapor Cell Array with Stark Comb,"Yuechun Jiao, Yuwen Yin, Yunhui He, Jinlian Hu, Cheng Lu, Jingxu Bai, Zhengyang Bai, Weibin Li, Suotang Jia, Jianming Zhao","Rydberg atoms have great potential for microwave (MW) measurements due to their high sensitivity, broad carrier bandwidth, and traceability. However, the narrow instantaneous bandwidth of the MW receiver limits its applications. Improving the instantaneous bandwidth of the receiver is an ongoing challenge. Here, we report on the achievement of an arbitrary instantaneous bandwidth MW receiver via a linear array of scalable Rydberg vapor cells with Stark comb, where the Stark comb consists of an MW frequency comb (MFC) and a position-dependent Stark field. In the presence of the Stark field, the resonance MW transition frequency between two Rydberg states is position dependent, so that we can make each MFC line act as a local oscillator (LO) field to resonantly couple one Rydberg cell. Thus, each cell receives part of a broadband MW signal within its instantaneous bandwidth using atomic heterodyne detection, achieving the measurements of the broadband MW signal simultaneously. In our proof-of-principle experiment, we demonstrate the MW receiver with 210~MHz instantaneous bandwidth using an MFC field with 21 lines. Meanwhile, we achieve an overall sensitivity of 326.6~nVcm$^{-1}$Hz$^{-1/2}$. In principle, the method allows for achieving an arbitrary instantaneous bandwidth of the receiver, provided we have enough MFC lines with enough power. Our work paves the way to design and develop a scalable MW receiver for applications in radar, communication, and spectrum monitoring.",2025-09-30,http://arxiv.org/pdf/2509.26026v1
User-Centric Comparison of 5G NTN and DVB-S2/RCS2 Using OpenAirInterface and OpenSAND,"Sumit Kumar, Juan Carlos Estrada-Jimenez, Ion Turcanu","The integration of satellite networks into next-generation mobile communication systems has gained considerable momentum with the advent of 5G Non-Terrestrial Networks (5G-NTN). Since established technologies like DVB-S2/RCS2 are already widely used for satellite broadband, a detailed comparison with emerging 5G NTN solutions is necessary to understand their relative merits and guide deployment decisions. This paper presents a user-centric, end-to-end evaluation of these technologies under realistic traffic conditions, showing how differences in architecture and protocols impact application-layer performance. Utilizing the 6G Sandbox platform, we employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2, replicating transparent payload GEO satellite scenarios under uniform downlink conditions. A range of real-world applications, such as web browsing, file downloads, and video streaming, are tested across both systems and systematically analyzed. While the emulation lacks real-time capability, it reveals key strengths and limitations of each approach, helping identify suitable deployment scenarios for 5G NTN and DVB-S2/RCS2.",2025-09-30,http://arxiv.org/pdf/2509.26013v1
Scaling Equilibrium Propagation to Deeper Neural Network Architectures,"Sankar Vinayak. E. P, Gopalakrishnan Srinivasan","Equilibrium propagation has been proposed as a biologically plausible alternative to the backpropagation algorithm. The local nature of gradient computations, combined with the use of convergent RNNs to reach equilibrium states, make this approach well-suited for implementation on neuromorphic hardware. However, previous studies on equilibrium propagation have been restricted to networks containing only dense layers or relatively small architectures with a few convolutional layers followed by a final dense layer. These networks have a significant gap in accuracy compared to similarly sized feedforward networks trained with backpropagation. In this work, we introduce the Hopfield-Resnet architecture, which incorporates residual (or skip) connections in Hopfield networks with clipped $\mathrm{ReLU}$ as the activation function. The proposed architectural enhancements enable the training of networks with nearly twice the number of layers reported in prior works. For example, Hopfield-Resnet13 achieves 93.92\% accuracy on CIFAR-10, which is $\approx$3.5\% higher than the previous best result and comparable to that provided by Resnet13 trained using backpropagation.",2025-09-30,http://arxiv.org/pdf/2509.26003v1
Weak Martingale Solutions of the Stochastic Schrödinger-Poisson-Landau-Lifshitz-Gilbert System,"Yurong Wei, Huaqiao Wang","The Schr\""{o}dinger-Poisson-Landau-Lifshitz-Gilbert (SPLLG) system can characterize the spin transfer torque mechanism transferring the spin angular momentum to the magnetization dynamics through spin-magnetization coupling. We study the three-dimensional stochastic SPLLG system driven by a multiplicative stochastic force containing a continuous noise and a small jump noise. We establish the existence of weak martingale solutions based on the penalized functional technique, the Faedo-Galerkin approximation, stochastic compactness method, and a careful identification of the limit. Due to the strong coupling and strong nonlinearity caused by the SPLLG system and stochastic effects, some crucial difficulties have been encountered in obtaining energy estimates and avoiding non-negativity of the test function. We mainly utilize the structure of equations and the property of martingales developing the new energy estimates, and apply the three-layer approximation to overcome these difficulties. In particular, we extend the results by Z. Brze\'{z}niak and U. Manna (Comm. Math. Phys., 2019) and by L.H. Chai, C.J. Garc\'{\i}a-Cervera and X. Yang (Arch. Ration. Mech. Anal., 2018) to both the stochastic case and the coupling case.",2025-09-30,http://arxiv.org/pdf/2509.25993v1
Rare-event detection in a backward-facing-step flow using live optical-flow velocimetry: observation of an upstream jet burst,"Juan Pimienta, Jean-Luc Aider","Rare and extreme events in turbulent flows play a critical role in transport, mixing and transition, yet are notoriously difficumt to capture experimentally. Here we report, to our knowledge, the first direct experimental detection of an upstream-directed jet burst in a backward-facing step (BFS) flow at $Re_h=2100$, using long-duration Live Optical Flow Velocimetry (L-OFV). Continuous monitoring over 1.5 h enabled a data-driven definition of extremes as rare velocity probes excursions deep into the observed distribution's tails; in practice, large negatice events ($u: Z < -6$, $v: Z < -5$ at $(x,y) = (2h,h / 2)$, where $|Z| > > 0$ stands for large deviations form the mean value) triggered the live capture of surrounding velocity fields. The recording is triggered when the probes surpass the defined threshold, using live analysis of the velocity fields. The detected event features a jet-like intrusino into the recirculation region initiated by the collapse of a merged Kelvin-Helmholtz vortex and sustained by counter-rotating vortices, and is accompanied with heavy-tailed probe statistics and simultaneous amplufucation of fluctuating kinetic energy and enstrophy. While a single event was recorded, underscoring its rarity, the results establish L-OFV as aviable platform for rare-event detection in separated shear layers and document a previously unreported mechanism of upstream jet bursting in BFS flow.",2025-09-30,http://arxiv.org/pdf/2509.25983v1
Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy,"Deniz Soysal, Xabier García-Andrade, Laura E. Rodriguez, Pablo Sobron, Laura M. Barge, Renaud Detry","Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots must interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels. Using curated subsets of the RRUFF database, we evaluate one-dimensional convolutional neural networks (CNNs) and report four advances: (i) Baseline-independent classification: compact CNNs surpass $k$-nearest-neighbors and support-vector machines on handcrafted features, removing background-correction and peak-picking stages while ensuring reproducibility through released data splits and scripts. (ii) Pooling-controlled robustness: tuning a single pooling parameter accommodates Raman shifts up to $30 \,\mathrm{cm}^{-1}$, balancing translational invariance with spectral resolution. (iii) Label-efficient learning: semi-supervised generative adversarial networks and contrastive pretraining raise accuracy by up to $11\%$ with only $10\%$ labels, valuable for autonomous deployments with scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and retraining only the softmax layer transfers models to unseen minerals at $\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited processors. This workflow, which involves training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets, provides a practical path toward robust, low-footprint Raman classification in autonomous exploration.",2025-09-30,http://arxiv.org/pdf/2509.25964v1
Neural Network State-Space Estimators,"Minxing Sun, Li Miao, Qingyu Shen, Yao Mao, Qiliang Bao","Classical state estimation algorithms rely on predefined target's state-space model, which complicates model derivation and limits adaptability when system dynamics change. Neural network based estimators offer a data-driven alternative, but rarely fuse classical estimation theory into their structure and demand large, pre-computed training sets. To overcome these limitations, we propose a unified state-space structure without target's state-space model and treats both the input-layer activations and all network weights as latent states to be estimated online. We instantiate this nonlinear model with three canonical estimators-the extended Kalman estimator, the unscented Kalman estimator, and the particle estimator to simulate different neural network and demonstrate its generality. We then benchmark our approach against seven leading neural network estimators across three representative scenarios. Results show that our neural network state-space estimators not only retain the robust learning capability, but also match or exceed the accuracy of both classical and pre-trained neural network methods. Code, data, and more result: github.com/ShineMinxing/PaperNNSSE.git",2025-09-30,http://arxiv.org/pdf/2509.25959v1
Dimensional Control of the Coherence Time of Scattered Light in Cold Atom Clouds,"Ana Cipris, Mateus a F Biscassi, J C C Capella, Martial Morisse, Hani Naim, Hugo Sedlacek, Apoorav Singh Deo, Stephan Asselie, Robin Kaiser, Raul Celistrino Teixeira, Romain Bachelard, Mathilde Hugbart","Cold atomic clouds are promising platforms for generating correlated photons, but multiple scattering and associated Doppler broadening limit their temporal coherence. Here we demonstrate that cloud geometry provides a powerful means to extend the coherence time of scattered light. In the experiment, intensity-correlation measurements show that an elongated (quasi-1D) cloud exhibits systematically longer coherence times than a spherical (3D) cloud of the same on-axis optical thickness, as a direct consequence of the suppression of multiple scattering in the elongated geometry. Random-walk simulations reproduce this trend and further show that elongation drives the coherence time toward the single-scattering limit. The combined results establish cloud geometry as a robust control parameter for temporal coherence in cold-atom ensembles, with potential applications in quantum optics and communication.",2025-09-30,http://arxiv.org/pdf/2509.25956v1
From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks,"Sven Brändle, Till Aczel, Andreas Plesner, Roger Wattenhofer","Differentiable Logic Gate Networks (DLGNs) are a very fast and energy-efficient alternative to conventional feed-forward networks. With learnable combinations of logical gates, DLGNs enable fast inference by hardware-friendly execution. Since the concept of DLGNs has only recently gained attention, these networks are still in their developmental infancy, including the design and scalability of their output layer. To date, this architecture has primarily been tested on datasets with up to ten classes.   This work examines the behavior of DLGNs on large multi-class datasets. We investigate its general expressiveness, its scalability, and evaluate alternative output strategies. Using both synthetic and real-world datasets, we provide key insights into the importance of temperature tuning and its impact on output layer performance. We evaluate conditions under which the Group-Sum layer performs well and how it can be applied to large-scale classification of up to 2000 classes.",2025-09-30,http://arxiv.org/pdf/2509.25933v1
DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models,"Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun","The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).",2025-09-30,http://arxiv.org/pdf/2509.25922v1
Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel,"Chuanyang Zheng, Jiankai Sun, Yihang Gao, Enze Xie, Yuehao Wang, Peihao Wang, Ting Xu, Matthew Chang, Liliang Ren, Jingyao Li, Jing Xiong, Kashif Rasul, Mac Schwager, Anderson Schneider, Zhangyang Wang, Yuriy Nevmyvaka","Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art large language models (LLMs). Traditionally, MoE relies on $\mathrm{Softmax}$ as the router score function to aggregate expert output, a designed choice that has persisted from the earliest MoE models to modern LLMs, and is now widely regarded as standard practice. However, the necessity of using $\mathrm{Softmax}$ to project router weights into a probability simplex remains an unchallenged assumption rather than a principled design choice. In this work, we first revisit the classical Nadaraya-Watson regression and observe that MoE shares the same mathematical formulation as Nadaraya-Watson regression. Furthermore, we show that both feed-forward neural network (FFN) and MoE can be interpreted as a special case of Nadaraya-Watson regression, where the kernel function corresponds to the input neurons of the output layer. Motivated by these insights, we propose the \textbf{zero-additional-cost} Kernel Inspired Router with Normalization (KERN), an FFN-style router function, as an alternative to $\mathrm{Softmax}$. We demonstrate that this router generalizes both $\mathrm{Sigmoid}$- and $\mathrm{Softmax}$-based routers. \textbf{Based on empirical observations and established practices in FFN implementation, we recommend the use of $\mathrm{ReLU}$ activation and $\ell_2$-normalization in $\mathrm{KERN}$ router function.} Comprehensive experiments in MoE and LLM validate the effectiveness of the proposed FFN-style router function \methodNorm.",2025-09-30,http://arxiv.org/pdf/2509.25913v1
Revealing Hidden Antiparallel Domains in Hexagonal Boron Nitride,"Yeri Lee, Juseung Oh, Kyung Yeol Ma, Seung Jin Lee, Eui Young Jung, Yani Wang, Kenji Watanabe, Takashi Taniguchi, Hailin Peng, Hiroki Ago, Ki Kang Kim, Hyeon Suk Shin, Sunmin Ryu","Hexagonal boron nitride (hBN) supports a wide range of two-dimensional (2D) technologies, yet assessing its crystalline quality over large areas remains a fundamental challenge. Both antiparallel domains, an intrinsic outcome of epitaxy on high-symmetry substrates, and orientational domains have long evaded optical detection. Here, we show that interferometric second-harmonic generation (SHG) polarimetry provides a powerful, non-destructive probe of lattice orientation and structural integrity in chemical vapor deposition-grown hBN. This approach reveals the ubiquitous formation of antiparallel domains and quantifies their impact on crystalline order. SHG intensity also emerges as a direct optical metric of domain disorder, spanning three orders of magnitude across films produced by ten different growth routes. Correlation with Raman spectroscopy establishes a unified framework for evaluating crystalline quality. Beyond hBN, this method offers a high-throughput route to wide-area structural imaging in other non-centrosymmetric 2D materials, advancing their deployment in electronics, photonics, and quantum technologies.",2025-09-30,http://arxiv.org/pdf/2509.25910v1
The Mimura Integral: A Unified Framework for Riemann and Lebesgue Integration,Yoshifumi Mimura,"An integral on Euclidean space, equivalent to the Lebesgue integral, is constructed by extending the notion of Riemann sums. In contrast to the Henstock--Kurzweil and McShane integrals, the construction recovers the full measure-theoretic structure -- outer measure, inner measure, and measurable sets -- rather than merely reproducing integration with respect to the Lebesgue measure. Whereas the classical approach to Lebesgue theory proceeds through a two-layer framework of measure and integration, these layers are unified here into a single framework, thereby avoiding duplication. Compared with the Daniell integral, the method is more concrete and accessible, serving both as an alternative to the Riemann integral and as a natural bridge to abstract Lebesgue theory.",2025-09-30,http://arxiv.org/pdf/2509.25875v1
SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV,"Jingyao Zhang, Jaewoo Park, Jongeun Lee, Elaheh Sadredini","Large Language Model (LLM) inference requires substantial computational resources, yet CPU-based inference remains essential for democratizing AI due to the widespread availability of CPUs compared to specialized accelerators. However, efficient LLM inference on CPUs faces two fundamental challenges: (1) existing CPU architectures struggle with low-precision arithmetic required by quantized models, where optimal bit precision varies across models and layers; and (2) the memory-bound nature of the token generation phase creates severe performance bottlenecks. To address these challenges, we propose SAIL (SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that efficiently supports arbitrary bit precisions with minimal overhead. SAIL integrates three key innovations: First, we introduce Batched LUT-based General Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory, enabling high data reuse through lookup tables and reducing memory movement. Second, our Pattern-Aware LUT optimization identifies and exploits redundancy in input activation patterns, reducing computation cycles by 13.8\%. Third, we develop an in-memory type conversion algorithm that leverages PIM's parallelism for efficient de-/quantization operations, alleviating pressure on CPU's vector units. Our architecture requires only 2\% hardware overhead and a single new instruction, while maintaining dual functionality as both compute and storage units. Experimental evaluations using a modified gem5 simulator demonstrate that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient CPU-based LLM inference.",2025-09-30,http://arxiv.org/pdf/2509.25853v1
Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation,"Zitong Bo, Yue Hu, Jinming Ma, Mingliang Zhou, Junhui Yin, Yachen Kang, Yuqi Liu, Tong Wu, Diyun Xiang, Hao Chen","Enabling robots to execute long-horizon manipulation tasks from free-form language instructions remains a fundamental challenge in embodied AI. While vision-language models (VLMs) have shown promise as high-level planners, their deployment in the real world is hindered by two gaps: (i) the scarcity of large-scale, sequential manipulation data that couples natural language with multi-step action plans, and (ii) the absence of dense, interpretable rewards for fine-tuning VLMs on planning objectives. To address these issues, we propose REVER, a framework that empowers VLMs to generate and validate long-horizon manipulation plans from natural language instructions in real-world scenarios. Under REVER we train and release RoboFarseer, a VLM incentivized to emit chain-of-thought that perform temporal and spatial reasoning, ensuring physically plausible and logically coherent plans. To obtain training data, we leverage the Universal Manipulation Interface framework to capture hardware-agnostic demonstrations of atomic skills. An automated annotation engine converts each demonstration into vision-instruction-plan triplet. We introduce a verifiable reward that scores the generated plan by its ordered bipartite matching overlap with the ground-truth skill sequence. At run time, the fine-tuned VLM functions both as a planner and as a monitor, verifying step-wise completion. RoboFarseer matches or exceeds the performance of proprietary models that are orders of magnitude larger, while on open-ended planning it surpasses the best baseline by more than 40%. In real-world, long-horizon tasks, the complete system boosts overall success by roughly 60% compared with the same low-level controller without the planner. We will open-source both the dataset and the trained model upon publication.",2025-09-30,http://arxiv.org/pdf/2509.25852v1
MuSLR: Multimodal Symbolic Logical Reasoning,"Jundong Xu, Hao Fei, Yuhui Zhang, Liangming Pan, Qijun Huang, Qian Liu, Preslav Nakov, Min-Yen Kan, William Yang Wang, Mong-Li Lee, Wynne Hsu","Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1's Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements. All data and code are publicly available at https://llm-symbol.github.io/MuSLR.",2025-09-30,http://arxiv.org/pdf/2509.25851v1
Dynamical Acoustic Control of Resonance Fluorescence from a Strongly Driven Two-Level System,"Yuan Zhan, Zixuan Wang, Richard P. Mirin, Kevin L. Silverman, Shuo Sun","Resonance fluorescence from a single two-level system is a cornerstone of quantum optics. In the strong driving regime, its emission spectrum exhibits the iconic Mollow triplet, with each spectral component corresponding to a transition between distinct atom-photon dressed states. Here, we experimentally study the resonance fluorescence spectrum under a novel driving configuration in which a second gigahertz-frequency field drives the Rabi transition between two atom-photon dressed states. Our experiment is performed on a single semiconductor quantum dot strongly driven by a laser and a surface acoustic wave. We observe emission spectra that are significantly altered from the standard Mollow triplet, including the dynamical cancellation of resonance fluorescence at the central emission frequency. These spectra are explained by a theoretical model that incorporates the hybridization of the two-level system, the optical field, and the acoustic field. Motivated by this model, we experimentally validate the condition for optimal cooling of acoustic phonons in an emitter-optomechanical system. Our results offer new insights into the quantum interactions among single two-level systems, optical fields, and acoustic fields in the strong driving limit, with important applications in nonclassical acoustic state generation, quantum transduction, and quantum sensing of thermal motions.",2025-09-30,http://arxiv.org/pdf/2509.25847v1
"Fine-Tuning Bulk-oriented Universal Interatomic Potentials for Surfaces: Accuracy, Efficiency, and Forgetting Control","Jaekyun Hwang, Taehun Lee, Yonghyuk Lee, Su-Hyun Yoo","Accurate prediction of surface energies and stabilities is essential for materials design, yet first-principles calculations remain computationally expensive and most existing interatomic potentials are trained only on bulk systems. Here, we demonstrate that fine-tuning foundation machine learning potentials (MLPs) significantly improves both computational efficiency and predictive accuracy for surface modeling. While existing universal interatomic potentials (UIPs) have been solely trained and validated on bulk datasets, we extend their applicability to complex and scientifically significant unary, binary, and ternary surface systems. We systematically compare models trained from scratch, zero-shot inference, conventional fine-tuning, and multi-head fine-tuning approach that enhances transferability and mitigates catastrophic forgetting. Fine-tuning consistently reduces prediction errors with orders-of-magnitude fewer training configurations, and multi-head fine-tuning delivers robust and generalizable predictions even for materials beyond the initial training domain. These findings offer practical guidance for leveraging pre-trained MLPs to accelerate surface modeling and highlight a scalable path toward data-efficient, next-generation atomic-scale simulations in computational materials science.",2025-09-30,http://arxiv.org/pdf/2509.25807v1
Scalable Reactive Atomistic Dynamics with GAIA,"Suhwan Song, Heejae Kim, Jaehee Jang, Hyuntae Cho, Gunhee Kim, Geonu Kim","The groundbreaking advance in materials and chemical research has been driven by the development of atomistic simulations. However, the broader applicability of the atomistic simulations remains restricted, as they inherently depend on energy models that are either inaccurate or computationally prohibitive. Machine learning interatomic potentials (MLIPs) have recently emerged as a promising class of energy models, but their deployment remains challenging due to the lack of systematic protocols for generating diverse training data. Here we automate the construction of training datasets to enable the development of general-purpose MLIPs, by introducing GAIA, an end-to-end framework to build a wide range of atomic arrangements. By employing systematic evaluation of metadynamics for effective structural exploration, GAIA overcomes the heuristic nature of conventional dataset generation. Using GAIA, we constructed Titan25, a benchmark-scale dataset, and trained MLIPs that closely match both static and dynamic density functional theory results. The models further reproduce experimental observations across reactive regimes, including detonation, coalescence, and catalytic activity. GAIA narrows the gap between experiment and simulation, and paves the way for the development of universal MLIPs that can reliably describe a wide spectrum of materials and chemical processes.",2025-09-30,http://arxiv.org/pdf/2509.25798v1
Growth Optimization of MoSi Thin Film and Measurement of Transport Critical Current Density of its Meander Structure,"Shekhar Chandra Pandey Shilpam Sharma, M. K. Chattopadhyay","Amorphous thin film superconductors are promising alternatives for the development of superconducting radiation detectors, especially superconducting nanowire single photon detectors (SNSPDs) and superconducting microwire single photon detectors (SWSPDs), due to their homogeneous nature, ease of deposition, and superconducting parameters comparable to the materials currently being used. A study on the optimization of the growth technology and superconducting transition temperature (TC) of MoSi thin films grown on SiO2 coated Si substrate is reported here. These films have been synthesized by co sputtering of Mo and Si targets with varying compositions and thicknesses to achieve optimized TC values close to that of the bulk. Mo80Si20 and Mo83Si17 compositions of the film, each with a thickness of 17 nm, exhibited the highest TC of 6.4 K and 5.9 K, respectively. Additionally, a meander structure with a 17 um wire width was patterned to estimate the transport critical current density (JC), which was measured to be 1.4E9 A per m2 at 4 K. Variation of the TC with film thickness and deposition pressure has been studied. Electrical resistance as a function of temperature of the film before and after meandering was also studied. These properties are compatible with the fabrication of superconducting nanowire, microwire and wide strip single photon detectors.",2025-09-30,http://arxiv.org/pdf/2509.25797v1
Effect of Deposition Pressure on the Superconductivity of Ti40V60 Alloy Thin Films,"Shekhar Chandra Pandey, Shilpam Sharma, R. Venkatesh, L. S. Sharath Chandra, M. K. Chattopadhyay","The growth and characterization of high quality superconducting thin films is essential for fundamental understanding and also for the use of these films in technological applications. In the present study, Ti40V60 alloy thin films have been deposited using DC magnetron co sputtering of Ti and V at ambient temperatures. The effect of deposition pressure on the film morphology, superconducting and normal state properties has been studied. Measurement of electrical resistance as a function of temperature indicates that up to a certain deposition pressure, the 20 nm thick Ti40V60 films exhibit metallic behavior in the normal state and superconductivity at low temperatures. Beyond a threshold pressure, the films show a negative temperature coefficient of resistance with a residual resistance ratio less than one. Electrical transport measurements in the presence of magnetic field were performed to find the current voltage characteristics of the thin films. Analysis of the I V curves indicates that the Ti40V60 alloy thin films have a large transport critical current density (JC) e.g. 1.475E10 A per m2 in zero magnetic field and 2.657E09 A per m2 in 4 T (both at 4 K). Analysis of the field dependence of flux line pinning force density indicates a combined effect of core delta k surface and core delta k point pinning mechanisms (where k is the Ginzburg Landau parameter). Additionally, spatial variations in the superconducting critical temperature (TC ) across the sample contribute to delta TC pinning. In higher magnetic fields, a contribution from delta l pinning (where l is the electron mean free path) also becomes significant. The findings indicate the potential of Ti40V60 alloy thin film for superconducting device applications like cryogenic radiation detectors.",2025-09-30,http://arxiv.org/pdf/2509.25772v1
Discovery of oxide Li-conducting electrolytes in uncharted chemical space via topology-constrained crystal structure prediction,"Seungwoo Hwang, Jiho Lee, Seungwu Han, Youngho Kang, Sungwoo Kang","Oxide Li-conducting solid-state electrolytes (SSEs) offer excellent chemical and thermal stability but typically exhibit lower ionic conductivity than sulfides and chlorides. This motivates the search for new oxide materials with enhanced conductivity. Crystal structure prediction is a powerful approach for identifying such candidates. However, the structural complexity of oxide SSEs, often involving unit cells with more than 100 atoms, presents significant challenges for conventional methods. In this study, we introduce TOPIC, a structure prediction algorithm that reduces configurational complexity by enforcing corner-sharing (CS) bond topology constraints. We demonstrate that TOPIC successfully reproduces the ground-state and metastable structures of known oxide SSEs, including LiTa$_2$PO$_8$ and Li$_7$La$_3$Zr$_2$O$_{12}$, which contain up to about 200 atoms per unit cell. By combining this approach with a pretrained machine-learning interatomic potential, we systematically screen quaternary oxide compositions and identify 92 promising candidates with CS frameworks. In particular, Li$_4$Hf$_2$Si$_3$O$_{12}$, which corresponds to the ground state at its composition, exhibits an ionic conductivity of 14 mS cm$^{-1}$, a hull energy of 21 meV atom$^{-1}$, and a band gap of 6.5 eV. Through our investigation, we identify the Li ratio as one of the key factors determining the stability of CS structures. Overall, our approach provides a practical and scalable pathway for discovering high-performance oxide solid electrolytes in previously unexplored chemical spaces.",2025-09-30,http://arxiv.org/pdf/2509.25763v1
HiFIRec: Towards High-Frequency yet Low-Intention Behaviors for Multi-Behavior Recommendation,"Ruiqi Luo, Ran Jin, Zhenglong Li, Kaixi Hu, Xiaohui Tao, Lin Li","Multi-behavior recommendation leverages multiple types of user-item interactions to address data sparsity and cold-start issues, providing personalized services in domains such as healthcare and e-commerce. Most existing methods utilize graph neural networks to model user intention in a unified manner, which inadequately considers the heterogeneity across different behaviors. Especially, high-frequency yet low-intention behaviors may implicitly contain noisy signals, and frequent patterns that are plausible while misleading, thereby hindering the learning of user intentions. To this end, this paper proposes a novel multi-behavior recommendation method, HiFIRec, that corrects the effect of high-frequency yet low-intention behaviors by differential behavior modeling. To revise the noisy signals, we hierarchically suppress it across layers by extracting neighborhood information through layer-wise neighborhood aggregation and further capturing user intentions through adaptive cross-layer feature fusion. To correct plausible frequent patterns, we propose an intensity-aware non-sampling strategy that dynamically adjusts the weights of negative samples. Extensive experiments on two benchmarks show that HiFIRec relatively improves HR@10 by 4.21%-6.81% over several state-of-the-art methods.",2025-09-30,http://arxiv.org/pdf/2509.25755v1
Test time training enhances in-context learning of nonlinear functions,"Kento Kuwataka, Taiji Suzuki","Test-time training (TTT) enhances model performance by explicitly updating designated parameters prior to each prediction to adapt to the test data. While TTT has demonstrated considerable empirical success, its theoretical underpinnings remain limited, particularly for nonlinear models. In this paper, we investigate the combination of TTT with in-context learning (ICL), where the model is given a few examples from the target distribution at inference time. We analyze this framework in the setting of single-index models $y=\sigma_*(\langle \beta, \mathbf{x} \rangle)$, where the feature vector $\beta$ is drawn from a hidden low-dimensional subspace. For single-layer transformers trained with gradient-based algorithms and adopting TTT, we establish an upper bound on the prediction risk. Our theory reveals that TTT enables the single-layer transformers to adapt to both the feature vector $\beta$ and the link function $\sigma_*$, which vary across tasks. This creates a sharp contrast with ICL alone, which is theoretically difficult to adapt to shifts in the link function. Moreover, we provide the convergence rate with respect to the data length, showing the predictive error can be driven arbitrarily close to the noise level as the context size and the network width grow.",2025-09-30,http://arxiv.org/pdf/2509.25741v1
A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise,"Indu Kant Deo, Akash Venkateshwaran, Rajeev K. Jaiman","Ship traffic is an increasing source of underwater radiated noise in coastal waters, motivating real-time digital twins of ocean acoustics for operational noise mitigation. We present a physics-guided probabilistic framework to predict three-dimensional transmission loss in realistic ocean environments. As a case study, we consider the Salish Sea along shipping routes from the Pacific Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver pairs was generated with a Gaussian beam solver across seasonal sound speed profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We first assess sparse variational Gaussian processes (SVGP) and then incorporate physics-based mean functions combining spherical spreading with frequency-dependent absorption. To capture nonlinear effects, we examine deep sigma-point processes and stochastic variational deep kernel learning. The final framework integrates four components: (i) a learnable physics-informed mean that represents dominant propagation trends, (ii) a convolutional encoder for bathymetry along the source-receiver track, (iii) a neural encoder for source, receiver, and frequency coordinates, and (iv) a residual SVGP layer that provides calibrated predictive uncertainty. This probabilistic digital twin facilitates the construction of sound-exposure bounds and worst-case scenarios for received levels. We further demonstrate the application of the framework to ship speed optimization, where predicted transmission loss combined with near-field source models provides sound exposure level estimates for minimizing acoustic impacts on marine mammals. The proposed framework advances uncertainty-aware digital twins for ocean acoustics and illustrates how physics-guided machine learning can support sustainable maritime operations.",2025-09-30,http://arxiv.org/pdf/2509.25730v1
Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities,"Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, Philip S. Yu","Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of ""atomic thinking"".",2025-09-30,http://arxiv.org/pdf/2509.25725v1
Towards A Universally Transferable Acceleration Method for Density Functional Theory,"Zhe Liu, Yuyan Ni, Zhichen Pu, Qiming Sun, Siyuan Liu, Wen Yan","Recently, sophisticated deep learning-based approaches have been developed for generating efficient initial guesses to accelerate the convergence of density functional theory (DFT) calculations. While the actual initial guesses are often density matrices (DM), quantities that can convert into density matrices also qualify as alternative forms of initial guesses. Hence, existing works mostly rely on the prediction of the Hamiltonian matrix for obtaining high-quality initial guesses. However, the Hamiltonian matrix is both numerically difficult to predict and intrinsically non-transferable, hindering the application of such models in real scenarios. In light of this, we propose a method that constructs DFT initial guesses by predicting the electron density in a compact auxiliary basis representation using E(3)-equivariant neural networks. Trained on small molecules with up to 20 atoms, our model is able to achieve an average 33.3% self-consistent field (SCF) step reduction on systems up to 60 atoms, substantially outperforming Hamiltonian-centric and DM-centric models. Critically, this acceleration remains nearly constant with increasing system sizes and exhibits strong transferring behaviors across orbital basis sets and exchange-correlation (XC) functionals. To the best of our knowledge, this work represents the first and robust candidate for a universally transferable DFT acceleration method. We are also releasing the SCFbench dataset and its accompanying code to facilitate future research in this promising direction.",2025-09-30,http://arxiv.org/pdf/2509.25724v1
Transformer-Based Neural Networks Backflow for Strongly Correlated Electronic Structure,"Huan Ma, Bowen Kan, Honghui Shang, Jinlong Yang","Solving the electronic Schr\""odinger equation for strongly correlated systems remains one of the grand challenges in quantum chemistry. Here we demonstrate that Transformer architectures can be adapted to capture the complex grammar of electronic correlations through neural network backflow. In this approach, electronic configurations are processed as token sequences, where attention layers learn non-local orbital correlations and token-specific neural networks map these contextual representations into backflowed orbitals. Application to strongly correlated iron-sulfur clusters validates our approach: for $\left[\mathrm{Fe}_2 \mathrm{~S}_2\left(\mathrm{SCH}_3\right)_4\right]^{2-}$ ([2Fe-2S]) (30e,20o), the ground-state energy within chemical accuracy of DMRG while predicting magnetic exchange coupling constants closer to experimental values than all compared methods including DMRG, CCSD(T), and recent neural network approaches. For $\left[\mathrm{Fe}_4 \mathrm{S}_4\left(\mathrm{SCH}_3\right)_4\right]^{2-}$ ([4Fe-4S]) (54e,36o), we match DMRG energies and accurately reproduce detailed spin-spin correlation patterns between all Fe centers. The approach scales favorably to large active spaces inaccessible to exact methods, with distributed VMC optimization enabling stable convergence. These results establish Transformer-based backflow as a powerful variational ansatz for strongly correlated electronic structure, achieving superior magnetic property predictions while maintaining chemical accuracy in total energies.",2025-09-30,http://arxiv.org/pdf/2509.25720v1
Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking,"Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen","Model merging, which combines multiple domain-specialized experts into a single model, offers a practical path to endow Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) with broad capabilities without the cost of joint training or serving many models. However, training-free methods rely on hand-tuned coefficients, whereas training-based methods primarily align parameters rather than downstream task behavior and typically treat all layers uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a training-light method that learns a small set of layer-wise coefficients using only unlabeled calibration data. The coefficients are optimized to explicitly align the merged model's hidden states and logits with those of the corresponding experts, with a coefficient regularizer for stability and task-weighted losses for controllable trade-offs. To capture inter-layer variation, Expert Merging++ augments this design with importance-guided chunking: a normalized layer-importance metric, derived from learned coefficients, task-vector magnitudes, and parameter counts, allocates more chunk-wise coefficients to high-importance layers while keeping low-importance layers lightweight. The result is a label-free, parameter-efficient, and scalable approach to multi-expert model merging across LLMs and MLLMs. Across MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our method surpasses strong training-free and training-based merging baselines, with Expert Merging++ delivering further gains and, in some cases, even exceeding supervised Mixture Training. The source code is available at https://github.com/Littleor/ExpertMerging.",2025-09-30,http://arxiv.org/pdf/2509.25712v1
Dual effects of Lamb Shift in Quantum Thermodynamical Systems,"Zi-chen Zhang, Chang-shui Yu","The Lamb shift as an additional energy correction induced by environments usually has a marginal contribution and hence is neglected. We demonstrate that the Lamb shift, which modifies the energy levels, can influence the heat current to varying extents. We focus on the steady-state heat current through two coupled two-level atoms, respectively, in contact with a heat reservoir at a certain temperature. We find that the Lamb shift suppresses the steady-state heat current at small temperature gradients, while at large gradients, the heat current is restricted by an upper bound without the Lamb shift but diverges when it is included. These results not only demonstrate the Lamb shift's critical role in quantum heat transport but also advance our understanding of its impact in quantum thermodynamics.",2025-09-30,http://arxiv.org/pdf/2509.25710v1
Dark Soliton Formation as a Dark-State Phase Transition in a Dissipative Superfluid Chain,"Robbe Ceulemans, Samuel E. Begg, Matthew J. Davis, Michiel Wouters","We identify and characterize a first-order dark-state phase transition between a discrete dark soliton and a uniform superfluid in a Bose-Hubbard chain with a single lossy site. Using classical-field (truncated-Wigner) simulations together with a Bogoliubov stability analysis, we show that the dark-state nature of the soliton suppresses fluctuations and shifts the critical point relative to the comparable phenomenon of optical bistability in driven-dissipative Kerr resonators. We then demonstrate that this mechanism quantitatively captures the bistability phase boundary observed in the experiment of R. Labouvie et al. [Phys. Rev. Lett. 116, 235302 (2016)], resolving substantial discrepancies in prior modeling efforts. Our results reveal how driving, dissipation and quantum coherence can interact to induce nonequilibrium phase transitions in ultra-cold atomic gases.",2025-09-30,http://arxiv.org/pdf/2509.25707v1
Pinching-Antenna Systems (PASS)-Enabled UAV Delivery,"Suyu Lv, Meng Li, Qi Li, Yuanwei Liu","A pinching-antenna systems (PASS)-enabled unmanned aerial vehicle (UAV) delivery framework is proposed, which exploits the capability of PASS to establish a strong line-of-sight link and reduce free-space pathloss.Aiming at minimizing the communication energy consumption in one cycle, a double-layer optimization (DLO) algorithm is developed by jointly optimizing the UAV delivery sequence and the pinching antenna (PA) activation vector. More specifically, at the outer layer, a hierarchical alternating optimization (HAO) scheme is proposed to tackle the NP-hard problem of delivery sequence planning, where a genetic algorithm performs global exploration to generate candidate solutions at the top-level, while a dynamic programming performs local refinement to obtain elite solutions at the lower-level. With determined UAV trajectory, at the inner layer, focus is placed on addressing the highly coupled mixed-integer nonlinear programming problem of PA activation vector optimization, where a pair of algorithms are proposed: 1) Branch-and-Bound (BnB) algorithm for finding global optimum; 2) incremental search and local refinement (ISLR) algorithm for reducing computational complexity. Simulation results indicate that: i) The proposed HAO-based delivery sequence planning scheme can effectively reduce the total flight distance, thereby decreasing flight time and communication energy consumption; ii) Both the proposed BnB and ISLR algorithms can achieve energy-efficient PA activation, with the former exhibiting better performance and the latter having lower complexity; iii) PASS outperforms the conventional multi-antenna systems, especially with higher communication rate requirements.",2025-09-30,http://arxiv.org/pdf/2509.25698v1
Lithium depth profiling in NMC/Graphite commercial coin cells under high C-rate cycling,"Naisargi Kanabar, Seiichiro Higashiya, Daniele Cherniak, Devendra Sadana, Stephen Bedell, Haralabos Efstathiadis","This study examines lithium distribution and its evolution in both anode and cathode materials of commercial lithium-ion coin cells subjected to high C-rate cycling, providing insights into lithium loss, trapping, and plating mechanisms. Cells were cycled at 1C to 3C rates, and post-mortem analysis were performed using Li nuclear reaction Analysis (Li-NRA), x-ray diffraction (XRD), and scanning electron microscopy (SEM) equipped with energy-dispersive x-ray spectroscopy (EDS). Li-NRA using the resonant nuclear reaction between an incident high-energy proton and lithium was used to measure the depth distribution of Li in the cathode and anode layers. The Li-NRA analysis revealed a surface lithium peak on the anode, likely associated with SEI formation and lithium plating, while the cathode exhibited a decrease in lithium content by ~19.7%. XRD analysis of the cathode showed a contraction of the c-lattice parameter and peak shifts consistent with lithium depletion and structural deformation, supported by SEM imaging. In contrast, the dead graphite anode shows an enhanced peak at 43.3{\deg}, which corresponds to the presence of metallic lithium or possibly Cu. 3-C rate cycling also led to capacity fade and an increase in internal resistance, highlighting the impact of lithium plating on cell performance.",2025-09-30,http://arxiv.org/pdf/2509.25697v1
A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation,"Zihui Zhao, Yuanbo Tang, Jieyu Ren, Xiaoping Zhang, Yang Li","Dictionary learning is traditionally formulated as an $L_1$-regularized signal reconstruction problem. While recent developments have incorporated discriminative, hierarchical, or generative structures, most approaches rely on encouraging representation sparsity over individual samples that overlook how atoms are shared across samples, resulting in redundant and sub-optimal dictionaries. We introduce a parsimony promoting regularizer based on the row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty encourages entire rows of the coefficient matrix to vanish, thereby reducing the number of dictionary atoms activated across the dataset. We derive the formulation from a probabilistic model with Beta-Bernoulli priors, which provides a Bayesian interpretation linking the regularization parameters to prior distributions. We further establish theoretical calculation for optimal hyperparameter selection and connect our formulation to both Minimum Description Length, Bayesian model selection and pathlet learning. Extensive experiments on benchmark datasets demonstrate that our method achieves substantially improved reconstruction quality (with a 20\% reduction in RMSE) and enhanced representation sparsity, utilizing fewer than one-tenth of the available dictionary atoms, while empirically validating our theoretical analysis.",2025-09-30,http://arxiv.org/pdf/2509.25690v1
Minimalist Explanation Generation and Circuit Discovery,"Pirzada Suhail, Aditya Anand, Amit Sethi","Machine learning models, by virtue of training, learn a large repertoire of decision rules for any given input, and any one of these may suffice to justify a prediction. However, in high-dimensional input spaces, such rules are difficult to identify and interpret. In this paper, we introduce an activation-matching based approach to generate minimal and faithful explanations for the decisions of pre-trained image classifiers. We aim to identify minimal explanations that not only preserve the model's decision but are also concise and human-readable. To achieve this, we train a lightweight autoencoder to produce binary masks that learns to highlight the decision-wise critical regions of an image while discarding irrelevant background. The training objective integrates activation alignment across multiple layers, consistency at the output label, priors that encourage sparsity, and compactness, along with a robustness constraint that enforces faithfulness. The minimal explanations so generated also lead us to mechanistically interpreting the model internals. In this regard we also introduce a circuit readout procedure wherein using the explanation's forward pass and gradients, we identify active channels and construct a channel-level graph, scoring inter-layer edges by ingress weight magnitude times source activation and feature-to-class links by classifier weight magnitude times feature activation. Together, these contributions provide a practical bridge between minimal input-level explanations and a mechanistic understanding of the internal computations driving model decisions.",2025-09-30,http://arxiv.org/pdf/2509.25686v1
LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts,"Yuan Zhuang, Yi Shen, Yuexin Bian, Qing Su, Shihao Ji, Yuanyuan Shi, Fei Miao","Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.",2025-09-30,http://arxiv.org/pdf/2509.25684v1
Charge Transfer States in Donor Acceptor Bulk Heterojunctions as Triplet Triplet Annihilation Sensitizer for Solid-State Photon Upconversion,"Maciej Klein, Alexander R. Ireland, Evan G. Moore, Dennis Delic, Ajay K. Pandey","Photon interconversion in semiconductors is of fundamental importance for digital imaging and quantum sensing. Nonlinear processes such as triplet-triplet annihilation (TTA) offer photon upconversion (UC) at yields desired for solid-state optoelectronic devices. Here, we present a multilayer molecular system where a near-infrared (NIR) photosensitizer facilitates robust photon UC. A molecular stack of squaraine DIB-SQ: 6,6-Phenyl C61 butyric acid methyl ester (PCBM) is optimized for UC by fine tuning the PCBM loading to engineer charge transfer states that facilitate further triplet generation. This composite NIR photosensitizer layer, at a 1:3 blend ratio in heterojunction with rubrene, drives triplet population density to levels desired for effective TTA. When paired with a fully optimized annihilator layer of tetraphenyldibenzoperiflanthene (DBP) doped rubrene, the NIR sensitizer produces a photon upconversion quantum yield of 1.36% at 690 nm, with a significantly low excitation intensity threshold for the onset of the linear regime.. Time-resolved photoluminescence, transient absorption spectroscopy, and magnetic field dependent photoluminescence measurements reveal a detailed balance of photoexcited states and formation of charge transfer states of triplet character (3CT), which work in tandem with molecular states to sensitize the triplet state (T1) of rubrene. This approach of harnessing near-infrared photons presents promising avenue for advancing solid-state photon interconversion.",2025-09-30,http://arxiv.org/pdf/2509.25679v1
Layer-wise dynamic rank for compressing large language models,"Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang","Large language models (LLMs) have rapidly scaled in size, bringing severe memory and computational challenges that hinder their deployment. Singular Value Decomposition (SVD)-based compression has emerged as an appealing post-training compression technique for LLMs, yet most existing methods apply a uniform compression ratio across all layers, implicitly assuming homogeneous information included in various layers. This overlooks the substantial intra-layer heterogeneity observed in LLMs, where middle layers tend to encode richer information while early and late layers are more redundant. In this work, we revisit the existing SVD-based compression method and propose D-Rank, a framework with layer-wise balanced Dynamic Rank allocation for LLMs compression. We first introduce effective rank as a principled metric to measure the information density of weight matrices, and then allocate ranks via a Lagrange multiplier-based optimization scheme to adaptively assign more capacity to groups with higher information density under a fixed compression ratio. Moreover, we rebalance the allocated ranks across attention layers to account for their varying importance and extend D-Rank to latest LLMs with grouped-query attention. Extensive experiments on various LLMs with different scales across multiple compression ratios demonstrate that D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40% compression ratio while achieving even higher throughput.",2025-09-30,http://arxiv.org/pdf/2509.25622v1
Nonuniform Water Distribution in Jupiter's Mid Latitudes: Influence of Precipitation and Planetary Rotation,"Huazhi Ge, Cheng Li, Xi Zhang, Andrew P. Ingersoll, Sihe Chen","Knowing the composition of Jupiter's atmosphere is crucial for constraining Jupiter's bulk metallicity and formation history. Yet, constraining Jupiter's atmospheric water abundance is challenging due to its potential non-uniform distribution. Here, we explicitly resolve the water hydrological cycle in Jupiter's mid-latitudes using high-resolution simulations. Falling precipitation leads to a significant large-scale depletion of water vapor beneath the lifting condensation level. A non-uniform water vapor distribution emerges in the mid-latitude simulation with a changing Coriolis parameter across latitudes and spatially uniform cooling and heating. Water abundance at the 7-bar level varies by up to a factor of ten across latitudes, from sub-solar to super-solar values. We propose that nonlinear large-scale eddies and waves tend to drift air parcels across latitudes along constant potential vorticity (PV) surfaces, thereby sustaining latitudinal dependencies in water vapor and the interplay between water distribution and large-scale dynamics. Therefore, water distribution is influenced by the vertical structure of density stratification and changing Coriolis parameter across Jupiter's mid-latitudes, as quantified by PV. Additionally, the water hydrological cycle amplifies the specific energy of air parcels through the latent heat effect, thereby slowing down vertical mixing with a latent heat flux. The horizontal gradient of water is expected to be more pronounced with a super-solar water abundance. We suggest that similar interplays between precipitating condensates, planetary rotation, and distribution of condensable species generally exist in the weather layer of fast-rotating giant planets. The ongoing Juno mission and future Uranus mission may further reveal the non-uniform distribution of condensed species and their interplay with large-scale dynamics.",2025-09-30,http://arxiv.org/pdf/2509.25610v1
Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models,"Max Hartman, Vidhata Jayaraman, Moulik Choraria, Akhil Bhimaraju, Lav R. Varshney","Vision-language models (VLMs) achieve incredible performance across a wide range of tasks, but their large size makes inference costly. Recent work shows that selectively skipping VLM layers can improve efficiency with minimal performance loss or even performance improvements. However, this technique remains underused due to the limited understanding of when layer skipping is beneficial. In this paper, we develop a framework that uses information and learning theory to characterize the conditions under which layer skipping enhances efficiency without sacrificing performance. Motivated by these observations, we analyze the evolution of the VLM's hidden representations through the LLM backbone and show that layers with large redundancy as predicted by our framework coincide with those skipped by popular layer-skipping methods in practice, providing a unified theoretical scaffolding for multiple efficient inference techniques. Our experiments demonstrate that skipping such layers yields faster inference that preserves performance, and also show that applying skipping outside these conditions leads to model degradation.",2025-09-29,http://arxiv.org/pdf/2509.25584v1
Development Status of the KIPM Detector Consortium,"Dylan J Temples, Zoë J. Smith, Selby Q Dang, Taylor Aralis, Chi Cap, Clarence Chang, Yen-Yung Chang, Maurice Garcia-Sciveres, Sunil Golwala, William Ho, Noah Kurinsky, Kungang Li, Xinran Li, Marharyta Lisovenko, Elizabeth Panner, Karthik Ramanathan, Shilin Ray, Brandon Sandoval, Aritoki Suzuki, Gensheng Wang, Osmond Wen, Michael Williams, Junwen Robin Xiong, Volodymyr Yefremenko","A Kinetic Inductance Phonon-Mediated Detector is a calorimeter that uses kinetic inductance detectors to read out phonon signals from the device substrate. We have established a consortium comprising university and national lab groups dedicated to advancing the state of the art in these detectors, with the ultimate goal of designing a detector sub-eV threshold on energy deposited in the substrate, enabling searches for both light dark matter and low-energy neutrino interactions. This consortium brings together experts in kinetic inductance detector design, phonon and quasiparticle dynamics, and noise modeling, along with specialized fabrication facilities, test platforms, and unique calibration capabilities. Recently, our consortium has demonstrated a resolution on energy absorbed by the sensor of 2.1 eV, the current record for such devices. The current focus of the consortium is modeling and improving the phonon collection efficiency and implementing low-$\boldsymbol{T_c}$ superconductors, both of which serve to improve the overall energy resolution and threshold of the detectors.",2025-09-29,http://arxiv.org/pdf/2509.25544v1
AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction,"Yonatan Gizachew Achamyeleh, Tongtao Zhang, Joshua Hyunki Kim, Gabriel Garcia, Shih-Yuan Yu, Anton Kocheturov, Mohammad Abdullah Al Faruque","Function name prediction is crucial for understanding stripped binaries in software reverse engineering, a key step for \textbf{enabling subsequent vulnerability analysis and patching}. However, existing approaches often struggle with architecture-specific limitations, data scarcity, and diverse naming conventions. We present AGNOMIN, a novel architecture-agnostic approach for multi-label function name prediction in stripped binaries. AGNOMIN builds Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs, Function Call Graphs, and dynamically learned \texttt{PCode} features. A hierarchical graph neural network processes this enriched structure to generate consistent function representations across architectures, vital for \textbf{scalable security assessments}. For function name prediction, AGNOMIN employs a Ren\'ee-inspired decoder, enhanced with an attention-based head layer and algorithmic improvements.   We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable binaries across three architectures, demonstrating its superior performance compared to state-of-the-art approaches, with improvements of up to 27.17\% in precision and 55.86\% in recall across the testing dataset. Moreover, AGNOMIN generalizes well to unseen architectures, achieving 5.89\% higher recall than the closest baseline. AGNOMIN's practical utility has been validated through security hackathons, where it successfully aided reverse engineers in analyzing and patching vulnerable binaries across different architectures.",2025-09-29,http://arxiv.org/pdf/2509.25514v1
A closed-loop $2\times4$ downlink MIMO Framework for 5G New Radio using OpenAirInterface,"Duc Tung Bui, Le-Nam Tran","We present the first-of-a-kind closed-loop $2\times4$ MIMO implementation for the downlink of 5G Open RAN using OpenAirInterface (OAI), which is capable of transmitting up to two transmission layers. Our implementation is a fully functional 5G New Radio (5G NR) system, including the 5G Core Network (5G CN), 5G Radio Access Network (5G RAN), as well as 5G NR User Equipment (UEs). This serves as a foundational framework for further advancements in the context of emerging Open RAN (O-RAN) development. A key feature of our implementation is the enhanced Channel State Information (CSI) reporting procedure at the UE, which includes Rank Indicator (RI), Precoding Matrix Indicator (PMI), and Channel Quality Indicator (CQI). It is adjusted for the extended configuration to maximize data rates. To demonstrate the performance of our implementation, we measure the downlink data rates using $\textit{iperf3}$ in two scenarios: (i) fixed channels to assess two-layer data transmission and (ii) $\textit{Rice1}$ channels for general transmission analysis. The obtained simulation results demonstrate that, compared to the existing $2\times2$ MIMO configuration in the OAI, our implementation improves the data rates in almost all scenarios, especially at the high Signal-to-Noise-Ratios (SNRs).",2025-09-29,http://arxiv.org/pdf/2509.25497v1
Scalable Boltzmann Generators for equilibrium sampling of large-scale materials,"Maximilian Schebek, Jutta Rogal","The use of generative models to sample equilibrium distributions of many-body systems, as first demonstrated by Boltzmann Generators, has attracted substantial interest due to their ability to produce unbiased and uncorrelated samples in `one shot'. Despite their promise and impressive results across the natural sciences, scaling these models to large systems remains a major challenge. In this work, we introduce a Boltzmann Generator architecture that addresses this scalability bottleneck with a focus on applications in materials science. We leverage augmented coupling flows in combination with graph neural networks to base the generation process on local environmental information, while allowing for energy-based training and fast inference. Compared to previous architectures, our model trains significantly faster, requires far less computational resources, and achieves superior sampling efficiencies. Crucially, the architecture is transferable to larger system sizes, which allows for the efficient sampling of materials with simulation cells of unprecedented size. We demonstrate the potential of our approach by applying it to several materials systems, including Lennard-Jones crystals, ice phases of mW water, and the phase diagram of silicon, for system sizes well above one thousand atoms. The trained Boltzmann Generators produce highly accurate equilibrium ensembles for various crystal structures, as well as Helmholtz and Gibbs free energies across a range of system sizes, able to reach scales where finite-size effects become negligible.",2025-09-29,http://arxiv.org/pdf/2509.25486v1
Conformal Prediction for Signal Temporal Logic Inference,"Danyang Li, Yixuan Wang, Matthew Cleaveland, Mingyu Cai, Roberto Tron","Signal Temporal Logic (STL) inference seeks to extract human-interpretable rules from time-series data, but existing methods lack formal confidence guarantees for the inferred rules. Conformal prediction (CP) is a technique that can provide statistical correctness guarantees, but is typically applied as a post-training wrapper without improving model learning. Instead, we introduce an end-to-end differentiable CP framework for STL inference that enhances both reliability and interpretability of the resulting formulas. We introduce a robustness-based nonconformity score, embed a smooth CP layer directly into training, and employ a new loss function that simultaneously optimizes inference accuracy and CP prediction sets with a single term. Following training, an exact CP procedure delivers statistical guarantees for the learned STL formulas. Experiments on benchmark time-series tasks show that our approach reduces uncertainty in predictions (i.e., it achieves high coverage while reducing prediction set size), and improves accuracy (i.e., the number of misclassifications when using a fixed threshold) over state-of-the-art baselines.",2025-09-29,http://arxiv.org/pdf/2509.25473v1
Bridging Mid-IR and Terahertz Domains in a Single High-Resolution Dual-Comb Spectroscopy Measurement,"D. Konnov, A. Muraviev, K. L. Vodopyanov","Dual-comb spectroscopy (DCS) utilizes a pair of broadband mutually coherent laser frequency combs to enable high-resolution, high-accuracy spectroscopic measurements with atomic-clock-level frequency referencing, and rapid, multiplexed acquisition without moving parts. It has traditionally been confined to specific domains: terahertz, infrared, visible, and ultraviolet, each requiring distinct comb sources and detection mechanisms tailored to the nature of the spectroscopic target. Yet, similar techniques may be implemented in the terahertz (THz) and mid-infrared (MIR) regions, such as optical rectification for comb generation and electro-optic sampling for detection, both using crystals with quadratic nonlinearity. However, in the Reststrahlen band near phonon resonances in these crystals, typically between 5 and 10 THz, both linear and nonlinear susceptibilities experience abnormally high dispersion, and light propagation is strongly suppressed. This confines DCS operation to spectral regions either below or above the Reststrahlen band and effectively separating the THz and MIR domains. Here we demonstrate high-resolution DCS performed simultaneously over two broad spectral bands, each spanning an octave or more. The measurements cover both the MIR (350-1150 cm$^{-1}$; 8.7-28.5 $\mu$m; 10.5-34.5 THz) and the THz region (80-160 cm$^{-1}$; 62.5-125 $\mu$m; 2.4-4.8 THz), effectively bridging these traditionally separate regions within a single acquisition. This enables direct cross-referencing of molecular absorption line strengths across widely separated spectral domains. As a proof of concept, we demonstrate the simultaneous acquisition of ro-vibrational and pure rotational absorption spectra of ammonia (NH$_3$) with a spectral resolution of 7.3 MHz (0.00024 cm$^{-1}$), sufficient to fully resolve Doppler-broadened line shapes across the entire measured spectral range.",2025-09-29,http://arxiv.org/pdf/2509.25468v1
BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions,"Yinghang Ma, Jiho Shin, Leuson Da Silva, Zhen Ming, Jiang, Song Wang, Foutse Khomh, Shin Hwei Tan","Recent advances in large language models (LLMs) have accelerated the development of AI-driven automated program repair (APR) solutions. However, these solutions are typically evaluated using static benchmarks such as Defects4J and SWE-bench, which suffer from two key limitations: (1) the risk of data contamination, potentially inflating evaluation results due to overlap with LLM training data, and (2) limited ability to assess the APR capabilities in dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel dynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers a structured approach to assess the cognitive capabilities of LLM-powered APR solutions across progressively complex reasoning levels. Using Defects4J as a case study, we evaluated two state-of-the-art LLM-powered APR solutions, ChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and StarCoder-2. Our findings show that while these solutions exhibit basic reasoning skills and effectively memorize bug-fixing patterns (fixing up to 81.57% of bugs at the Remember layer), their performance increases with synthetically generated bugs (up to 60.66% increase at the Understand layer). However, they perform worse on minor syntactic changes (fixing up to 43.32% at the Apply layer), and they struggle to repair similar bugs when injected into real-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer). These results underscore the urgent need for evolving benchmarks and provide a foundation for more trustworthy evaluation of LLM-powered software engineering solutions.",2025-09-29,http://arxiv.org/pdf/2509.25465v1
Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains,"Moritz von Tresckow, Ion Gabriel Ion, Dimitrios Loukrezis","This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.",2025-09-29,http://arxiv.org/pdf/2509.25450v1
Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs,"Hao Ban, Kaiyi Ji","Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.",2025-09-29,http://arxiv.org/pdf/2509.25414v1
Experimental demonstration of boson sampling as a hardware accelerator for monte carlo integration,"Malaquias Correa Anguita, Teun Roelink, Sara Marzban, Wim Briels, Claudia Filippi, Jelmer Renema","We present an experimental demonstration of boson sampling as a hardware accelerator for Monte Carlo integration. Our approach leverages importance sampling to factorize an integrand into a distribution that can be sampled using quantum hardware and a function that can be evaluated classically, enabling hybrid quantum-classical computation. We argue that for certain classes of integrals, this method offers a quantum advantage by efficiently sampling from probability distributions that are hard to simulate classically. We also identify structural criteria that must be satisfied to preserve computational hardness, notably the sensitivity of the classical post-processing function to high-order quantum correlations. To validate our protocol, we implement a proof-of-principle experiment on a programmable photonic platform to compute the first-order energy correction of a three-boson system in a harmonic trap under an Efimov-inspired three-body perturbation. The experimental results are consistent with theoretical predictions and numerical simulations, with deviations explained by photon distinguishability, discretization, and unitary imperfections. Additionally, we provide an error budget quantifying the impact of these same sources of noise. Our work establishes a concrete use case for near-term photonic quantum devices and highlights a viable path toward practical quantum advantage in scientific computing.",2025-09-29,http://arxiv.org/pdf/2509.25404v1
smallNet: Implementation of a convolutional layer in tiny FPGAs,"Fernanda Zapata Bascuñán, Alan Ezequiel Fuster","Since current neural network development systems in Xilinx and VLSI require codevelopment with Python libraries, the first stage of a convolutional network has been implemented by developing a convolutional layer entirely in Verilog. This handcoded design, free of IP cores and based on a filter polynomial like structure, enables straightforward deployment not only on low cost FPGAs but also on SoMs, SoCs, and ASICs. We analyze the limitations of numerical representations and compare our implemented architecture, smallNet, with its computer based counterpart, demonstrating a 5.1x speedup, over 81% classification accuracy, and a total power consumption of just 1.5 W. The algorithm is validated on a single-core Cora Z7, demonstrating its feasibility for real time, resource-constrained embedded applications.",2025-09-29,http://arxiv.org/pdf/2509.25391v1
Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation,"Penglei Gao, Yan Zou, Abhijit Duggal, Shuaiqi Huang, Faming Liang, Xiaofeng Wang","We introduce the Functional Competing Risk Net (FCRN), a unified deep-learning framework for discrete-time survival analysis under competing risks, which seamlessly integrates functional covariates and handles missing data within an end-to-end model. By combining a micro-network Basis Layer for functional data representation with a gradient-based imputation module, FCRN simultaneously learns to impute missing values and predict event-specific hazards. Evaluated on multiple simulated datasets and a real-world ICU case study using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates substantial improvements in prediction accuracy over random survival forests and traditional competing risks models. This approach advances prognostic modeling in critical care by more effectively capturing dynamic risk factors and static predictors while accommodating irregular and incomplete data.",2025-09-29,http://arxiv.org/pdf/2509.25381v1
From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models,"Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo","Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition."" We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.",2025-09-29,http://arxiv.org/pdf/2509.25373v1
From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation,"Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Anna Vasileva, Anna Antipina, Tatyana Zaitseva, Alina Ermilova, Evgeny Burnaev, Egor Shvetsov","This paper bridges internal and external analysis approaches to large language models (LLMs) by demonstrating that geometric properties of internal model representations serve as reliable proxies for evaluating generated text quality. We validate a set of metrics including Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms measured across different layers of LLMs, demonstrating that Intrinsic Dimensionality and Effective Rank can serve as universal assessments of text naturalness and quality. Our key finding reveals that different models consistently rank text from various sources in the same order based on these geometric properties, indicating that these metrics reflect inherent text characteristics rather than model-specific artifacts. This allows a reference-free text quality evaluation that does not require human-annotated datasets, offering practical advantages for automated evaluation pipelines.",2025-09-29,http://arxiv.org/pdf/2509.25359v1
Editing Physiological Signals in Videos Using Latent Representations,"Tianwen Zhou, Akshay Paruchuri, Josef Spjut, Kaan Akşit","Camera-based physiological signal estimation provides a non-contact and convenient means to monitor Heart Rate (HR). However, the presence of vital signals in facial videos raises significant privacy concerns, as they can reveal sensitive personal information related to the health and emotional states of an individual. To address this, we propose a learned framework that edits physiological signals in videos while preserving visual fidelity. First, we encode an input video into a latent space via a pretrained 3D Variational Autoencoder (3D VAE), while a target HR prompt is embedded through a frozen text encoder. We fuse them using a set of trainable spatio-temporal layers with Adaptive Layer Normalizations (AdaLN) to capture the strong temporal coherence of remote Photoplethysmography (rPPG) signals. We apply Feature-wise Linear Modulation (FiLM) in the decoder with a fine-tuned output layer to avoid the degradation of physiological signals during reconstruction, enabling accurate physiological modulation in the reconstructed video. Empirical results show that our method preserves visual quality with an average PSNR of 38.96 dB and SSIM of 0.98 on selected datasets, while achieving an average HR modulation error of 10.00 bpm MAE and 10.09% MAPE using a state-of-the-art rPPG estimator. Our design's controllable HR editing is useful for applications such as anonymizing biometric signals in real videos or synthesizing realistic videos with desired vital signs.",2025-09-29,http://arxiv.org/pdf/2509.25348v1
Direct Collapse Black Hole Candidates from Decaying Dark Matter,"Yash Aggarwal, James B. Dent, Philip Tanedo, Tao Xu","Injecting 1-13.6 eV photons into the early universe can suppress the molecular hydrogen abundance and alter the star formation history dramatically enough to produce direct collapse black holes. These, in turn, could explain the recently observed population of puzzling high-redshift supermassive black holes that appear to require super-Eddington accretion. We show that axion dark matter decay in the intergalactic medium can account for this energy injection. We use a single zone model of the gas core and semi-analytically evolve its chemo-thermal properties to track the conditions for which the system becomes an atomic cooling halo-a necessary precursor for the production of heavy black hole seeds to explain the high-redshift black hole population. Windows of axions masses between 24.5-26.5 eV with photon couplings as low as $4\times 10^{-12}$/GeV may realize this atomic cooling halo condition. We highlight the significance of the band structure of molecular hydrogen on the effectiveness of this process and discuss estimates of the heavy seed population and prospects for testing this model.",2025-09-29,http://arxiv.org/pdf/2509.25325v1
Incentive-Aligned Multi-Source LLM Summaries,"Yanchen Jiang, Zhe Feng, Aranyak Mehta","Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source's stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source's incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation.",2025-09-29,http://arxiv.org/pdf/2509.25184v1
A bound-preserving multinumerics scheme for steady-state convection-diffusion equations,Maurice S. Fabien,"We solve the convection-diffusion equation using a coupling of cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into disjoint regions assigned to FV or DG, and the two methods are coupled through an interface term. DG is stable and resolves sharp layers in convection-dominated regimes, but it can produce sizable spurious oscillations and is computationally expensive; FV (two-point flux) is low-order and monotone, but inexpensive. We propose a novel adaptive partitioning strategy that automatically selects FV and DG subdomains: whenever the solution's cell average violates the bounds, we switch to FV on a small neighborhood of that element. Viewed as a natural analog of $p$-adaptivity, this process is repeated until all cell averages are bound-preserving (up to some specified tolerance). Thereafter, standard conservative limiters may be applied to ensure the full solution is bound-preserving. Standard benchmarks confirm the effectiveness of the adaptive technique.",2025-09-29,http://arxiv.org/pdf/2509.25181v1
Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding,"Bingkui Tong, Jiaer Xia, Kaiyang Zhou","Multimodal Large Language Models (MLLMs) have shown impressive perception and reasoning capabilities, yet they often suffer from hallucinations -- generating outputs that are linguistically coherent but inconsistent with the context of the input image, including inaccuracies in objects, attributes, and relations. To address this challenge, we propose a simple approach called Layer Contrastive Decoding (LayerCD). Our design is motivated by the observation that shallow visual features are much more likely than deep visual features to cause an MLLM to hallucinate as they only capture biased, low-level information that is insufficient for high-level reasoning. Therefore, LayerCD aims to filter out hallucinations by contrasting the output distributions generated from visual features of different levels, specifically those from the shallow and deep layers of the vision encoder, respectively. We conduct extensive experiments on two hallucination benchmarks and show that LayerCD significantly outperforms current state-of-the-art. The code for LayerCD is available at https://github.com/maifoundations/LayerCD .",2025-09-29,http://arxiv.org/pdf/2509.25177v1
"Fast, accurate, and precise detector simulation with vision transformers","Luigi Favaro, Andrea Giammanco, Claudius Krause","The speed and fidelity of detector simulations in particle physics pose compelling questions about LHC analysis and future colliders. The sparse high-dimensional data, combined with the required precision, provide a challenging task for modern generative networks. We present a comparison between solutions with different trade-offs, including accurate Conditional Flow Matching and faster coupling-based Normalising Flows. Vision Transformers allows us to emulate the energy deposition from detailed Geant4 simulations. We evaluate the networks using high-level observables, neural network classifiers, and sampling timings, showing minimum deviations from Geant4 while achieving faster generation. We use the CaloChallenge benchmark datasets for reproducibility and further development.",2025-09-29,http://arxiv.org/pdf/2509.25169v1
LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model,"Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue","While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.",2025-09-29,http://arxiv.org/pdf/2509.25304v1
V\textit{z}$-$GAL: Probing Cold Molecular Gas in Dusty Star-forming Galaxies at $\bf \textit{z}=1-6$,"Prachi Prajapati, Dominik Riechers, Pierre Cox, Axel Weiss, Amélie Saintonge, Bethany Jones, Tom J. L. C. Bakx, Stefano Berta, Paul van der Werf, Roberto Neri, Kirsty M. Butler, Asantha Cooray, Diana Ismail, Andrew Baker, Edoardo Borsato, Andrew Harris, Rob Ivison, Matthew Lehnert, Lucia Marchetti, Hugo Messias, Alain Omont, Catherine Vlahakis, Chentao Yang","We present the first results of V\textit{z}-GAL, a high-redshift CO(\textit{J} = 1 $-$ 0) large survey with the Karl G. Jansky Very Large Array, targeting 92 \textit{Herschel}-selected, infrared-luminous, dusty star-forming galaxies (DSFGs). This flux density-limited sample includes 106 DSFGs in total, along with 14 galaxies from a recent pilot study -- altogether doubling the to-date available high-redshift \coonezero observations. These sources cover redshifts 1 to 6 with available mid/high-\textit{J} CO transitions from the Northern Extended Millimeter Array \zgal survey. We detect \coonezero emission in 90/92 targets above signal-to-noise ratio of 2, while two DSFGs remain undetected. \vzgal also covers additional \cotwoone emission lines in 10 of these sources. We find gas masses of the entire \vzgal sample to be $\rm (\alpha_{CO}/{4.0}) \mu {M}_{\rm H_2}$ = $(2-20) \times {10}^{11}~\mathrm{M_{\odot}}$, calibrated using \coonezero line luminosities. We confirm that these DSFGs -- with derived gas depletion timescales of $(50-600)$ Myr -- represent a heterogeneous population, potentially comprising both main-sequence galaxies and starbursts in the early Universe. Using \coonezero luminosities as an anchor, we robustly derive the CO brightness temperature ratios with a good statistical significance up to \textit{J} = 6. Our measurements reveal a broad range of gas excitation conditions across the \vzgal DSFGs. We also explore [CI](1$-$0)/CO(1$-$0) ratios in 23 \vzgal galaxies that have existing detections of atomic carbon ground-state emission, [CI](1$-$0), from the \zgal survey. Our results show similar [CI]/CO ratio values across cosmic time for starbursts and local star-forming galaxy populations, supporting the use of \cionezero as an alternative cold gas tracer.",2025-09-29,http://arxiv.org/pdf/2509.25167v1
High-efficiency Pt$_{75}$Au$_{25}$-based spintronic terahertz emitters,"Wenlu Shi, Gene D. Nelson, Han-Hsuan Wu, Yiwei Ju, Xiaoqing Pan, Wilson Ho, Ilya N. Krivorotov","Spintronic terahertz emitters (STEs) generate broadband THz radiation via ultrafast spin-charge conversion in magnetic multilayers, offering spectral coverage beyond that of photoconductive antennas and nonlinear optical crystals. Here, we demonstrate a new type of STE based on PtxAu100-x alloy that achieves significantly higher THz output power than widely used Pt-based devices. Alloy composition and layer thickness tuning yield Pt75Au25 as the optimal alloy providing a 30 % increase in THz power in CoFeB/Pt75Au25 bilayer STEs compared to the optimized CoFeB/Pt reference STE. In W/CoFeB/Pt$_{75}$Au$_{25}$ trilayer STEs, we observe a 10 % higher THz power than in the optimized W/CoFeB/Pt trilayer. The STE efficiency is reduced upon annealing for both Pt$_{75}$Au$_{25}$- and Pt-based STEs due to formation of interfacial alloys. Our results establish Pt$_{75}$Au$_{25}$ as a promising platform for high-performance STEs, where its giant spin Hall effect significantly enhances efficiency over conventional Pt-based devices.",2025-09-29,http://arxiv.org/pdf/2509.25303v1
High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification,"Nicholas Barnfield, Hugo Cui, Yue M. Lu","When and how can an attention mechanism learn to selectively attend to informative tokens, thereby enabling detection of weak, rare, and sparsely located features? We address these questions theoretically in a sparse-token classification model in which positive samples embed a weak signal vector in a randomly chosen subset of tokens, whereas negative samples are pure noise. In the long-sequence limit, we show that a simple single-layer attention classifier can in principle achieve vanishing test error when the signal strength grows only logarithmically in the sequence length $L$, whereas linear classifiers require $\sqrt{L}$ scaling. Moving from representational power to learnability, we study training at finite $L$ in a high-dimensional regime, where sample size and embedding dimension grow proportionally. We prove that just two gradient updates suffice for the query weight vector of the attention classifier to acquire a nontrivial alignment with the hidden signal, inducing an attention map that selectively amplifies informative tokens. We further derive an exact asymptotic expression for the test error and training loss of the trained attention-based classifier, and quantify its capacity -- the largest dataset size that is typically perfectly separable -- thereby explaining the advantage of adaptive token selection over nonadaptive linear baselines.",2025-09-29,http://arxiv.org/pdf/2509.25153v1
Pretraining Large Language Models with NVFP4,"NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu","Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.",2025-09-29,http://arxiv.org/pdf/2509.25149v1
Momentum-resolved two-dimensional spectroscopy as a probe of nonlinear quantum field dynamics,"Duilio De Santis, Alex Gómez Salvador, Nataliia Bazhan, Sebastian Erne, Maximilian Prüfer, Claudio Guarcello, Davide Valenti, Jörg Schmiedmayer, Eugene Demler","Emergent collective excitations constitute a hallmark of interacting quantum many-body systems, yet in solid-state platforms their study has been largely limited by the constraints of linear-response probes and by finite momentum resolution. We propose to overcome these limitations by combining the spatial resolution of ultracold atomic systems with the nonlinear probing capabilities of two-dimensional spectroscopy (2DS). As a concrete illustration, we analyze momentum-resolved 2DS of the quantum sine-Gordon model describing the low energy dynamics of two weakly coupled one-dimensional Bose-Einstein condensates. This approach reveals distinctive many-body signatures, most notably asymmetric cross-peaks reflecting the interplay between isolated ($B_2$ breather) and continuum ($B_1$ pair) modes. The protocol further enables direct characterization of anharmonicity and disorder, establishing momentum-resolved 2DS as both a powerful diagnostic for quantum simulators and a versatile probe of correlated quantum matter.",2025-09-29,http://arxiv.org/pdf/2509.25147v1
BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression,David González Martínez,"Neural network compression techniques typically require expensive fine-tuning or search procedures, rendering them impractical on commodity hardware. Inspired by recent LLM compression research, we present a general activation-aware factorization framework that can be applied to a broad range of layers. Moreover, we introduce a scalable budgeted rank allocator that allows flexible control over compression targets (e.g., retaining 50% of parameters) with no overhead. Together, these components form BALF, an efficient pipeline for compressing models without fine-tuning. We demonstrate its effectiveness across multiple scales and architectures, from ResNet-20 on CIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it achieves excellent results in the fine-tuning-free regime. For instance, BALF reduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1 accuracy drop.",2025-09-29,http://arxiv.org/pdf/2509.25136v1
LayerD: Decomposing Raster Graphic Designs into Layers,"Tomoyuki Suzuki, Kang-Jun Liu, Naoto Inoue, Kota Yamaguchi","Designers craft and edit graphic designs in a layer representation, but layer-based editing becomes impossible once composited into a raster image. In this work, we propose LayerD, a method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose a simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop a quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing.",2025-09-29,http://arxiv.org/pdf/2509.25134v1
From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones,"Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng","Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",2025-09-29,http://arxiv.org/pdf/2509.25123v1
Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs,"Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna","Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as unstructured graphs, achieving state of the art performance in computer vision tasks such as image classification, object detection, and instance segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by connecting patches (nodes) based on feature similarity, and is dynamically repeated in each ViG layer following GNN based patch (node) feature updates. However, DIGC constitutes over 50% of end to end ViG inference latency, rising to 95% at high image resolutions, making it the dominant computational bottleneck. While hardware acceleration holds promise, prior works primarily optimize graph construction algorithmically, often compromising DIGC flexibility, accuracy, or generality. To address these limitations, we propose a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip buffers that process input features in small, uniform blocks. Our design minimizes external memory traffic via localized computation and performs efficient parallel sorting with local merge sort and global k way merging directly on streaming input blocks via heap insertion. This modular architecture scales seamlessly across image resolutions, ViG layer types, and model sizes and variants, and supports DIGC across diverse ViG based vision backbones. The design achieves high clock frequencies post place and route due to the statically configured parallelism minimizing critical path delay and delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC baselines.",2025-09-29,http://arxiv.org/pdf/2509.25121v1
Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication,"Wai Ming Chan, Remi Chou, Taejoon Kim","This paper introduces the first two-dimensional XOR-based secret sharing scheme for layered multipath communication networks. We present a construction that guarantees successful message recovery and perfect privacy when an adversary observes and disrupts any single path at each transmission layer. The scheme achieves information-theoretic security using only bitwise XOR operations with linear $O(|S|)$ complexity, where $|S|$ is the message length. We provide mathematical proofs demonstrating that the scheme maintains unconditional security regardless of computational resources available to adversaries. Unlike encryption-based approaches vulnerable to quantum computing advances, our construction offers provable security suitable for resource-constrained military environments where computational assumptions may fail.",2025-09-29,http://arxiv.org/pdf/2509.25113v1
HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis,"Yiquan Wang, Tin-Yeh Huang, Qingyun Gao, Jialin Zhang","Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.",2025-09-29,http://arxiv.org/pdf/2509.25112v1
Unsourced Random Access,"Kirill Andreev, Pavel Rybin, Alexey Frolov","Current wireless networks are designed to optimize spectral efficiency for human users, who typically require sustained connections for high-data-rate applications like file transfers and video streaming. However, these networks are increasingly inadequate for the emerging era of machine-type communications (MTC). With a vast number of devices exhibiting sporadic traffic patterns consisting of short packets, the grant-based multiple access procedures utilized by existing networks lead to significant delays and inefficiencies. To address this issue the unsourced random access (URA) paradigm has been proposed. This paradigm assumes the devices to share a common encoder thus simplifying the reception process by eliminating the identification procedure. The URA paradigm not only addresses the computational challenges but it also considers the random access (RA) as a coding problem, i.e., takes into account both medium access protocols and physical layer effects. In this monograph we provide a comprehensive overview of the URA problem in noisy channels, with the main task being to explain the major ideas rather than to list all existing solutions.",2025-09-29,http://arxiv.org/pdf/2509.25074v1
Interstellar Dust-Catalyzed Molecular Hydrogen Formation Enabled by Nuclear Quantum Effects,"Xiaolong Yang, Lile Wang, Di Li, Shenzhen Xu","Molecular hydrogen (H$_2$) is one of the key chemical species that controls and shapes a wide spectrum of astrophysical processes ranging from galaxy evolution to planet formation. Although the catalyzation on dust grain surfaces is considered as the dominant formation channel of H$_2$ in the interstellar medium (ISM), which could nonetheless suffer from the Boltzmann factor suppression at low temperatures. Here we demonstrate that quantum tunneling can dominate the H$_2$ formation process, effectively resolving the long-standing efficiency problem across a wide range of temperatures. By employing the path integral method in hybrid Monte Carlo simulations to account for nuclear quantum effects (NQEs), we quantitatively identify that the tunneling of hydrogen atoms maintains relatively stable efficiencies even at temperatures below 50 K on both graphitic and silicate grain surfaces. The potential barriers associated with chemisorption/desorption and two-H association, rather than diffusion and hopping, are the dominant factors governing the actual reaction efficiency at low temperatures. These findings provide a solid physical foundation for molecule formation, which historically relied on ad-hoc formation rate multipliers to explain observed rates. The quantitative rates also offer new methodologies for observational constraints on H$_2$ formation and destruction, thereby enabling more accurate astrophysical models and interpretations on interstellar molecular materials.",2025-09-29,http://arxiv.org/pdf/2509.25070v1
AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation,"Ryosuke Takanami, Petr Khrapchenkov, Shu Morikuni, Jumpei Arima, Yuta Takaba, Shunsuke Maeda, Takuya Okubo, Genki Sano, Satoshi Sekioka, Aoi Kadoya, Motonari Kambara, Naoya Nishiura, Haruto Suzuki, Takanori Yoshimoto, Koya Sakamoto, Shinnosuke Ono, Hu Yang, Daichi Yashima, Aoi Horo, Tomohiro Motoda, Kensuke Chiyoma, Hiroshi Ito, Koki Fukuda, Akihito Goto, Kazumi Morinaga, Yuya Ikeda, Riko Kawada, Masaki Yoshikawa, Norio Kosuge, Yuki Noguchi, Kei Ota, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo, Tetsuya Ogata","As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",2025-09-29,http://arxiv.org/pdf/2509.25032v1
Generalization of Variadic Structures with Binders: A Tool for Structural Code Comparison,"Alexander Baumgartner, Temur Kutsia","This paper introduces a novel anti-unification algorithm for the generalization of variadic structures with binders, designed as a flexible tool for structural code comparison. By combining nominal techniques for handling variable binding with support for variadic expressions (common in abstract syntax trees and programming languages), the approach addresses key challenges such as overemphasis on bound variable names and difficulty handling insertions or deletions in code fragments. The algorithm distinguishes between atoms and two kinds of variables (term and hedge variables) to compute best generalizations that maximally preserve structural similarities while abstracting systematic differences. It also provides detailed information to reconstruct original expressions and quantify structural differences. This information can be useful in tasks like code clone detection, refactoring, and program analysis. By introducing a parametrizable rigidity function, the technique offers fine-grained control over similarity criteria and reduces nondeterminism, enabling flexible adaptation to practical scenarios where trivial similarities should be discounted. Although demonstrated primarily in the context of code similarity detection, this framework is broadly applicable wherever precise comparison of variadic and binder-rich representations is required.",2025-09-29,http://arxiv.org/pdf/2509.25023v1
Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting,"Spyros Kondylatos, Gustau Camps-Valls, Ioannis Papoutsis","Wildfires are among the most severe natural hazards, posing a significant threat to both humans and natural ecosystems. The growing risk of wildfires increases the demand for forecasting models that are not only accurate but also reliable. Deep Learning (DL) has shown promise in predicting wildfire danger; however, its adoption is hindered by concerns over the reliability of its predictions, some of which stem from the lack of uncertainty quantification. To address this challenge, we present an uncertainty-aware DL framework that jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance short-term wildfire danger forecasting. In the next-day forecasting, our best-performing model improves the F1 Score by 2.3% and reduces the Expected Calibration Error by 2.1% compared to a deterministic baseline, enhancing both predictive skill and calibration. Our experiments confirm the reliability of the uncertainty estimates and illustrate their practical utility for decision support, including the identification of uncertainty thresholds for rejecting low-confidence predictions and the generation of well-calibrated wildfire danger maps with accompanying uncertainty layers. Extending the forecast horizon up to ten days, we observe that aleatoric uncertainty increases with time, showing greater variability in environmental conditions, while epistemic uncertainty remains stable. Finally, we show that although the two uncertainty types may be redundant in low-uncertainty cases, they provide complementary insights under more challenging conditions, underscoring the value of their joint modeling for robust wildfire danger prediction. In summary, our approach significantly improves the accuracy and reliability of wildfire danger forecasting, advancing the development of trustworthy wildfire DL systems.",2025-09-29,http://arxiv.org/pdf/2509.25017v1
Unified laboratory-frame analysis of atomic gravitational-wave sensors,"Simon Schaffrath, Daniel Störk, Fabio Di Pumpo, Enno Giese","Atomic sensors using light-matter interactions, in particular atomic clocks and atom interferometers, have the potential to complement optical gravitational-wave detectors in the mid-frequency regime. Although both rely on interference, the interfering components of clocks are spatially colocated, whereas atom interferometers are based on spatial superpositions. Both the electromagnetic fields that drive the transitions and generate superpositions, while propagating through spacetime, as well as the atoms themselves as massive particles are influenced by gravitational waves, leading to effective potentials that induce phase differences inferred by the sensor. In this work, we analyze the effects of these potentials on atomic clocks and atom interferometers in the laboratory frame. We show that spatial superpositions in atom interferometers, both light-pulse and guided ones, give rise to a gravitational-wave signal. Although these spatial superpositions are suppressed for clocks, we show that the light pulses driving internal transitions measure the spatial distance between the centers of two separate clocks. We highlight that this mechanism only yields a sensitivity if both clocks, including possible trapping setups, move on geodesics given by the gravitational wave. While such configurations are natural for satellite free-fliers, terrestrial optical clocks normally rely on stationary traps, rendering them insensitive to leading order. Moreover, we show that both sensors can be enhanced by composite interrogation protocols in a common framework. To this end, we propose a pulse sequence that can be used for large-momentum-transfer atom interferometers and for hyper-echo atomic clocks, leading to a signal enhancement and noise suppression.",2025-09-29,http://arxiv.org/pdf/2509.24993v1
Intra-request branch orchestration for efficient LLM reasoning,"Weifan Jiang, Rana Shahout, Yilun Du, Michael Mitzenmacher, Minlan Yu","Large Language Models (LLMs) increasingly rely on inference-time reasoning algorithms such as chain-of-thought and multi-branch reasoning to improve accuracy on complex tasks. These methods, however, substantially increase token usage and per-request latency. Prior work has largely focused on reducing token usage, often at the expense of accuracy, while overlooking other latency factors. We present DUCHESS, an LLM serving system that reduces cost and latency without sacrificing accuracy through intra-request branch orchestration guided by predictions. DUCHESS employs a lightweight linear probing model over LLM layer activations to estimate branch correctness, and its orchestration policy decides whether to terminate, duplicate, or continue a branch. When handling multiple requests, DUCHESS further reduces latency by prioritizing easier reasoning tasks when complexity can be estimated from the prompt. Experiments on three reasoning benchmarks show that DUCHESS consistently improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with First-Come-First-Served scheduling, and achieves additional gains under difficulty-aware scheduling at higher request rates.",2025-09-29,http://arxiv.org/pdf/2509.24957v1
Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks,"Tereza Burianová, Martin Perešíni, Ivan Homoliak","Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern due to the risk of targeted attacks such as malicious denial-of-service (DoS) and censorship attacks. While several Secret Single Leader Election (SSLE) mechanisms have been proposed to address these threats, their practical impact and trade-offs remain insufficiently explored. In this work, we present a unified experimental framework for evaluating SSLE mechanisms under adversarial conditions, grounded in a simplified yet representative model of Ethereum's PoS consensus layer. The framework includes configurable adversaries capable of launching targeted DoS and censorship attacks, including coordinated strategies that simultaneously compromise groups of validators. We simulate and compare key protection mechanisms - Whisk, and homomorphic sortition. To the best of our knowledge, this is the first comparative study to examine adversarial DoS scenarios involving multiple attackers under diverse protection mechanisms. Our results show that while both designs offer strong protection against targeted DoS attacks on the leader, neither defends effectively against coordinated attacks on validator groups. Moreover, Whisk simplifies a DoS attack by narrowing the target set from all validators to a smaller list of known candidates. Homomorphic sortition, despite its theoretical strength, remains impractical due to the complexity of cryptographic operations over large validator sets.",2025-09-29,http://arxiv.org/pdf/2509.24955v1
Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer,"Sooraj Sathish, Keshav Goyal, Raghuram Bharadwaj Diddigi","Deep Reinforcement Learning (RL) has demonstrated success in solving complex sequential decision-making problems by integrating neural networks with the RL framework. However, training deep RL models poses several challenges, such as the need for extensive hyperparameter tuning and high computational costs. Transfer learning has emerged as a promising strategy to address these challenges by enabling the reuse of knowledge from previously learned tasks for new, related tasks. This avoids the need for retraining models entirely from scratch. A commonly used approach for transfer learning in RL is to leverage the internal representations learned by the neural network during training. Specifically, the activations from the last hidden layer can be viewed as refined state representations that encapsulate the essential features of the input. In this work, we investigate whether these representations can be used as input for training simpler models, such as linear function approximators, on new tasks. We observe that the representations learned by standard deep RL models can be highly correlated, which limits their effectiveness when used with linear function approximation. To mitigate this problem, we propose a novel deep Q-learning approach that introduces a regularization term to reduce positive correlations between feature representation of states. By leveraging these reduced correlated features, we enable more effective use of linear function approximation in transfer learning. Through experiments and ablation studies on standard RL benchmarks and MinAtar games, we demonstrate the efficacy of our approach in improving transfer learning performance and thereby reducing computational overhead.",2025-09-29,http://arxiv.org/pdf/2509.24947v1
Scalable GANs with Transformers,"Sangeek Hyun, MinKyu Lee, Jae-Pil Heo","Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",2025-09-29,http://arxiv.org/pdf/2509.24935v1
Impact of Atomic Substitution on Core-Hole Relaxation Dynamics: A Study of Br$_2$ and IBr,"Nivedita Bhat, Yeonsig Nam, Linda Young, Stephen H. Southworth, Phay J. Ho","Understanding inner-shell decay processes in heavy-element molecules is essential for unraveling x-ray-induced photodynamics and advancing molecular imaging techniques. In this study, we investigate the influence of atomic substitution on core-hole relaxation dynamics and molecular fragmentation in Br2 and IBr, initiated by x-ray absorption at the Br K-edge. Using a combination of X-ray/ion coincidence measurements and Monte Carlo/molecular dynamics simulations, we track charge distribution and the kinetic energy release (KER) of fragment ions with a total charge from 2+ to 8+. For both molecules, the simulated KER values show good agreement with experiment across different fragmentation channels. Our comparison reveals that substituting Br with the heavier I atom in IBr has minimal impact on the inner-shell electronic decay process, but significantly influences nuclear motion, leading to slower dissociation, thereby a KER close to the Coulomb limit, an effect attributed to the atomic mass. These findings highlight the interplay between electronic and nuclear effects in molecular fragmentation, particularly in heavy-element species, and provide new insights into medical therapies, structural biology, and astrophysics.",2025-09-29,http://arxiv.org/pdf/2509.24915v1
Inductive Bias and Spectral Properties of Single-Head Attention in High Dimensions,"Fabrizio Boncoraglio, Vittorio Erba, Emanuele Troiani, Florent Krzakala, Lenka Zdeborová","We study empirical risk minimization in a single-head tied-attention layer trained on synthetic high-dimensional sequence tasks, given by the recently introduced attention-indexed model. Using tools from random matrix theory, spin-glass physics, and approximate message passing, we derive sharp asymptotics for training and test errors, locate interpolation and recovery thresholds, and characterize the limiting spectral distribution of the learned weights. Weight decay induces an implicit nuclear-norm regularization, favoring low-rank query and key matrices. Leveraging this, we compare the standard factorized training of query and key matrices with a direct parameterization in which their product is trained element-wise, revealing the inductive bias introduced by the factorized form. Remarkably, the predicted spectral distribution echoes empirical trends reported in large-scale transformers, offering a theoretical perspective consistent with these phenomena.",2025-09-29,http://arxiv.org/pdf/2509.24914v1
Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer,"Mohsen Ghafoorian, Denis Korzhenkov, Amirhossein Habibian","Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \textit{Attention Surgery}, an efficient framework for \textit{linearizing} or \textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40\% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks.",2025-09-29,http://arxiv.org/pdf/2509.24899v1
Towards Understanding the Shape of Representations in Protein Language Models,"Kosio Beshkov, Anders Malthe-Sørenssen","While protein language models (PLMs) are one of the most promising avenues of research for future de novo protein design, the way in which they transform sequences to hidden representations, as well as the information encoded in such representations is yet to be fully understood. Several works have attempted to propose interpretability tools for PLMs, but they have focused on understanding how individual sequences are transformed by such models. Therefore, the way in which PLMs transform the whole space of sequences along with their relations is still unknown. In this work we attempt to understand this transformed space of sequences by identifying protein structure and representation with square-root velocity (SRV) representations and graph filtrations. Both approaches naturally lead to a metric space in which pairs of proteins or protein representations can be compared with each other.   We analyze different types of proteins from the SCOP dataset and show that the Karcher mean and effective dimension of the SRV shape space follow a non-linear pattern as a function of the layers in ESM2 models of different sizes. Furthermore, we use graph filtrations as a tool to study the context lengths at which models encode the structural features of proteins. We find that PLMs preferentially encode immediate as well as local relations between residues, but start to degrade for larger context lengths. The most structurally faithful encoding tends to occur close to, but before the last layer of the models, indicating that training a folding model ontop of these layers might lead to improved folding performance.",2025-09-29,http://arxiv.org/pdf/2509.24895v1
DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning,"Jiayi Li, Flora D. Salim","Learning PDE dynamics with neural solvers can significantly improve wall-clock efficiency and accuracy compared with classical numerical solvers. In recent years, foundation models for PDEs have largely adopted multi-scale windowed self-attention, with the scOT backbone in \textsc{Poseidon} serving as a representative example.   However, because of their locality, truly globally consistent spectral coupling can only be propagated gradually through deep stacking and window shifting. This weakens global coupling and leads to error accumulation and drift during closed-loop rollouts. To address this, we propose \textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral branch and an image branch. The spectral branch is responsible for capturing global, large-scale low-frequency information, whereas the image branch focuses on local details and nonstationary structures. Specifically, we first perform controlled, lightweight mixing within the low-frequency range. Then we fuse the spectral and image paths at each layer via bandwise weighting, which avoids the width inflation and training instability caused by naive concatenation. The fused result is transformed back into the spatial domain and added to the image branch, thereby preserving both global structure and high-frequency details across scales. Compared with strong attention-based baselines, DRIFT-Net achieves lower error and higher throughput with fewer parameters under identical training settings and budget. On Navier--Stokes benchmarks, the relative $L_{1}$ error is reduced by 7\%--54\%, the parameter count decreases by about 15\%, and the throughput remains higher than scOT. Ablation studies and theoretical analyses further demonstrate the stability and effectiveness of this design. The code is available at https://github.com/cruiseresearchgroup/DRIFT-Net.",2025-09-29,http://arxiv.org/pdf/2509.24868v1
Anisotropy by design in superconducting Nb thin films via ultrashort pulse laser irradiation,"Javier Frechilla, Nicolas Lejeune, Elena Martínez, Emile Fourneau, Alejandro Frechilla, Sergio Martín, Leonardo R Cadorim, Luis A Angurel, Germán F de la Fuente, Alejandro V Silhanek, Milorad V Milosevic, Antonio Badía-Majós","The ability to fabricate anisotropic superconducting layers a la carte is desired in technologies such as fluxon screening or removal in field-resilient devices, flux lensing in ultra-sensitive sensors, or in templates for imprinting magnetic structures in hybrid magnetic/superconducting multilayers. In this work, we demonstrate tailored superconductivity in polycrystalline niobium thin films exposed to femtosecond ultraviolet laser pulses. The samples exhibit significant changes in their superconducting properties, directly connected with the observed topography, crystallite geometry, and lattice parameter modifications. On the mesoscopic scale, quasi-parallel periodic ripple structures (about 260 nm of spatial period) gradually form on the film surface by progressively increasing the laser energy per pulse, Ep. This gives way to a stepwise increase of the critical current anisotropy and magnetic flux channeling effects along the ripples. As demonstrated in our resistive and inductive measurements, these superstructures determine the electromagnetic response of the sample within the regime dominated by flux-pinning. Time-dependent Ginzburg-Landau simulations corroborate the topographical origin of the customized anisotropy. Concurrently, intrinsic superconducting parameters (critical field and temperature) are moderately and isotropically depressed upon increasing Ep, as is the lattice parameter of Nb. These findings promote pulsed laser processing as a flexible, one-step, and scalable lithography-free technique for versatile surface functionalization in microelectronic superconducting technology.",2025-09-29,http://arxiv.org/pdf/2509.24862v1
DelRec: learning delays in recurrent spiking neural networks,"Alexandre Queant, Ulysse Rançon, Benoit R Cottereau, Timothée Masquelier","Spiking neural networks (SNNs) are a bio-inspired alternative to conventional real-valued deep learning models, with the potential for substantially higher energy efficiency. Interest in SNNs has recently exploded due to a major breakthrough: surrogate gradient learning (SGL), which allows training SNNs with backpropagation, strongly outperforming other approaches. In SNNs, each synapse is characterized not only by a weight but also by a transmission delay. While theoretical works have long suggested that trainable delays significantly enhance expressivity, practical methods for learning them have only recently emerged. Here, we introduce ''DelRec'', the first SGL-based method to train axonal or synaptic delays in recurrent spiking layers, compatible with any spiking neuron model. DelRec leverages a differentiable interpolation technique to handle non-integer delays with well-defined gradients at training time. We show that trainable recurrent delays outperform feedforward ones, leading to new state-of-the-art (SOTA) on two challenging temporal datasets (Spiking Speech Command, an audio dataset, and Permuted Sequential MNIST, a vision one), and match the SOTA on the now saturated Spiking Heidelberg Digit dataset using only vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous) synapses. Our results demonstrate that recurrent delays are critical for temporal processing in SNNs and can be effectively optimized with DelRec, paving the way for efficient deployment on neuromorphic hardware with programmable delays. Our code is available at : https://github.com/alexmaxad/DelRec.",2025-09-29,http://arxiv.org/pdf/2509.24852v1
Physical Layer Security over Fluid Reconfigurable Intelligent Surface-assisted Communication Systems,"Masoud Kaveh, Farshad Rostami Ghadi, Francisco Hernando-Gallego, Diego Martín, Kai-Kit Wong, Riku Jäntti","This letter investigates the secrecy performance of wireless communication systems assisted by a fluid reconfigurable intelligent surface (FRIS). Unlike conventional reconfigurable intelligent surfaces (RISs) with fixed geometries, FRISs dynamically select a subset of reflective elements based on real-time channel conditions, offering enhanced spatial diversity and adaptability. Using this foundation, we model a secure downlink scenario where a base station communicates with a legitimate user in the presence of an eavesdropper, and the propagation is assisted by a FRIS with a limited number of elements set to the ON state. We analyze the system's secrecy performance under spatial correlation by deriving analytical lower and upper bounds for the secrecy outage probability (SOP) and average secrecy capacity (ASC), respectively. Our results demonstrate that FRIS effectively enables secure communication under spatial correlation. Even with partial activation, FRIS significantly outperforms conventional RISs in enhancing secrecy performance under varying deployment densities and element correlations.",2025-09-29,http://arxiv.org/pdf/2509.24845v1
Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement,"Zhilong Zhao, Yindi Liu","Large Language Models face significant performance challenges in specialized domains, with state-of-the-art models achieving only 45.9% accuracy on medical coding tasks. This study proposes a Hierarchical Error Correction (HEC) framework that addresses domain-specific AI limitations through systematic error analysis and targeted intervention strategies.   We analyze error patterns across four specialized domains and find that AI errors follow consistent hierarchical structures: Knowledge-layer errors (58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%). Based on these patterns, we develop a three-stage correction framework that addresses errors according to their hierarchical importance and demonstrates that framework effectiveness correlates inversely with baseline task performance.   Experimental validation across medical transcription (4,921 cases), legal document classification (1,000 cases), political bias detection (645 cases), and legal reasoning (1,000 cases) shows consistent improvements. Cross-model validation across five LLM architectures demonstrates average improvements of 11.2 percentage points (p < 0.001). However, analysis reveals framework limitations in high-baseline tasks (>75% accuracy), where hierarchical intervention may interfere with effective reasoning processes.   The results suggest that systematic error analysis can guide effective AI enhancement strategies in specialized domains, particularly for moderate-baseline tasks, while highlighting the importance of understanding framework boundaries for optimal deployment.",2025-09-29,http://arxiv.org/pdf/2509.24841v1
Ligand co-deposition in focused electron beam induced nanoprinting: a predictive composition model,"Jakub Jurczyk, Leo Brockhuis, Amalio Fernández-Pacheco, Ivo Utke","Recent advances in nanotechnology have created the need to manufacture three-dimensional nanostructures with controlled material composition. Focused Electron Beam Induced Deposition (FEBID) is a nanoprinting technique offering highest spatial resolution combined with the ability to directly 3D-print almost any shape. It relies on local electron-induced dissociation of metal-ligand organometallic molecules adsorbed onto a substrate. So far FEBID continuum modelling involved the surface kinetics of precursor molecules during electron irradiation and succeeded in the prediction of nanoprint shape and growth rate and forms nowadays the basis of software for 3D nano-printing of nanostructures. Here, we expand the model to the surface kinetics of detached ligands. Involving their dissociation and desorption behavior allows to predict trends in the metallic composition of the nanoprinted material and to define desirable nanoprint process windows as function of electron exposure time and flux. We present the theoretical foundations of the model, validate it experimentally for chromium and silver precursors, compare calculated values with literature data for various precursors, and discuss its potential to design new experiments. This contribution enhances our understanding of FEBID dynamics and provides a versatile framework for predictive FEBID material nano-printing.",2025-09-29,http://arxiv.org/pdf/2509.24838v1
Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models,"Youngeun Kim, Youjia Zhang, Huiling Liu, Aecheon Jung, Sunwoo Lee, Sungeun Hong","Large Vision-Language Models (VLMs) enable strong multimodal reasoning but incur heavy inference costs from redundant visual tokens. Token pruning alleviates this issue, yet existing approaches face limitations. Attention-based methods rely on raw attention scores, which are often unstable across layers and heads and can lead to redundant selections. Diversity-based methods improve robustness by selecting tokens far apart in feature space but risk dropping regions needed for accurate prediction. We propose \ours, a training-free framework built on a simple intuition: tokens with higher sensitivity are more likely to influence the model's output, and they should also capture complementary visual cues rather than overlapping information. To achieve this, we estimate token sensitivity using zeroth-order perturbations at the projection layer, a shallow and computationally light component of the model. This approach measures how small random perturbations affect the projection outputs, allowing us to approximate each token's influence through lightweight forward passes without backpropagation. Extensive experiments across multiple VLMs and benchmarks show that \ours consistently outperforms prior methods, pruning up to 94.4\% of tokens while maintaining accuracy and significantly improving efficiency, achieving up to 2.30x faster end-to-end inference over the baseline.",2025-09-29,http://arxiv.org/pdf/2509.24837v1
Turbulence and dust fragility in protoplanetary discs,"Simin Tong, Richard Alexander, Giovanni Rosotti","Dust growth from micron- to planet-size in protoplanetary discs involves multiple physical processes, including dust growth and fragmentation, the streaming instability, and pebble accretion. Disc turbulence and dust fragility matter at almost every stage. Previous studies typically vary one of them while fixing the other, failing to provide a complete picture. Here, we use analytical models and numerical dust evolution models DustPy to study the combinations of gas turbulence and dust fragility that can reproduce multi-wavelength ALMA observables. We find only appropriate combinations -- fragile dust (fragmentation velocity $v_\mathrm{frag}$= 1-2 m/s) in discs with viscous $\alpha=10^{-4}$ or resilient dust ($v_\mathrm{frag}$= 6-10 m/s) in discs with viscous $\alpha=10^{-3}$ -- can reproduce observations. Our result is robust to two widely used opacities (DSHARP and Ricci opacities). Regardless of the strength of disc turbulence, reproducing observations requires observed dust rings to be optically thick at $\lambda=1.3$ and $3$ mm. As only small dust can be lifted above the midplane to reach the emitting layers, SED analysis probably yields lower limits on the maximum grain sizes. We highlight the challenge of creating detectable dust rings at large radii when incorporating bouncing in models, and the need for earlier formation of dust rings at smaller radii to reproduce the decreasing ring brightness with radius observed across ALMA wavelengths.",2025-09-29,http://arxiv.org/pdf/2509.24818v1
TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models,"Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan","Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.",2025-09-29,http://arxiv.org/pdf/2509.24803v1
DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting,"Zixu Wang, Hongbin Dong, Xiaoping Zhang","Time series forecasting is crucial for various applications, such as weather, traffic, electricity, and energy predictions. Currently, common time series forecasting methods are based on Transformers. However, existing approaches primarily model limited time series or fixed scales, making it more challenging to capture diverse features cross different ranges. Additionally, traditional methods like STL for complex seasonality-trend decomposition require pre-specified seasonal periods and typically handle only single, fixed seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive Transformer (DSAT-HD), which integrates three key innovations to address the limitations of existing methods: 1) A hybrid decomposition mechanism combining EMA and Fourier decomposition with RevIN normalization, dynamically balancing seasonal and trend components through noise Top-k gating; 2) A multi-scale adaptive pathway leveraging a sparse allocator to route features to four parallel Transformer layers, followed by feature merging via a sparse combiner, enhanced by hybrid attention combining local CNNs and global interactions; 3) A dual-stream residual learning framework where CNN and MLP branches separately process seasonal and trend components, coordinated by a balanced loss function minimizing expert collaboration variance. Extensive experiments on nine datasets demonstrate that DSAT-HD outperforms existing methods overall and achieves state-of-the-art performance on some datasets. Notably, it also exhibits stronger generalization capabilities across various transfer scenarios.",2025-09-29,http://arxiv.org/pdf/2509.24800v1
Vision Function Layer in Multimodal LLMs,"Cheng Shi, Yizhou Yu, Sibei Yang","This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.",2025-09-29,http://arxiv.org/pdf/2509.24791v1
VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning,"Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song","Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models.",2025-09-29,http://arxiv.org/pdf/2509.24773v2
A Quantum Computer Based on Donor-Cluster Arrays in Silicon,"Shihang Zhang, Chunhui Zhang, Guanyong Wang, Tao Xin, Guangchong Hu, Yu He, Peihao Huang","Significant advances in silicon spin qubits highlight the potential of silicon quantum dots for scalable quantum computing, given their compatibility with industrial fabrication and long coherence times. In particular, phosphorus (P)-doped spin qubits possess excellent coherence and have demonstrated high-fidelity two-qubit gates exceeding 99.9%. However, scaling P-donor systems is challenging due to crosstalk caused by the uniformity of individual P donors and the low tolerance for imprecise atomic placement. Stochastic placement can lead to multiple donors located within a small region (diameter <3 nm), forming a so-called donor cluster. Notably, in cluster-based systems, high-fidelity multi-qubit quantum gates and all-to-all connectivity have recently been demonstrated experimentally on nuclear spin qubits. In this work, we propose a scalable cluster-array architecture for nuclear spin qubits and a corresponding control protocol. We analyze crosstalk-induced errors, a major error source, during primitive operations under various parameters, showing that they can be suppressed through device design and control optimization. We evaluate the fidelities of intra- and inter-cluster multi-qubit gates between nuclear spins, confirming the feasibility of our architecture and establishing design requirements and parameter targets. The local all-to-all connectivity within clusters provides unique flexibility for quantum error correction. Our scalable scheme provides a path toward large-scale spin-based quantum processors.",2025-09-29,http://arxiv.org/pdf/2509.24749v1
Symmetry-preserving random batch Ewald method for constant-potential simulation of electrochemical systems,"Weihang Gao, Qi Zhou, Qianru Zhang, Zhenli Xu","Constant potential molecular dynamics simulation plays important role for applications of electrochemical systems, yet the calculation of charge fluctuation on electrodes remains a computational bottleneck. We propose a highly scalable, symmetry-preserving random batch Ewald (SRBE) algorithm to address this challenge. The SRBE algorithm deterministically computes the low-frequency components along the direction perpendicular to electrodes, while efficiently approximating the remaining components using random batch sampling. This approach simultaneously reduces charge and force fluctuations while satisfying the symmetry-preserving mean field condition in anisotropic systems with large aspect ratios. Numerical experiments on electrode/ionic liquid systems validate the high accuracy of the SRBE method in capturing dynamic charging processes and equilibrium electric double layer structures. The SRBE method achieves parallel efficiency improvements of up to two orders of magnitude compared with conventional FFT-based algorithms. These findings highlight its strong potential for enabling large-scale electrochemical simulations and its broad applicability to practical problems in the field.",2025-09-29,http://arxiv.org/pdf/2509.24742v1
A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity,"Giordano Cicchetti, Eleonora Grassucci, Danilo Comminiello","Multimodal learning plays a pivotal role in advancing artificial intelligence systems by incorporating information from multiple modalities to build a more comprehensive representation. Despite its importance, current state-of-the-art models still suffer from severe limitations that prevent the successful development of a fully multimodal model. Such methods may not provide indicators that all the involved modalities are effectively aligned. As a result, some modalities may not be aligned, undermining the effectiveness of the model in downstream tasks where multiple modalities should provide additional information that the model fails to exploit. In this paper, we present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed similarity measure that is directly computed in the higher-dimensional space spanned by the modality embeddings. TRIANGLE improves the joint alignment of three modalities via a triangle-area similarity, avoiding additional fusion layers or pairwise similarities. When incorporated in contrastive losses replacing cosine similarity, TRIANGLE significantly boosts the performance of multimodal modeling, while yielding interpretable alignment rationales. Extensive evaluation in three-modal tasks such as video-text and audio-text retrieval or audio-video classification, demonstrates that TRIANGLE achieves state-of-the-art results across different datasets improving the performance of cosine-based methods up to 9 points of Recall@1.",2025-09-29,http://arxiv.org/pdf/2509.24734v1
Non-destructive optical read-out and manipulation of circular Rydberg atoms,"Yohann Machu, Andrés Durán-Hernández, Gautier Creutzer, Aurore Alice Young, Jean-Michel Raimond, Michel Brune, Clément Sayrin","Among the thriving quantum computation and quantum simulation platforms based on arrays of Rydberg atoms, those using circular Rydberg atoms are particularly promising. These atoms uniquely combine the strong dipole-dipole interactions typical of Rydberg states with long lifetimes. However, low-angular-momentum ($\ell$) laser-accessible Rydberg levels have been so far mostly used, because circular Rydberg atoms have no optical transitions, hindering their individual detection and manipulation. We remove this limitation with a hybrid platform, combining an array of logical laser-trapped circular Rydberg atoms of rubidium with an auxiliary array of Rb ancilla atoms transiently excited to a low-$\ell$ Rydberg level. We perform a quantum non-demolition detection of the logical qubit with the ancilla, through the blockade of the ancilla optical excitation induced by a F\""orster resonance. Conversely, we locally manipulate the logical qubit through the excitation of the ancilla. This dual-Rydberg platform is highly promising for quantum computation and simulation. It adds to the circular-atom toolbox the mid-circuit measurements, essential for error correction. More strikingly, it gives access to time correlations in long-term quantum simulations, uniquely accessible to circular Rydberg atoms.",2025-09-29,http://arxiv.org/pdf/2509.24691v1
Robust Majorana Platform Driven by a Meissner-Induced Anisotropic Doppler Shift,"Xiao-Hong Pan, Si-Qi Yu, Li Chen, Fu-Chun Zhang, Xin Liu","The realization of robust Majorana zero modes (MZMs), a cornerstone for fault-tolerant quantum computing, is hindered by the challenge of creating a platform that simultaneously offers a large topological gap, high tunability, and resilience to disorder. A system unifying these properties has remained elusive. Here, we propose and validate a novel platform that harnesses the Meissner effect in a topological insulator (TI) nanowire partially covered by a superconducting (SC) layer. Under an external magnetic field, Meissner screening currents in the SC induce a spatially varying Doppler shift on the TI surface. This effect generates a highly anisotropic effective g-factor, which selectively drives a topological phase transition localized on the nanowire's bottom surface. This mechanism is crucial as it spatially separates the topological phase from the SC/TI interface, permitting strong proximity-induced superconductivity while preventing detrimental band renormalization at the interface from closing the topological gap. Furthermore, by confining the topological superconducting phase to the gate-tunable bottom surface, our platform fully leverages the intrinsic disorder resilience of the TI's topologically protected surface states. Through a combination of supercurrent simulations, self-consistent Schr\""odinger-Poisson calculations, and large-scale tight-binding computations, we validate the platform's robustness. Our work establishes a practical pathway toward Meissner-mediated topological superconductivity for realizing robust MZMs in SC/TI hybrid systems.",2025-09-29,http://arxiv.org/pdf/2509.24686v1
Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation,"Hanyu Zhang, Yiming Zhou, Jinxia Zhang","Open-vocabulary camouflaged object segmentation requires models to segment camouflaged objects of arbitrary categories unseen during training, placing extremely high demands on generalization capabilities. Through analysis of existing methods, it is observed that the classification component significantly affects overall segmentation performance. Accordingly, a classifier-centric adaptive framework is proposed to enhance segmentation performance by improving the classification component via a lightweight text adapter with a novel layered asymmetric initialization. Through the classification enhancement, the proposed method achieves substantial improvements in segmentation metrics compared to the OVCoser baseline on the OVCamo benchmark: cIoU increases from 0.443 to 0.493, cSm from 0.579 to 0.658, and cMAE reduces from 0.336 to 0.239. These results demonstrate that targeted classification enhancement provides an effective approach for advancing camouflaged object segmentation performance.",2025-09-29,http://arxiv.org/pdf/2509.24681v1
A classification of mode-1 internal solitary waves in a three-layer fluid,"Ricardo Barros, Alex Doak, Wooyoung Choi, Paul Milewski","We explore the bifurcation structure of mode-1 solitary waves in a three-layer fluid confined between two rigid boundaries. A recent study (Lamb, J. Fluid Mech. 2023, 962, A17) proposed a method to predict the coexistence of solitary waves with opposite polarity in a continuously stratified fluid with a double pycnocline by examining the conjugate states for the Euler equations. We extend this line of inquiry to a piecewise-constant three-layer stratification, taking advantage of the fact that the conjugate states for the Euler equations are exactly preserved by the strongly nonlinear model that we will refer to as the three-layer Miyata-Maltseva-Choi-Camassa (MMCC3) equations. In this reduced setting, solitary waves are governed by a Hamiltonian system with two degrees of freedom, whose critical points are used to explain the bifurcation structure. Through this analysis, we also discover families of solutions that have not been previously reported. Using the shared conjugate state structure between the MMCC3 model and the full Euler equations, we propose criteria for distinguishing the full range of solution behaviours. This alignment between the reduced and full models provides strong evidence that partitioning the parameter space into regions associated with distinct solution types is valid within both theories. This classification is further substantiated by numerical solutions to both models, which show excellent agreement.",2025-09-29,http://arxiv.org/pdf/2509.24669v1
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,"Max van Spengler, Artem Moskalev, Tommaso Mansi, Mangal Prakash, Rui Liao","Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.",2025-09-29,http://arxiv.org/pdf/2509.24655v1
SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving,"Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng","Serving long-context LLMs is costly because attention computation grows linearly with context length. Dynamic sparse attention algorithms (DSAs) mitigate this by attending only to the key-value (KV) cache of critical tokens. However, with DSAs, the main performance bottleneck shifts from HBM bandwidth to HBM capacity: KV caches for unselected tokens must remain in HBM for low-latency decoding, constraining parallel batch size and stalling further throughput gains. Offloading these underutilized KV caches to DRAM could free HBM capacity, allowing larger parallel batch sizes. Yet, achieving such hierarchical HBM-DRAM storage raises new challenges, including fragmented KV cache access, HBM cache contention, and high HBM demands of hybrid batching, that remain unresolved in prior work.   This paper proposes SparseServe, an LLM serving system that unlocks the parallel potential of DSAs through efficient hierarchical HBM-DRAM management. SparseServe introduces three key innovations to address the challenges mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch sizes based on real-time working set estimation to minimize HBM cache thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a single layer, enabling efficient execution even for long prompts. Extensive experimental results demonstrate that SparseServe achieves up to 9.26x lower mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation throughput compared to state-of-the-art LLM serving systems.",2025-09-29,http://arxiv.org/pdf/2509.24626v1
FreeRet: MLLMs as Training-Free Retrievers,"Yuhan Zhu, Xiangyu Zeng, Chenting Wang, Xinhao Li, Yicheng Xu, Ziang Yan, Yi Wang, Limin Wang","Multimodal large language models (MLLMs) are emerging as versatile foundations for mixed-modality retrieval. Yet, they often require heavy post-hoc training to convert them into contrastive encoders for retrieval. This work asks: Can off-the-shelf MLLMs serve as powerful retrievers without additional training? We present FreeRet, a plug-and-play framework that turns any MLLM into a two-stage retriever. FreeRet first derives semantically grounded embeddings directly from the model for fast candidate search, and then exploits its reasoning ability for precise reranking. The framework contributes three advances: bypassing lexical alignment layers to obtain semantically faithful embeddings, conditioning representation generation with explicit priors, and mitigating framing effect in reranking via neutral choice framing. On the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially outperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is model-agnostic and scales seamlessly across MLLM families and sizes, preserves their generative abilities, supports arbitrary modality combinations, and unifies retrieval, reranking, and generation into end-to-end RAG within a single model. Our findings demonstrate that pretrained MLLMs, when carefully harnessed, can serve as strong retrieval engines without training, closing a critical gap in their role as generalists.",2025-09-29,http://arxiv.org/pdf/2509.24621v1
Precision spectroscopy in Yb+ ions,"Jialiang Yu, Kai C. Grensemann, Chih-Han Yeh, Ikbal Ahamed Biswas, Abhilasha Singh, Laura S. Dreissen, Henning A. Fürst, Tanja E. Mehlstäubler","We report on our progress on multi-ion spectroscopy with Yb+ ions. We first characterized the spatial homogeneity of the magnetic- and the rf-field. Subsequently, to equalize the immense AC Stark shifts of the individual ions in a Coulomb crystal, we utilize a holographic phase plate to convert the Gaussian beam into a flat-top shape. These efforts enable us to excite an 8-ion crystal (172Yb+) on the highly forbidden electric octupole transition. Using 173Yb+ with reduced AC Stark shift, the same beam profile could support the multi-ion operation of 20 ions. This paves the way for future multi-ion optical clocks.",2025-09-29,http://arxiv.org/pdf/2509.24599v1
n-alkanoate + n-alkane mixtures: folding of hydrocarbon chains of n-alkanoates,"Juan Antonio González, Fernando Hevia, Luis Felipe Sanz, Daniel Lozano-Martín, Isaías García de la Fuente, José Carlos Cobos","The mixtures CH$_3$(CH$_2$)$_{u-1}$COO(CH$_2$)$_{v-1}$CH$_3$ ($u=5-13$, $v=1,2$; $u=1,2,3$, $v=3,4$; $u=1,2,4$, $v=5$) + n-alkane have been investigated using experimental data (viscosity and excess molar functions: enthalpy, $H_{\text{m}}^{\text{E}}$, volume, $V_{\text{m}}^{\text{E}}$, isobaric heat capacity, and isochoric internal energy, $U_{V\text{m}}^{\text{E}}$) and models (Flory, Grunberg-Nissan, Bloomfield-Dewan). They are characterized by weak orientational effects. Large structural effects are found in some systems, like those containing pentane. Some considerations from standard enthalpies of vaporization, cohesive energy densities and $V_{\text{m}}^{\text{E}}$ of heptane mixtures reveal the existence of structural changes in longer n-alkanoates, which lead to stronger interactions between them. The observed decrease of $H_{\text{m}}^{\text{E}}$ for systems with a given n-alkane seems to be more related to the steric hindrance of the COO group than to interactional effects. The $U_{V\text{m}}^{\text{E}}(n)$ function ($n=$ number of C atoms in the n-alkane) shows a minimum for systems with esters with ($u\geq4$, $v=1$); ($u\geq7$, $v=2$), or ($u\geq 1$, $v=4,5$). A similar dependence was found for n-alkane mixtures involving cyclic molecules (cyclohexane, benzene). This result suggests that certain n-alkanoates, in an alkane medium, can form quasi-cyclic structures. Viscosities are well described by means of free volume effects only. For systems with butyl ethanoate or methyl decanoate, the variation of $\Delta \eta (n)$ (deviation of dynamic viscosity) is consistent with that of $U_{V\text{m}}^{\text{E}}(n)$, which supports the existence of cyclic structures in these esters. The Flory model provides poor results on $H_{\text{m}}^{\text{E}}$ for systems with large structural effects. Results improve when the model is applied to $U_{V\text{m}}^{\text{E}}(n)$ data.",2025-09-29,http://arxiv.org/pdf/2509.24589v1
Event-based complexity in atmospheric turbulence,"Paolo Paradisi, Rita Cesari","Since the studies of Kolmogorov and Oboukhov in 1941, the problem of intermittent large velocity excursions was recognized to be one of the most intriguing and elusive aspects of turbulent flows. While many efforts were devoted since 1960s to the magnitude intermittency, related to the statistics of the large increments in the turbulent signals, the attention towards the so-called clustering intermittency has started to increase only in the last two decades, even if some pioneering studies were carried in previous years. The low attention towards the clustering intermittency is somewhat surprising, as intermittency itself is essentially defined as an alternance of extended quiescent periods/regions and short/small high activity periods/zones. It is then natural to characterize intermittency by means of events marked along the dependent variable axes, whatever time or space. Conversely, the concept of crucial events and of temporal complexity, related to non-trivial clustering properties of these same events, has been proposed as a general interpretative framework for the investigation of complex systems with metastable self-organizing dynamics. Without any claim to being complete, in this chapter we give a review of event-based complexity approaches used in literature for the description of turbulence in the planetary boundary layer. The main goal is to put a first bridge between turbulence studies exploiting event-based complexity approaches, such as clustering exponents and classical Oboukhov intermittency exponents, and studies about intermittent complex systems, where concepts and ideas developed in the fields of non-equilibrium statistical physics, probability theory and dynamical system theory are jointly exploited.",2025-09-29,http://arxiv.org/pdf/2509.24576v1
Emergent World Representations in OpenVLA,"Marco Molinari, Leonardo Nevali, Saharsha Navani, Omar G. Younis","Vision Language Action models (VLAs) trained with policy-based reinforcement learning (RL) encode complex behaviors without explicitly modeling environmental dynamics. However, it remains unclear whether VLAs implicitly learn world models, a hallmark of model-based RL. We propose an experimental methodology using embedding arithmetic on state representations to probe whether OpenVLA, the current state of the art in VLAs, contains latent knowledge of state transitions. Specifically, we measure the difference between embeddings of sequential environment states and test whether this transition vector is recoverable from intermediate model activations. Using linear and non linear probes trained on activations across layers, we find statistically significant predictive ability on state transitions exceeding baselines (embeddings), indicating that OpenVLA encodes an internal world model (as opposed to the probes learning the state transitions). We investigate the predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that the world model emerges as training progresses. Finally, we outline a pipeline leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.",2025-09-29,http://arxiv.org/pdf/2509.24559v1
M3DIS -- A grid of 3D radiation-hydrodynamics stellar atmosphere models for stellar surveys. II. Carbon-enhanced metal-poor stars,"Philipp Eitner, Maria Bergemann, Richard Hoppe, Nicholas Storm, Veronika Lipatova, Simon C. O. Glover, Ralf S. Klessen, Åke Nordlund, Andrius Popovas","Understanding the origin and evolution of carbon-enhanced metal-poor (CEMP) stars is key to tracing the Galaxy's early chemical enrichment. We investigate how realistic 3D radiation-hydrodynamics (RHD) model atmospheres affect carbon abundances in CEMP stars and implications for their classification and Galactic chemical evolution (GCE). We focus on biases from traditional 1D hydrostatic models. We use the M3DIS code to compute 3D RHD model atmospheres for main-sequence and sub-giant stars over a wide range of metallicities and carbon enhancements. Synthetic spectra of the CH G-band are calculated with 3D radiative transfer and compared to spectra from classical 1D MARCS models. We derive abundance corrections and apply them to a large SAGA database sample to quantify effects on the carbon abundance distribution and CEMP classification. Our new 3D CEMP models predict cooler upper atmospheric layers than in 1D models, resulting in stronger CH absorption and lower inferred carbon abundances by up to -0.9 dex at the lowest metallicities. Carbon enhancement in the atmosphere itself increases molecular opacities and leads to radiative re-heating, partly offsetting adiabatic cooling in 3D models and reducing 3D-1D abundance corrections. Applying these corrections lowers the CEMP fraction by up to 20% below [Fe/H]=-3 and alters the relative contribution of CEMP sub-classes. The fraction of CEMP-no stars increases while the number of CEMP-r/s stars decreases, due to the downward revision of absolute carbon abundances. These changes bring the Galactic carbon distribution into better agreement with GCE models assuming a 20% contribution from faint supernovae. Realistic model atmospheres are essential to reliably reconstruct the Galaxy's early chemical enrichment history.",2025-09-29,http://arxiv.org/pdf/2509.24555v1
Short window attention enables long-term memorization,"Loïc Cabannes, Maximilian Beck, Gergely Szilvasy, Matthijs Douze, Maria Lomeli, Jade Copet, Pierre-Emmanuel Mazaré, Gabriel Synnaeve, Hervé Jégou","Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.   A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.   The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.",2025-09-29,http://arxiv.org/pdf/2509.24552v1
Foggy Crowd Counting: Combining Physical Priors and KAN-Graph,"Yuhao Wang, Zhuoran Zheng, Han Hu, Dianjie Lu, Guijuan Zhang, Chen Lyu","Aiming at the key challenges of crowd counting in foggy environments, such as long-range target blurring, local feature degradation, and image contrast attenuation, this paper proposes a crowd-counting method with a physical a priori of atmospheric scattering, which improves crowd counting accuracy under complex meteorological conditions through the synergistic optimization of the physical mechanism and data-driven.Specifically, first, the method introduces a differentiable atmospheric scattering model and employs transmittance dynamic estimation and scattering parameter adaptive calibration techniques to accurately quantify the nonlinear attenuation laws of haze on targets with different depths of field.Secondly, the MSA-KAN was designed based on the Kolmogorov-Arnold Representation Theorem to construct a learnable edge activation function. By integrating a multi-layer progressive architecture with adaptive skip connections, it significantly enhances the model's nonlinear representation capability in feature-degraded regions, effectively suppressing feature confusion under fog interference.Finally, we further propose a weather-aware GCN that dynamically constructs spatial adjacency matrices using deep features extracted by MSA-KAN. Experiments on four public datasets demonstrate that our method achieves a 12.2\%-27.5\% reduction in MAE metrics compared to mainstream algorithms in dense fog scenarios.",2025-09-29,http://arxiv.org/pdf/2509.24545v1
Quantitative convergence of trained single layer neural networks to Gaussian processes,"Eloy Mosig, Andrea Agazzi, Dario Trevisan","In this paper, we study the quantitative convergence of shallow neural networks trained via gradient descent to their associated Gaussian processes in the infinite-width limit.   While previous work has established qualitative convergence under broad settings, precise, finite-width estimates remain limited, particularly during training.   We provide explicit upper bounds on the quadratic Wasserstein distance between the network output and its Gaussian approximation at any training time $t \ge 0$, demonstrating polynomial decay with network width.   Our results quantify how architectural parameters, such as width and input dimension, influence convergence, and how training dynamics affect the approximation error.",2025-09-29,http://arxiv.org/pdf/2509.24544v1
Charge-localization-driven metal-insulator phase transition in layered molecular conductors,"Savita Priya, Maxim Wenzel, Olga Iakutkina, Marvin Schmidt, Christian Prange, Dieter Schweitzer, Yohei Saito, Reizo Kato, Koichi Hiraki, Martin Dressel","The organic conductor $\alpha$-(BEDT-TTF)$_2$I$_3$ provides the prime example of a charge-order-driven metal-insulator transition. Restricted chemical substitution of S atoms by Se in the constituent molecules allows us to modify the electronic properties. This not only decreases the transition temperature but, in addition, alters the phase transition mechanism, resulting in the ground state deviating from the charge-ordered insulator state of the parent compound. Employing infrared optical spectroscopy, we investigate changes in the charge dynamics. Furthermore, we demonstrate the absence of charge ordering in the Se-substituted materials and suggest that the phase transition is instead driven by the localization of the itinerant charge carriers due to strong electron-phonon interactions.",2025-09-29,http://arxiv.org/pdf/2509.24518v1
Knowledge Editing with Subspace-Aware Key-Value Mappings,"Haewon Park, Sangwoo Kim, Yohan Jo","Knowledge editing aims to efficiently correct factual errors in Language Models (LMs). The popular locate-then-edit approach modifies an MLP layer by finding an optimal mapping between its input vector (key) and output vector (value) that leads to the expression of the edited knowledge. However, existing methods without any constraints on the key and value vectors cause significant perturbations to the edited model. To address this, we propose Subspace Knowledge Edit (SUIT), a method that identifies and modifies only the subspace of critical features relevant to the edit. Our empirical results on LLaMA-3-8B, GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge preservation over strong baselines while maintaining high edit efficacy. This effectiveness confirms that SUIT successfully identifies the critical subspace for the edit. Further analyses provide additional validation for our approach. The source code and data will be released to the public upon publication of the paper.",2025-09-29,http://arxiv.org/pdf/2509.24502v1
Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model,"Charmaine Barker, Daniel Bethell, Simos Gerasimou","Reliable uncertainty quantification remains a major obstacle to the deployment of deep learning models under distributional shift. Existing post-hoc approaches that retrofit pretrained models either inherit misplaced confidence or merely reshape predictions, without teaching the model when to be uncertain. We introduce GUIDE, a lightweight evidential learning meta-model approach that attaches to a frozen deep learning model and explicitly learns how and when to be uncertain. GUIDE identifies salient internal features via a calibration stage, and then employs these features to construct a noise-driven curriculum that teaches the model how and when to express uncertainty. GUIDE requires no retraining, no architectural modifications, and no manual intermediate-layer selection to the base deep learning model, thus ensuring broad applicability and minimal user intervention. The resulting model avoids distilling overconfidence from the base model, improves out-of-distribution detection by ~77% and adversarial attack detection by ~80%, while preserving in-distribution performance. Across diverse benchmarks, GUIDE consistently outperforms state-of-the-art approaches, evidencing the need for actively guiding uncertainty to close the gap between predictive confidence and reliability.",2025-09-29,http://arxiv.org/pdf/2509.24492v1
Quantum Optics Nature of the Elementary Excitations in Few-Layer WSe2 Semiconductors,"Ji Zhou, Yitong Wang, Debao Zhang, Xuguang Cao, Wanggui Ye, Xinye Fan, Jiqiang Ning, Shijie Xu","A fully quantized description of a two-level system resonantly coupled with an electromagnetic field (light) is among the central topics of quantum electrodynamics, which is theorized by the quantum Rabi model. It is also a fundamental issue of light-matter interactions. The rapid development of two-dimensional (2D) transition-metal dichalcogenide (TMDC) atomically-thin semiconductors brings excellent great chance to test and demonstrate some basic predictions by quantum optics theory in textbook, i.e., by the quantum Rabi model. Such test and demonstration are of both scientific and technological significance because of the quick emergence of the second quantum revolution. In this Letter, we show a quantum optics demonstration of the variable-temperature optical responses of the elementary excitations in few-layer WSe2 flakes. It is unraveled that the variable-temperature reflectance and fluorescence patterns of the elementary excitations (i.e., the band-edge excitons) of monolayer, bilayer and hBN capped WSe2 match well with the predictions by the quantum Rabi model under the rotating wave approximation. Decoherence times, Rabi frequencies, and transition matrix elements of the elementary excitations in these few-layer WSe2 flakes are found to be all negatively correlated with temperature, and to show dependence on layer number and capping layer. These findings may provide a novel perspective for comprehending the fundamental quantum physical properties of two-dimensional materials.",2025-09-29,http://arxiv.org/pdf/2509.24474v1
FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing,"Ran Elbaz, Guy Bar-Shalom, Yam Eitan, Fabrizio Frasca, Haggai Maron","Permutation equivariant neural networks employing parameter-sharing schemes have emerged as powerful models for leveraging a wide range of data symmetries, significantly enhancing the generalization and computational efficiency of the resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated promise through their improved interpretability and expressivity compared to traditional architectures based on MLPs. While equivariant KANs have been explored in recent literature for a few specific data types, a principled framework for applying them to data with permutation symmetries in a general context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a principled approach to constructing equivariant and invariant KA layers for arbitrary permutation symmetry groups, unifying and significantly extending previous work in this domain. We derive the basic construction of these FS-KAN layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup and provide a theoretical analysis demonstrating that FS-KANs have the same expressive power as networks that use standard parameter-sharing layers, allowing us to transfer well-known and important expressivity results from parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data types and symmetry groups show that FS-KANs exhibit superior data efficiency compared to standard parameter-sharing layers, by a wide margin in certain cases, while preserving the interpretability and adaptability of KANs, making them an excellent architecture choice in low-data regimes.",2025-09-29,http://arxiv.org/pdf/2509.24472v1
"A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions","Leila Ismail, Abdelmoneim Abdelmoti, Arkaprabha Basu, Aymen Dia Eddine Berini, Mohammad Naouss","With the increasing complexity of industrial systems, there is a pressing need for predictive maintenance to avoid costly downtime and disastrous outcomes that could be life-threatening in certain domains. With the growing popularity of the Internet of Things, Artificial Intelligence, machine learning, and real-time big data analytics, there is a unique opportunity for efficient predictive maintenance to forecast equipment failures for real-time intervention and optimize maintenance actions, as traditional reactive and preventive maintenance practices are often inadequate to meet the requirements for the industry to provide quality-of-services of operations. Central to this evolution is digital twin technology, an adaptive virtual replica that continuously monitors and integrates sensor data to simulate and improve asset performance. Despite remarkable progress in digital twin implementations, such as considering DT in predictive maintenance for industrial engineering. This paper aims to address this void. We perform a retrospective analysis of the temporal evolution of the digital twin in predictive maintenance for industrial engineering to capture the applications, middleware, and technological requirements that led to the development of the digital twin from its inception to the AI-enabled digital twin and its self-learning models. We provide a layered architecture of the digital twin technology, as well as a taxonomy of the technology-enabled industrial engineering applications systems, middleware, and the used Artificial Intelligence algorithms. We provide insights into these systems for the realization of a trustworthy and efficient smart digital-twin industrial engineering ecosystem. We discuss future research directions in digital twin for predictive maintenance in industrial engineering.",2025-09-29,http://arxiv.org/pdf/2509.24443v1
NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding,"Yanpeng Zhao, Shanyan Guan, Yunbo Wang, Yanhao Ge, Wei Li, Xiaokang Yang","We introduce NeoWorld, a deep learning framework for generating interactive 3D virtual worlds from a single input image. Inspired by the on-demand worldbuilding concept in the science fiction novel Simulacron-3 (1964), our system constructs expansive environments where only the regions actively explored by the user are rendered with high visual realism through object-centric 3D representations. Unlike previous approaches that rely on global world generation or 2D hallucination, NeoWorld models key foreground objects in full 3D, while synthesizing backgrounds and non-interacted regions in 2D to ensure efficiency. This hybrid scene structure, implemented with cutting-edge representation learning and object-to-3D techniques, enables flexible viewpoint manipulation and physically plausible scene animation, allowing users to control object appearance and dynamics using natural language commands. As users interact with the environment, the virtual world progressively unfolds with increasing 3D detail, delivering a dynamic, immersive, and visually coherent exploration experience. NeoWorld significantly outperforms existing 2D and depth-layered 2.5D methods on the WorldScore benchmark.",2025-09-29,http://arxiv.org/pdf/2509.24441v1
Quantum Zeno Effect in the Spatial Evolution of a Single Atom,"Zheng-Yuan Zhang, Han-Chao Chen, Xin Liu, Li-Hua Zhang, Bang Liu, Shi-Yao Shao, Jun Zhang, Qi-Feng Wang, Qing Li, Yu Ma, Tian-Yu Han, Ya-Jun Wang, Dong-Yang Zhu, Jia-Dou Nan, Yi-Ming Yin, Qiao-Qiao Fang, Dong-Sheng Ding, Bao-Sen Shi","The quantum Zeno effect (QZE) reveals that frequent measurements can suppress quantum evolution, but the detailed dynamics of the system under finite-duration measurements in experiments remain insufficiently explored. Here, we employ an optical dipole trap as a projective measurement to study the motion of a single cold atom in free space. By monitoring atomic loss, we directly observe the QZE in single-atom motion in free space and find that the effect of dipole measurements on the atom comprises a short-time collapse followed by subsequent periodic unitary evolution, thereby providing an intuitive physical picture of measurement backaction across different timescales. We further investigate the effects of measurement frequency, strength, and spatial position, demonstrating that measurements not only suppress the spreading of quantum states but also enable deterministic preparation of distinct motional states. Furthermore, by dynamically controlling the measurement position, we achieve measurement-induced directional transport of a single atom without imparting additional momentum. Our results provide a direct experimental demonstration of the QZE in real space and establish a versatile framework for measurement-based control of atomic motion, paving the way for motional-state engineering in cold-atom platforms.",2025-09-29,http://arxiv.org/pdf/2509.24438v1
Energy-Efficient Movable Antennas: Mechanical Power Modeling and Performance Optimization,"Xin Wei, Weidong Mei, Xuan Huang, Zhi Chen, Boyu Ning","Movable antennas (MAs) offer additional spatial degrees of freedom (DoFs) to enhance communication performance through local antenna movement. However, to achieve accurate and fast antenna movement, MA drivers entail non-negligible mechanical power consumption, rendering energy efficiency (EE) optimization more critical compared to conventional fixed-position antenna (FPA) systems. To address this issue, we develop a fundamental power consumption model for stepper motor-driven multi-MA systems based on electric motor theory. Based on this model, we formulate an EE maximization problem from a multi-MA base station (BS) to multiple single-FPA users. We aim to jointly optimize the MAs' positions, moving speeds, and the BS's transmit precoding matrix subject to collision-avoidance constraints during the multi-MA movements. However, this problem is difficult to solve. To tackle this challenge, we first reveal that the collision-avoidance constraints can always be relaxed without loss of optimality by properly renumbering the MA indices. For the resulting relaxed problem, we first consider a simplified single-user setup and uncover a hidden monotonicity of the EE performance with respect to the MAs' moving speeds. To solve the remaining optimization problem, we develop a two-layer optimization framework. In the inner layer, the Dinkelbach algorithm is employed to derive the optimal beamforming solution for any given MA positions. In the outer layer, a sequential update algorithm is proposed to iteratively refine the MA positions based on the optimal values obtained from the inner layer. Next, we proceed to the general multi-user case and propose an alternating optimization (AO) algorithm. Numerical results demonstrate that despite the additional mechanical power consumption, the proposed algorithms can outperform both conventional FPA systems and other existing EE maximization benchmarks",2025-09-29,http://arxiv.org/pdf/2509.24433v1
CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers,"Kai Liu, Shaoqiu Zhang, Linghe Kong, Yulun Zhang","Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",2025-09-29,http://arxiv.org/pdf/2509.24416v1
Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure,"Nhan T. Luu, Duong T. Luu, Pham Ngoc Nam, Truong Cong Thang","Spiking Neural Networks (SNNs) have gained significant traction in both computational neuroscience and artificial intelligence for their potential in energy-efficient computing. In contrast, artificial neural networks (ANNs) excel at gradient-based optimization and high accuracy. This contrast has consequently led to a growing subfield of hybrid ANN-SNN research. However, existing hybrid approaches often rely on either a strict separation between ANN and SNN components or employ SNN-only encoders followed by ANN classifiers due to the constraints of non-differentiability of spike encoding functions, causing prior hybrid architectures to lack deep layer-wise cooperation during backpropagation. To address this gap, we propose a novel hybrid ANN-SNN framework that integrates layer-wise encode-decode SNN blocks within conventional ANN pipelines. Central to our method is the use of surrogate gradients for a bit-plane-based spike encoding function, enabling end-to-end differentiable training across ANN and SNN layers. This design achieves competitive accuracy with state-of-the-art pure ANN and SNN models while retaining the potential efficiency and temporal representation benefits of spiking computation. To the best of our knowledge, this is the first implementation of a surrogate gradient for bit plane coding specifically and spike encoder interface in general to be utilized in the context of hybrid ANN-SNN, successfully leading to a new class of hybrid models that pave new directions for future research.",2025-09-29,http://arxiv.org/pdf/2509.24411v1
FAST search for circumstellar atomic hydrogen. IV. bubbles associated with planetary nebulae,"Jun-Yang Liu, Yong Zhang, Xu-Jia Ouyang","Investigating the bubbles generated by the interaction between asymptotic giant branch stellar outflows and the interstellar medium (ISM) is pivotal for elucidating the mechanism by which evolved low- to intermediate-mass stars enrich the ISM with heavy elements. Using archival datasets from the Galactic Plane Pulsar Snapshot survey and the Galactic Arecibo L-Band Array \ion{H}{1} survey, we have identified 14 bubbles within interstellar atomic hydrogen (\ion{H}{1}) maps, each showing evidence of potential association with planetary nebulae (PNe).We pursue two primary objectives centered on the identified ISM bubbles and their association with PNe. First, leveraging the calibrated distance measurements of PNe from Gaia Data Release 3, we utilize these ISM bubbles as observational tracers to investigate and constrain the Galactic rotation curve. Second, we note that distance determinations for some PNe remain unreliable, partly because their central stars are obscured by extended nebular envelopes or are misidentified. Therefore, we develop a novel methodological framework to derive kinematic distances for PNe by leveraging the velocities of their associated ISM bubbles and constraints from the Galactic rotation curve.",2025-09-29,http://arxiv.org/pdf/2509.24400v1
SQuaD: Smart Quantum Detection for Photon Recognition and Dark Count Elimination,"Karl C. Linne, Sho Uemura, Yue Ji, Andrew Karmen, Allen Zang, Alex Kolar, Martin Di Federico, Orlando Quaranta, Gustavo Cancelo, Tian Zhong","Quantum detectors of single photons are an essential component for quantum information processing across computing, communication and networking. Today's quantum detection system, which consists of single photon detectors, timing electronics, control and data processing software, is primarily used for counting the number of single photon detection events. However, it is largely incapable of extracting other rich physical characteristics of the detected photons, such as their wavelengths, polarization states, photon numbers, or temporal waveforms. This work, for the first time, demonstrates a smart quantum detection system, SQuaD, which integrates a field programmable gate array (FPGA) with a neural network model, and is designed to recognize the features of photons and to eliminate detector dark-count. The SQuaD is a fully integrated quantum system with high timing-resolution data acquisition, onboard multi-scale data analysis, intelligent feature recognition and extraction, and feedback-driven system control. Our \name experimentally demonstrates 1) reliable photon counting on par with the state-of-the art commercial systems; 2) high-throughput data processing for each individual detection events; 3) efficient dark count recognition and elimination; 4) up to 100\% accurate feature recognition of photon wavelength and polarization. Additionally, we deploy the SQuaD to an atomic (erbium ion) photon emitter source to realize noise-free control and readout of a spin qubit in the telecom band, enabling critical advances in quantum networks and distributed quantum information processing.",2025-09-29,http://arxiv.org/pdf/2509.24383v1
Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models,"Jitai Hao, Hao Liu, Xinyan Xiao, Qiang Huang, Jun Yu","Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X",2025-09-29,http://arxiv.org/pdf/2509.24365v1
An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation,"Dayu Tan, Cheng Kong, Yansen Su, Hai Chen, Dongliang Yang, Junfeng Xia, Chunhou Zheng","In the field of multi-organ medical image segmentation, recent methods frequently employ Transformers to capture long-range dependencies from image features. However, these methods overlook the high computational cost of Transformers and their deficiencies in extracting local detailed information. To address high computational costs and inadequate local detail information, we reassess the design of feature extraction modules and propose a new deep-learning network called LamFormer for fine-grained segmentation tasks across multiple organs. LamFormer is a novel U-shaped network that employs Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture multi-scale long-range dependencies. We construct the Parallel Hierarchical Feature Aggregation (PHFA) module to aggregate features from different layers of the encoder, narrowing the semantic gap among features while filtering information. Finally, we design the Reduced Transformer (RT), which utilizes a distinct computational approach to globally model up-sampled features. RRT enhances the extraction of detailed local information and improves the network's capability to capture long-range dependencies. LamFormer outperforms existing segmentation methods on seven complex and diverse datasets, demonstrating exceptional performance. Moreover, the proposed network achieves a balance between model performance and model complexity.",2025-09-29,http://arxiv.org/pdf/2509.24358v1
NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis,"Yixuan Ren, Hanyu Wang, Hao Chen, Bo He, Abhinav Shrivastava","We present NeRV-Diffusion, an implicit latent video diffusion model that synthesizes videos via generating neural network weights. The generated weights can be rearranged as the parameters of a convolutional neural network, which forms an implicit neural representation (INR), and decodes into videos with frame indices as the input. Our framework consists of two stages: 1) A hypernetworkbased tokenizer that encodes raw videos from pixel space to neural parameter space, where the bottleneck latent serves as INR weights to decode. 2) An implicit diffusion transformer that denoises on the latent INR weights. In contrast to traditional video tokenizers that encode videos into frame-wise feature maps, NeRV-Diffusion compresses and generates a video holistically as a unified neural network. This enables efficient and high-quality video synthesis via obviating temporal cross-frame attentions in the denoiser and decoding video latent with dedicated decoders. To achieve Gaussian-distributed INR weights with high expressiveness, we reuse the bottleneck latent across all NeRV layers, as well as reform its weight assignment, upsampling connection and input coordinates. We also introduce SNR-adaptive loss weighting and scheduled sampling for effective training of the implicit diffusion model. NeRV-Diffusion reaches superior video generation quality over previous INR-based models and comparable performance to most recent state-of-the-art non-implicit models on real-world video benchmarks including UCF-101 and Kinetics-600. It also brings a smooth INR weight space that facilitates seamless interpolations between frames or videos.",2025-09-29,http://arxiv.org/pdf/2509.24353v1
Exploring Similarity between Neural and LLM Trajectories in Language Processing,"Xin Xiao, Kaiwen Wei, Jiang Zhong, Dongshuo Yin, Yu Tian, Xuekai Wei, Mingliang Zhou","Understanding the similarity between large language models (LLMs) and human brain activity is crucial for advancing both AI and cognitive neuroscience. In this study, we provide a multilinguistic, large-scale assessment of this similarity by systematically comparing 16 publicly available pretrained LLMs with human brain responses during natural language processing tasks in both English and Chinese. Specifically, we use ridge regression to assess the representational similarity between LLM embeddings and electroencephalography (EEG) signals, and analyze the similarity between the ""neural trajectory"" and the ""LLM latent trajectory."" This method captures key dynamic patterns, such as magnitude, angle, uncertainty, and confidence. Our findings highlight both similarities and crucial differences in processing strategies: (1) We show that middle-to-high layers of LLMs are central to semantic integration and correspond to the N400 component observed in EEG; (2) The brain exhibits continuous and iterative processing during reading, whereas LLMs often show discrete, stage-end bursts of activity, which suggests a stark contrast in their real-time semantic processing dynamics. This study could offer new insights into LLMs and neural processing, and also establish a critical framework for future investigations into the alignment between artificial intelligence and biological intelligence.",2025-09-29,http://arxiv.org/pdf/2509.24307v1
An SoS Entropy Dichotomy via Windowed Hypercontractivity,Marko Lela,"We prove an entropy versus degree dichotomy for low-degree tests and the Sum-of-Squares (SoS) hierarchy on a calibrated window after a gadget layer. For a target distribution \(\mu\) and a product-like proxy \(u\), we study the low-degree discrepancy \(\Delta_k(\mu,u)\), defined as the optimal distinguishing advantage of degree \(\le k\) polynomial tests. Using a bias-orthonormal Walsh basis and a test-moment equivalence on the window, we relate \(\Delta_k\) (up to constants) to the squared \(\ell_2\) mass of signed low-degree moments. Calibrated pseudoexpectations match \(u\) on all moments of degree \(\le k\), hence test discrepancy equals SoS pseudoexpectation deviation. Under bias, product, and width assumptions along a switching path, a windowed Bonami--Beckner inequality yields hypercontractive tail bounds. Combining these with moment matching, we obtain a discrepancy-to-degree theorem: if \(\Delta_k(\mu,u) \ge n^{-\beta}\), then any polynomial-calculus or SoS refutation separating \(\mu\) from \(u\) requires degree \(\Omega(k)\). Instantiating \(k = c \log n\) gives an explicit \(\Omega(\log n)\) SoS degree lower bound whenever \(\Delta_k \ge n^{-\eta}\). All constants are explicit and depend only on calibrated window parameters. This work provides the SoS/low-degree core and complements a prior calibration blueprint; a companion paper lifts the windowed statements to full distribution families.",2025-09-29,http://arxiv.org/pdf/2509.24280v1
G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge,"Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan","Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.",2025-09-29,http://arxiv.org/pdf/2509.24276v1
LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models,"Nimisha Ghosh, Dheeran Sankaran, Rahul Balakrishnan Adhi, Sharath S, Amrut Anand","Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the understanding of cell function, molecular interactions as well as regulatory functions. Owing to their high similarity, most of the existing approaches face challenges in differentiating between DBPs and RBPs leading to high cross-prediction errors. Moreover, identifying proteins which bind to both DNA and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a novel framework viz. LAMP-PRo which is based on pre-trained protein language model (PLM), attention mechanisms and multi-label learning to mitigate these issues. First, pre-trained PLM such ESM-2 is used for embedding the protein sequences followed by convolutional neural network (CNN). Subsequently multi-head self-attention mechanism is applied for the contextual information while label-aware attention is used to compute class-specific representations by attending to the sequence in a way that is tailored to each label (DBP, RBP and non-NABP) in a multi-label setup. We have also included a novel cross-label attention mechanism to explicitly capture dependencies between DNA- and RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a linear layer followed by a sigmoid function are used for the final prediction. Extensive experiments are carried out to compare LAMP-PRo with the existing methods wherein the proposed model shows consistent competent performance. Furthermore, we also provide visualization to showcase model interpretability, highlighting which parts of the sequence are most relevant for a predicted label. The original datasets are available at http://bliulab.net/iDRBP\_MMC and the codes are available at https://github.com/NimishaGhosh/LAMP-PRo.",2025-09-29,http://arxiv.org/pdf/2509.24262v1
When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs,"Jinming Liu, Zhaoyang Jia, Jiahao Li, Bin Li, Xin Jin, Wenjun Zeng, Yan Lu","The increasing deployment of powerful Multimodal Large Language Models (MLLMs), typically hosted on cloud platforms, urgently requires effective compression techniques to efficiently transmit signal inputs (e.g., images, videos) from edge devices with minimal bandwidth usage. However, conventional image codecs are optimized for fidelity to serve the Human Visual System (HVS) and ill-suited for MLLMs, in which diverse downstream tasks are jointly considered. In this paper, we first systematically analyze the impact of compression artifacts on several mainstream MLLMs. We find that: Compression distortion unevenly impacts different-level image features, leading to varying effects on MLLMs' downstream tasks depending on their feature-level reliance. Motivated by this discovery, we propose an image Codec TAilored to MLLMs (CoTAM) designed to adaptively protect multi-level features and suit different demands of downstream tasks. The encoder leverages CLIP's shallow-layer attention to generate an importance map for bit allocation, preserving critical semantic regions. Concurrently, the decoder integrates a lightweight adapter with a multi-level loss function to ensure the faithful reconstruction both of low-level details and high-level semantic context for robust synthesis of cross-level features. Extensive experiments validate that our method achieves up to 35.99\% bitrate saving while maintaining the same performance on the MLLM tasks, outperforming previous SOTA neural codecs.",2025-09-29,http://arxiv.org/pdf/2509.24258v1
Prompt and Parameter Co-Optimization for Large Language Models,"Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong","Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.",2025-09-29,http://arxiv.org/pdf/2509.24245v1
Far-infrared lines hidden in archival deep multi-wavelength surveys: Limits on [CII]-158$μ$m at $z \sim 0.3-2.9$,"Shubh Agrawal, James Aguirre, Ryan Keenan","Singly-ionized carbon is theorized to be the brightest emission line feature in star-forming galaxies, and hence an excellent tracer of the evolution of cosmic star formation. Archival maps from far-infrared and sub-millimeter surveys potentially contain the redshifted [CII]-158$\mu$m, hidden in the much brighter continuum emission. We present a search for aggregate [CII]-158$\mu$m line emission across the predicted peak of star formation history by tomographically stacking a high-completeness galaxy catalog on broadband deep maps of the COSMOS field and constraining residual excess emission after subtracting the continuum spectral energy distribution (SED).   We obtain constraints on the sky-averaged [CII]-158$\mu$m signal from the three Herschel/SPIRE maps: $11.8\pm10.2$, $11.0\pm8.7$, $9.6\pm9.8$, and $9.2\pm6.6$ $k$Jy/sr at redshifts $z\sim 0.65$, $\sim1.3$, $\sim2.1$, and $\sim2.6$ respectively, corresponding to $1-1.4\sigma$ significance in each bin. Our $3\sigma$ upper limits are in tension with past $z\sim2.6$ results from cross-correlating SDSS-BOSS quasars with high-frequency Planck maps, and indicate a much less dramatic evolution ($\sim\times7.5$) of mean [CII] intensity across the peak of star formation history than collisional excitation models or frameworks calibrated to the tentative PlanckxBOSS measurement. We discuss this tension, particularly in the context of in-development surveys (TIM, EXCLAIM) that will map this [CII] at high redshift resolution. Having demonstrated stacking in broadband deep surveys as a complementary methodology to next-generation spectrometers for line intensity mapping, our novel methods can be extended to upcoming galaxy surveys such as Euclid, as well as to place upper limits on fainter atomic and molecular lines.",2025-09-29,http://arxiv.org/pdf/2509.24211v1
The role of the solid-melt interface in accelerating the self-catalyzed growth kinetics of III-V semiconductors,"Zhucong Xi, Abby Liu, Xiaobo Chen, Meng Li, Dmitri N. Zakharov, Judith C. Yang, Rachel S. Goldman, Liang Qi","Solid-melt interfaces play a pivotal role in governing crystal growth and metal-mediated epitaxy of gallium nitride (GaN) and other semiconductor materials. Using atomistic simulations based on machine-learning interatomic potentials (MLIPs), we uncover that multiple layers of Ga atoms at the GaN-Ga melt interface form structurally ordered and electronically charged configurations that are critical for the growth kinetics of GaN. These ordered layers modulate the free energy landscape (FEL) for N adsorption and substantially reduce the migration barriers for N at the interface compared to a clean GaN surface. Leveraging these interfacial energetics, kinetic Monte Carlo (KMC) simulations reveal that GaN growth follows a diffusion-controlled, layer-by-layer mechanism, with the FEL for N adsorption emerging as the rate-limiting factor. By incorporating facet-specific FELs and the diffusivity/solubility of N in Ga melt, we develop a predictive, fitting-free transport model that estimates facet-dependent growth rates in the range of ~0.01 to 0.04 nm/s, in agreement with experimental growth rates observed in GaN nanoparticles synthesized by Ga-mediated molecular beam epitaxy (MBE). This multiscale framework offers a generalizable and quantitative approach to link atomic-scale ordering and interfacial energetics to macroscopic phenomena, providing actionable insights for the rational design of metal-mediated epitaxial processes.",2025-09-29,http://arxiv.org/pdf/2509.24206v1
Negative Pre-activations Differentiate Syntax,"Linghao Kong, Angelina Ning, Micah Adler, Nir Shavit","A recently discovered class of entangled neurons, known as Wasserstein neurons, is disproportionately critical in large language models despite constituting only a very small fraction of the network: their targeted removal collapses the model, consistent with their unique role in differentiating similar inputs. Interestingly, in Wasserstein neurons immediately preceding smooth activation functions, such differentiation manifests in the negative pre-activation space, especially in early layers. Pairs of similar inputs are driven to highly distinct negative values, and these pairs involve syntactic tokens such as determiners and prepositions. We show that this negative region is functional rather than simply favorable for optimization. A minimal, sign-specific intervention that zeroes only the negative pre-activations of a small subset of entangled neurons significantly weakens overall model function and disrupts grammatical behavior, while both random and perplexity-matched controls leave grammatical performance largely unchanged. Part of speech analysis localizes the excess surprisal to syntactic scaffolding tokens, and layer-specific interventions reveal that small local degradations accumulate across depth. Over training checkpoints, the same ablation impairs grammatical behavior as Wasserstein neurons emerge and stabilize. Together, these results identify negative differentiation in a sparse subset of entangled neurons as a crucial mechanism that language models rely on for syntax.",2025-09-29,http://arxiv.org/pdf/2509.24198v1
"Structural, optical, and electrical properties of Cu-doped NiO films synthesized by spray pyrolysis for potential gas sensing applications","Eka Nurfani, Grace Grace, Mahardika Yoga Darmawan, Resti Marlina, Jumaeda Jatmika, Asnan Rinovian, Aditya Rianjanu","Cu-doped NiO thin films were deposited on ITO substrates via spray pyrolysis at 450 {\deg}C for 1.5 minutes. XRD confirmed a cubic NiO structure, with Cu incorporation reducing crystallite size, increasing interplanar spacing, and expanding the lattice parameter, indicating successful substitution of Cu ions. Optical analysis showed a slight bandgap reduction from 3.70 to 3.65 eV, while PL revealed lower emission intensity, suggesting enhanced defect states and suppressed carrier recombination. Raman spectroscopy exhibited redshifts in LO, 2TO, and 2LO modes and the disappearance of the TO mode, confirming lattice distortion from Cu doping. Gas sensing tests under ambient and LPG conditions demonstrated significantly improved sensitivity and voltage-dependent response for doped films. These results establish that Cu incorporation enhances charge transport and gas interaction mechanisms, making Cu-doped NiO films highly promising for efficient and reliable gas sensors.",2025-09-29,http://arxiv.org/pdf/2509.24197v1
Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces,"Huanshu Zhang, Lei Kang, Sawyer D. Campbell, Douglas H. Werner","Traditional metasurface design is limited by the computational cost of full-wave simulations, preventing thorough exploration of complex configurations. Data-driven approaches have emerged as a solution to this bottleneck, replacing costly simulations with rapid neural network evaluations and enabling near-instant design for meta-atoms. Despite advances, implementing a new optical function still requires building and training a task-specific network, along with exhaustive searches for suitable architectures and hyperparameters. Pre-trained large language models (LLMs), by contrast, sidestep this laborious process with a simple fine-tuning technique. However, applying LLMs to the design of nanophotonic devices, particularly for arbitrarily shaped metasurfaces, is still in its early stages; as such tasks often require graphical networks. Here, we show that an LLM, fed with descriptive inputs of arbitrarily shaped metasurface geometries, can learn the physical relationships needed for spectral prediction and inverse design. We further benchmarked a range of open-weight LLMs and identified relationships between accuracy and model size at the billion-parameter level. We demonstrated that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily shaped metasurfaces. Linking natural-language interaction to electromagnetic modelling, this ""chat-to-chip"" workflow represents a step toward more user-friendly data-driven nanophotonics.",2025-09-29,http://arxiv.org/pdf/2509.24196v1
BladderFormer: A Streaming Transformer for Real-Time Urological State Monitoring,"Chengwei Zhou, Steve Majerus, Gourav Datta","Bladder pressure monitoring systems are increasingly vital in diagnosing and managing urinary tract dysfunction. Existing solutions rely heavily on hand-crafted features and shallow classifiers, limiting their adaptability to complex signal dynamics. We propose a one-layer streaming transformer model for real-time classification of bladder pressure states, operating on wavelet-transformed representations of raw time-series data. Our model incorporates temporal multi-head self-attention and state caching, enabling efficient online inference with high adaptability. Trained on a dataset of 91 patients with 20,000-80,000 samples each, our method demonstrates improved accuracy, higher energy- and latency-efficiency. Implementation considerations for edge deployment on low-power hardware, such as edge graphical processing units (GPU) and micro-controllers, are also discussed.",2025-09-29,http://arxiv.org/pdf/2509.24178v1
"Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight","Haolin Yang, Hakaze Cho, Kaize Ding, Naoya Inoue","Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of ""key heads"" most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL.",2025-09-29,http://arxiv.org/pdf/2509.24169v1
Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs,"Arpit Garg, Hemanth Saratchandran, Ravi Garg, Simon Lucey","Machine unlearning in large language models (LLMs) is essential for privacy and safety; however, existing approaches remain unstable and unreliable. A widely used strategy, the gradient difference method, applies gradient descent on retained data while performing gradient ascent on forget data, the data whose influence should be removed. However, when combined with cross-entropy loss, this procedure causes unbounded growth of weights and gradients, leading to training instability and degrading both forgetting and retention. We provide a theoretical framework that explains this failure, explicitly showing how ascent on the forget set destabilizes optimization in the feedforward MLP layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient Unlearning, a parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters. This simple modification controls the weight dynamics during ascent, enabling the gradient difference method to converge reliably. Across the TOFU, TDEC, and MUSE benchmarks, and across architectures and scales from 125M to 8B parameters, our method achieves substantial improvements in forgetting while preserving retention, establishing a novel theoretically grounded and practically scalable framework for unlearning in LLMs.",2025-09-29,http://arxiv.org/pdf/2509.24166v1
Neural Visibility of Point Sets,"Jun-Hao Wang, Yi-Yang Tian, Baoquan Chen, Peng-Shuai Wang","Point clouds are widely used representations of 3D data, but determining the visibility of points from a given viewpoint remains a challenging problem due to their sparse nature and lack of explicit connectivity. Traditional methods, such as Hidden Point Removal (HPR), face limitations in computational efficiency, robustness to noise, and handling concave regions or low-density point clouds. In this paper, we propose a novel approach to visibility determination in point clouds by formulating it as a binary classification task. The core of our network consists of a 3D U-Net that extracts view-independent point-wise features and a shared multi-layer perceptron (MLP) that predicts point visibility using the extracted features and view direction as inputs. The network is trained end-to-end with ground-truth visibility labels generated from rendered 3D models. Our method significantly outperforms HPR in both accuracy and computational efficiency, achieving up to 126 times speedup on large point clouds. Additionally, our network demonstrates robustness to noise and varying point cloud densities and generalizes well to unseen shapes. We validate the effectiveness of our approach through extensive experiments on the ShapeNet, ABC Dataset and real-world datasets, showing substantial improvements in visibility accuracy. We also demonstrate the versatility of our method in various applications, including point cloud visualization, surface reconstruction, normal estimation, shadow rendering, and viewpoint optimization. Our code and models are available at https://github.com/octree-nn/neural-visibility.",2025-09-29,http://arxiv.org/pdf/2509.24150v1
Arbitrary Total Angular Momentum Vectorial Holography Using Bi-Layer Metasurfaces,"Joonkyo Jung, Hyeonhee Kim, Jonghwa Shin","Advanced holographic techniques are increasingly demanded for high-capacity and secure information processing. In this context, orbital angular momentum (OAM) stands out as a powerful resource for optical multiplexing, offering access to an unbounded set of orthogonal modes. To harness this potential, metasurfaces, with their considerable ability to control light, have emerged as key platforms for OAM-multiplexed holography. Nevertheless, conventional OAM holography suffers from limited polarization engineering capabilities due to the lack of chirality control in single-layer metasurfaces. Here, we introduce a bi-layer metasurface architecture that realizes total angular momentum (TAM) vectorial holography, where TAM represents the combination of spin angular momentum (SAM, equivalent to polarization) and OAM of light. In contrast to previous approaches, this scheme enables true polarization-OAM multiplexing, facilitating the independent generation of vectorial holographic images for each orthogonal TAM input state. This concept is validated numerically and experimentally, confirming the feasibility of TAM vectorial holography. The proposed scheme can be easily integrated with other recent holography generation approaches, such as vector beam multiplexing and bidirectional holography, thereby further expanding its multiplexing capability. This work establishes a versatile framework for advanced full-vectorial holography, showing how metasurfaces can unlock multiplexing strategies for emerging photonic systems.",2025-09-29,http://arxiv.org/pdf/2509.24139v1
"Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework",Nooshin Bahador,"This article presents a modular, component-based architecture for developing and evaluating AI agents that bridge the gap between natural language interfaces and complex enterprise data warehouses. The system directly addresses core challenges in data accessibility by enabling non-technical users to interact with complex data warehouses through a conversational interface, translating ambiguous user intent into precise, executable database queries to overcome semantic gaps. A cornerstone of the design is its commitment to transparent decision-making, achieved through a multi-layered reasoning framework that explains the ""why"" behind every decision, allowing for full interpretability by tracing conclusions through specific, activated business rules and data points. The architecture integrates a robust quality assurance mechanism via an automated evaluation framework that serves multiple functions: it enables performance benchmarking by objectively measuring agent performance against golden standards, and it ensures system reliability by automating the detection of performance regressions during updates. The agent's analytical depth is enhanced by a statistical context module, which quantifies deviations from normative behavior, ensuring all conclusions are supported by quantitative evidence including concrete data, percentages, and statistical comparisons. We demonstrate the efficacy of this integrated agent-development-with-evaluation framework through a case study on an insurance claims processing system. The agent, built on a modular architecture, leverages the BigQuery ecosystem to perform secure data retrieval, apply domain-specific business rules, and generate human-auditable justifications. The results confirm that this approach creates a robust, evaluable, and trustworthy system for deploying LLM-powered agents in data-sensitive, high-stakes domains.",2025-09-28,http://arxiv.org/pdf/2509.24127v1
Progressive Layer Stripping Analysis for HVSR Interpretation,"Mersad Fathizadeh, Clinton M. Wood, Hosna Kianfar","The horizontal-to-vertical spectral ratio (HVSR) technique is widely used to determine site fundamental periods from ambient noise recordings, but relating the observed peak to a specific impedance contrast within layered soils remains challenging. This paper presents an enhanced implementation of hvstrip-progressive, a Python package for forward HVSR modelling under the diffuse-field assumption and systematic progressive layer stripping. The package computes theoretical HVSR curves from shear-wave velocity (Vs) profiles, iteratively removes the deepest finite layer and promotes the next layer to half-space, and tracks how the fundamental frequency and amplitude change with each step. Compared with previous implementations, the software now supports adaptive frequency scanning, rigorous model validation, and publication-quality visualizations. Using a synthetic seven-layer soil profile, we show that the fundamental peak shifts from 6.99 Hz to 23.45 Hz as layers are stripped and that the maximum impedance contrast of 1.46 at 17 m depth controls the resonance. The transparent workflow, reproducible outputs and open-source distribution make hvstrip-progressive a practical tool for seismic site characterization and microzonation studies.",2025-09-28,http://arxiv.org/pdf/2509.24121v1
"ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs","Evan Dramko, Yihuang Xiong, Yizhi Zhu, Geoffroy Hautier, Thomas Reps, Christopher Jermaine, Anastasios Kyrillidis","Point defects play a central role in driving the properties of materials. First-principles methods are widely used to compute defect energetics and structures, including at scale for high-throughput defect databases. However, these methods are computationally expensive, making machine-learning force fields (MLFFs) an attractive alternative for accelerating structural relaxations. Most existing MLFFs are based on graph neural networks (GNNs), which can suffer from oversmoothing and poor representation of long-range interactions. Both of these issues are especially of concern when modeling point defects. To address these challenges, we introduce the Accelerated Deep Atomic Potential Transformer (ADAPT), an MLFF that replaces graph representations with a direct coordinates-in-space formulation and explicitly considers all pairwise atomic interactions. Atoms are treated as tokens, with a Transformer encoder modeling their interactions. Applied to a dataset of silicon point defects, ADAPT achieves a roughly 33 percent reduction in both force and energy prediction errors relative to a state-of-the-art GNN-based model, while requiring only a fraction of the computational cost.",2025-09-28,http://arxiv.org/pdf/2509.24115v1
PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM,"Ju-Hyung Lee, Yanqing Lu, Klaus Doppler","We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber states to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which normalizes latency by application tolerances and modulates energy by device battery states, provides richer supervision for KL-based finetuning. We study two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms inference at near-identical objective scores. Across synthetic scenarios grounded in real measurements, PEARL improves objective scores over heuristic and compact model baselines and reduces energy by up to 16% in cooperative low-battery cases. These results demonstrate that peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control.",2025-09-28,http://arxiv.org/pdf/2509.24085v1
Autoregressive Video Generation beyond Next Frames Prediction,"Sucheng Ren, Chen Chen, Zhenbang Wang, Liangchen Song, Xiangxin Zhu, Alan Yuille, Yinfei Yang, Jiasen Lu","Autoregressive models for video generation typically operate frame-by-frame, extending next-token prediction from language to video's temporal dimension. We question that unlike word as token is universally agreed in language if frame is a appropriate prediction unit? To address this, we present VideoAR, a unified framework that supports a spectrum of prediction units including full frames, key-detail frames, multiscale refinements, and spatiotemporal cubes. Among these designs, we find model video generation using \textit{spatiotemporal} cubes as prediction units, which allows autoregressive models to operate across both spatial and temporal dimensions simultaneously. This approach eliminates the assumption that frames are the natural atomic units for video autoregression. We evaluate VideoAR across diverse prediction strategies, finding that cube-based prediction consistently delivers superior quality, speed, and temporal coherence. By removing the frame-by-frame constraint, our video generator surpasses state-of-the-art baselines on VBench while achieving faster inference and enabling seamless scaling to minute-long sequences. We hope this work will motivate rethinking sequence decomposition in video and other spatiotemporal domains.",2025-09-28,http://arxiv.org/pdf/2509.24081v1
Wafer-Level Prototyping Tools for CMOS Bioelectronic Sensors,"Advait Madhavan, Ruohong Shi, Alokik Kanwal, Glenn Holland, Jacob M. Majikes, Paul N. Patrone, Anthony J. Kearsley, Arvind Balijepalli","Integrating biology with complementary metal-oxide-semiconductor (CMOS) sensors can enable highly parallel measurements with minimal parasitic effects, significantly enhancing sensitivity. However, realizing this potential often requires overcoming substantial barriers related to design, fabrication, and heterogeneous integration. In this context, we present a comprehensive suite of tools and methods designed for wafer-scale biosensor prototyping that is sensitive, highly parallelizable, and manufacturable. A central component of our approach is a new initiative that allows for open-source multi-project wafers (MPW), giving all participants access to the designs submitted by others. We demonstrate that this strategy not only promotes design reuse but also facilitates advanced back-end-of-line (BEOL) fabrication techniques, improving the manufacturability and process yield of CMOS biosensors. Developing CMOS-based biosensors also involves the challenge of heterogeneous integration, which includes external electrical, mechanical, and fluid layers. We demonstrate simple modular designs that enable such integration for sample delivery and signal readout. Finally, we showcase the effectiveness of our approach in measuring the hybridization of DNA molecules by focusing on data acquisition and machine learning (ML) methods that leverage the parallelism of the sensors to enable robust classification of desirable analyte interactions.",2025-09-28,http://arxiv.org/pdf/2509.24075v1
PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features,"Lingyao Li, Haolun Wu, Zhenkun Li, Jiabei Hu, Yu Wang, Xiaoshan Huang, Wenyue Hua, Wenqian Wang","High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.",2025-09-28,http://arxiv.org/pdf/2509.24046v1
Excitation and ionization by electron impact in transition and super-transition arrays,"Djamel Benredjem, Jean-Christophe Pain","This study investigates the ionization and excitation processes induced by electron impact between two configurations or superconfigurations. Rate coefficients are calculated for transition arrays or super-transition arrays rather than level-to-level transitions. Special attention is given to a series of oxygen-like ions relevant to inertial confinement fusion, specifically silicon, germanium, argon, and krypton. Calculations are performed over a wide temperature range, from 50 to 3000 eV. To facilitate the determination of rates at any temperature and ion charge, the computed rates are fitted with a two-dimensional Chebyshev polynomial expansion, yielding a set of coefficients for practical applications. Additionally, an extension of the Clenshaw algorithm to two dimensions, using the Chebyshev coefficients, is proposed to address numerical challenges and enhance computational efficiency.",2025-09-28,http://arxiv.org/pdf/2509.24042v1
