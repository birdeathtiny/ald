# íŒŒì¼ëª…: app.py (Live íƒì‚¬ AI ìµœì¢… ì™„ì„±ë³¸)

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
import os
import time
import sys
import requests
import io
import pdfplumber

try:
    from serpapi import GoogleSearch
    SERPAPI_ENABLED = True
except ImportError:
    SERPAPI_ENABLED = False

# ==============================================================================
# 0. ë°ì´í„° ë¶„ì„ ë° ì •ì œ í•¨ìˆ˜
# ==============================================================================
def analyze_and_clean_data(df):
    rename_map = {}
    for col in df.columns:
        col_lower = str(col).lower()
        if 'temp' in col_lower: rename_map[col] = 'temperature_c'
        elif 'press' in col_lower: rename_map[col] = 'pressure_torr'
        elif 'cycle' in col_lower: rename_map[col] = 'total_cycles'
        elif 'thick' in col_lower or 'rate' in col_lower or 'gpc' in col_lower: rename_map[col] = 'thickness_nm'
    
    df_std = df.rename(columns=rename_map)
    
    # ë² í…Œë‘ ìˆ˜ì‚¬ê´€ì˜ ëŠ¥ë ¥: 'ì˜¨ë„'ì™€ 'ë‘ê»˜'ë§Œ ìˆì–´ë„ ìœ íš¨ ë°ì´í„°ë¡œ ì¸ì •
    required_cols = ['temperature_c', 'thickness_nm']
    if all(col in df_std.columns for col in required_cols):
        # ìˆ«ì ë³€í™˜ì´ ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ ë‚¨ê¹€
        for col in required_cols:
            df_std[col] = pd.to_numeric(df_std[col], errors='coerce')
        df_std = df_std.dropna(subset=required_cols)
        
        if not df_std.empty:
            return df_std
    return None

# ==============================================================================
# 1. Live íƒì‚¬ AI ëª¨ë¸ ìƒì„±
# ==============================================================================
@st.cache_resource(show_spinner=False) # ìŠ¤í”¼ë„ˆë¥¼ ì§ì ‘ ì œì–´í•˜ê¸° ìœ„í•´ Falseë¡œ ì„¤ì •
def build_live_explorer_ai():
    is_web_mode = "streamlit" in " ".join(sys.argv)
    
    with st.spinner("Live íƒì‚¬ AIê°€ ì„ë¬´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."):
        if not is_web_mode: print("Live íƒì‚¬ AIê°€ ì„ë¬´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_valid_dfs = []

        # --- 1ë‹¨ê³„: ë¡œì»¬ íŒŒì¼ íƒì‚¬ ---
        if not is_web_mode: print("\n[1ë‹¨ê³„] ë¡œì»¬ íŒŒì¼ì„ ì •ë°€ ë¶„ì„í•©ë‹ˆë‹¤...")
        for filename in os.listdir('.'):
            if filename.lower().startswith(('app', 'requirement', '~$')): continue
            try:
                if filename.lower().endswith('.xlsx'):
                    df = pd.read_excel(filename)
                elif filename.lower().endswith(('.csv', '.cell')):
                    df = pd.read_csv(filename, sep=None, engine='python', on_bad_lines='skip', encoding='utf-8')
                else:
                    continue
                
                valid_df = analyze_and_clean_data(df)
                if valid_df is not None:
                    all_valid_dfs.append(valid_df)
                    if not is_web_mode: print(f"  âœ… '{filename}'ì—ì„œ ìœ íš¨ ë°ì´í„° {len(valid_df)}ê°œ í™•ë³´.")
            except Exception:
                pass

        # --- 2ë‹¨ê³„: ì¸í„°ë„· Live íƒì‚¬ ---
        if not is_web_mode: print("\n[2ë‹¨ê³„] ì¸í„°ë„· Live íƒì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        SERPAPI_API_KEY = "52b9b85163f1d3b8819e9aae64c928bf034b99d9e5be51b39374712e8d32318b"
        
        if SERPAPI_ENABLED and SERPAPI_API_KEY != "...":
            search_queries = [
                'atomic layer deposition experimental data filetype:csv',
                'ALD process parameters "growth rate" filetype:pdf'
            ]
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

            for query in search_queries:
                try:
                    if not is_web_mode: print(f"\n  -> íƒì‚¬ ì„ë¬´: '{query}'")
                    params = {"engine": "google", "q": query, "api_key": SERPAPI_API_KEY, "num": 5} # 5ê°œ ê²°ê³¼ë§Œ í™•ì¸
                    search = GoogleSearch(params)
                    results = search.get_dict().get('organic_results', [])
                    
                    for result in results:
                        url = result.get('link')
                        if not url: continue

                        if not is_web_mode: print(f"    -> '{url[:60]}...' íƒì‚¬ ì¤‘...")
                        
                        try:
                            response = requests.get(url, headers=headers, timeout=15)
                            response.raise_for_status()

                            if url.endswith('.csv'):
                                df = pd.read_csv(io.StringIO(response.text), on_bad_lines='skip')
                                valid_df = analyze_and_clean_data(df)
                                if valid_df is not None: all_valid_dfs.append(valid_df)
                            
                            elif url.endswith('.pdf'):
                                with pdfplumber.open(io.BytesIO(response.content)) as pdf:
                                    for page in pdf.pages:
                                        for table in page.extract_tables():
                                            df = pd.DataFrame(table[1:], columns=table[0])
                                            valid_df = analyze_and_clean_data(df)
                                            if valid_df is not None: all_valid_dfs.append(valid_df)
                        except Exception:
                            pass # ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
                except Exception as e:
                    if not is_web_mode: print(f"  ğŸ”¥ íƒì‚¬ ì„ë¬´ ì‹¤íŒ¨: {e}")
        else:
            if not is_web_mode: print("  âš ï¸ íƒì‚¬ í—ˆê°€ì¦(SerpApi í‚¤)ì´ ì—†ì–´ ì¸í„°ë„· íƒì‚¬ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # --- 3ë‹¨ê³„: ë³´ê³  ë° ìµœì¢… ë¶„ì„ ---
        if not all_valid_dfs:
            raise ValueError("íƒì‚¬ ì‹¤íŒ¨: ì–´ë– í•œ ìœ íš¨ ë°ì´í„°ë„ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì´ë‚˜ ë¡œì»¬ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        if not is_web_mode: print("\n[3ë‹¨ê³„] ëª¨ë“  íƒì‚¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤...")
        master_df = pd.concat(all_valid_dfs, ignore_index=True)
        
        # ëˆ„ë½ëœ ì •ë³´ ì¶”ë¦¬
        final_cols = ['temperature_c', 'pressure_torr', 'total_cycles', 'thickness_nm']
        for col in final_cols:
            if col not in master_df.columns: master_df[col] = np.nan
        
        master_df = master_df[final_cols].apply(pd.to_numeric, errors='coerce')
        for col in ['pressure_torr', 'total_cycles']:
            if master_df[col].isnull().any():
                master_df[col].fillna(master_df[col].median(), inplace=True)
        master_df.dropna(inplace=True)

        if len(master_df) < 5:
            raise ValueError(f"ë³´ê³ ì„œ ì‘ì„± ì‹¤íŒ¨: ìµœì¢… ìœ íš¨ ë°ì´í„°ê°€ {len(master_df)}ê°œë¿ì…ë‹ˆë‹¤. ë¶„ì„ì„ ì§„í–‰í•˜ê¸°ì— ì •ë³´ê°€ ë„ˆë¬´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        X = master_df[['temperature_c', 'pressure_torr', 'total_cycles']]
        y = master_df['thickness_nm']
        scaler = StandardScaler().fit(X); X_scaled = scaler.transform(X)
        model = XGBRegressor(random_state=42, n_estimators=100, max_depth=3).fit(X_scaled, y)
        
        if not is_web_mode: print(f"âœ… ì´ {len(master_df)}ê°œì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œ(AI ëª¨ë¸) ì‘ì„± ì™„ë£Œ!")
        return model, scaler, X.columns

# ==============================================================================
# 2. ìµœì  ì¡°ê±´ íƒìƒ‰ ë° 3. UI ì‹¤í–‰ ë¡œì§ (ì´ì „ê³¼ ë™ì¼)
# ==============================================================================
@st.cache_data
def find_optimal_conditions(_model, _scaler, _feature_names, target_thickness, num_samples=50000):
    ranges = {'temperature_c': (100, 400), 'pressure_torr': (0.01, 2.0), 'total_cycles': (50, 1000)}
    candidates = pd.DataFrame({key: np.random.uniform(low, high, num_samples) for key, (low, high) in ranges.items()})
    candidates_scaled = _scaler.transform(candidates[_feature_names])
    predictions = _model.predict(candidates_scaled)
    best_index = np.abs(predictions - target_thickness).argmin()
    best_conditions = candidates.iloc[best_index].round(2)
    predicted_thickness = predictions[best_index]
    return best_conditions, predicted_thickness

def run_web_app(model, scaler, feature_names):
    st.set_page_config(page_title="ALD ê³µì • ìµœì í™” ì‹œìŠ¤í…œ")
    st.title("ğŸ¯ AI ê¸°ë°˜ ìµœì  ê³µì • ë ˆì‹œí”¼ ì œì•ˆ ì‹œìŠ¤í…œ")
    st.sidebar.header("ğŸ† ëª©í‘œ ê²°ê³¼ê°’ ì…ë ¥")
    target_thick = st.sidebar.number_input("ëª©í‘œ ë°•ë§‰ ë‘ê»˜ (nm)", min_value=1.0, max_value=100.0, value=25.0, step=0.1)
    if st.sidebar.button("ğŸ¤– ìµœì  ì¡°ê±´ ì°¾ê¸°"):
        with st.spinner("AIê°€ ìµœì ì˜ ê³µì • ì¡°ê±´ì„ íƒìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
            best_conditions, predicted_thickness = find_optimal_conditions(model, scaler, feature_names, target_thick)
        st.subheader("ğŸ’¡ AIê°€ ì œì•ˆí•˜ëŠ” ìµœì  ê³µì • ë ˆì‹œí”¼")
        st.metric("ëª©í‘œ ëŒ€ë¹„ AI ì˜ˆì¸¡ ë‘ê»˜", f"{predicted_thickness:.2f} nm", f"{predicted_thickness - target_thick:.2f} nm ì˜¤ì°¨")
        st.table(pd.DataFrame(best_conditions).T)

def run_terminal_app(model, scaler, feature_names):
    print("\n--- ğŸ’» AI ìµœì  ê³µì • íƒìƒ‰ í„°ë¯¸ë„ ëª¨ë“œ (1íšŒ ì‹¤í–‰) ---")
    try:
        target_thick_str = input("\nëª©í‘œ ë°•ë§‰ ë‘ê»˜(nm)ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        target_thick = float(target_thick_str)
        best_conditions, predicted_thickness = find_optimal_conditions(model, scaler, feature_names, target_thick)
        print("\nğŸ’¡ AIê°€ ì œì•ˆí•˜ëŠ” ìµœì  ê³µì • ë ˆì‹œí”¼:")
        print(f"   - AI ì˜ˆì¸¡ ë‘ê»˜: {predicted_thickness:.2f} nm (ëª©í‘œ: {target_thick:.2f} nm)")
        print("--- ì œì•ˆ ì¡°ê±´ ---")
        print(pd.DataFrame(best_conditions).T.to_string())
    except (ValueError, KeyboardInterrupt):
        print("\nâš ï¸ ì˜ëª»ëœ ì…ë ¥ì´ê±°ë‚˜ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    finally:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    is_streamlit_run = "streamlit" in " ".join(sys.argv)
    model, scaler, feature_names = build_live_explorer_ai()
    if is_streamlit_run:
        run_web_app(model, scaler, feature_names)
    else:
        run_terminal_app(model, scaler, feature_names)