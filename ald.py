import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import re
import warnings

warnings.filterwarnings('ignore')

def load_and_clean_data(file_name):
    """CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³ , ìˆ«ì ì—´ì— í¬í•¨ëœ ë¬¸ìë¥¼ ì œê±°í•˜ëŠ” í´ë¦¬ë‹ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("="*50)
    print("### 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° í´ë¦¬ë‹ ###")
    print("="*50)
    
    try:
        df = pd.read_csv(file_name)
        print(f"âœ… '{file_name}' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"ğŸš¨ '{file_name}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œì™€ ê°™ì€ í´ë”ì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

    # ìˆ«ìì—¬ì•¼ í•˜ì§€ë§Œ ë¬¸ìê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ì—´ì˜ ëª©ë¡
    numeric_cols_to_clean = [
        'T (oC)', 'P (mTorr)', 'F (sccm)', 'Knudsen Number (Kn)', 
        'Sticking Coefficient (s)', 'Step Coverage (SC, %)', 'Depo Rate (nm/cycle)', 
        'C/H Impurity (at.%)', 'Particle Density (cm-3)'
    ]

    for col in numeric_cols_to_clean:
        if col in df.columns:
            df[col] = df[col].astype(str).str.extract(r'(\d+\.?\d*)').astype(float)
            
    print("âœ¨ ë°ì´í„° í´ë¦¬ë‹ ì™„ë£Œ!")
    return df

def train_model(df):
    """ì •ì œëœ ë°ì´í„°ë¥¼ ë°›ì•„ AI ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("\n" + "="*50)
    print("### 2ë‹¨ê³„: AI ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ###")
    print("="*50)
    
    try:
        input_columns = ['T (oC)', 'P (mTorr)', 'F (sccm)', 'T Level', 'Knudsen Number (Kn)', 'Sticking Coefficient (s)']
        output_columns = ['Step Coverage (SC, %)', 'Depo Rate (nm/cycle)', 'C/H Impurity (at.%)', 'Particle Density (cm-3)']
        categorical_features = ['T Level']

        # í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ëŠ” í–‰ ì œê±°
        df.dropna(subset=input_columns + output_columns, inplace=True)
        if len(df) < 10: # í•™ìŠµì— í•„ìš”í•œ ìµœì†Œ ë°ì´í„° ìˆ˜
            print("ğŸš¨ ìœ íš¨ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        X = df[input_columns]
        Y = df[output_columns]
        
        preprocessor = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough'
        )

        from sklearn.multioutput import MultiOutputRegressor
        model_pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('regressor', MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)))
        ])

        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

        print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        model_pipeline.fit(X_train, Y_train)
        print("âœ¨ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        predictions = model_pipeline.predict(X_test)
        print("\n--- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Mean Absolute Error) ---")
        for i, col_name in enumerate(output_columns):
            mae = mean_absolute_error(Y_test.iloc[:, i], predictions[:, i])
            print(f"{col_name} ì˜ˆì¸¡ ì˜¤ì°¨: {mae:.2f}")
        
        return model_pipeline, X # ìµœì í™” íƒìƒ‰ì„ ìœ„í•´ X ë°ì´í„°ë„ í•¨ê»˜ ë°˜í™˜

    except KeyError as e:
        print(f"ğŸš¨ KeyError: CSV íŒŒì¼ì— í•„ìš”í•œ ì—´({e})ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"ğŸš¨ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def find_optimal_conditions(model, original_X, optimization_target, num_simulations=10000):
    """í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ ê°€ìƒ ì‹¤í—˜ì„ í•˜ê³  ìµœì  ì¡°ê±´ì„ íƒìƒ‰í•©ë‹ˆë‹¤."""
    print("\n" + "="*50)
    print("### 3ë‹¨ê³„: ìµœì  ê³µì • ì¡°ê±´ íƒìƒ‰ (ì‹œë®¬ë ˆì´ì…˜) ###")
    print("="*50)

    # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ê°€ìƒ ë°ì´í„° ìƒì„±
    sim_data = {}
    for col in original_X.columns:
        if col in ['T Level']: # ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬
             sim_data[col] = np.random.choice(original_X[col].unique(), num_simulations)
        else: # ìˆ«ìí˜• ë°ì´í„° ì²˜ë¦¬
            min_val = original_X[col].min()
            max_val = original_X[col].max()
            sim_data[col] = np.random.uniform(min_val, max_val, num_simulations)
    
    simulation_df = pd.DataFrame(sim_data)

    print(f"{num_simulations}ê°œì˜ ê°€ìƒ ì¡°ê±´ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")
    # ê°€ìƒ ì¡°ê±´ë“¤ë¡œ ê²°ê³¼ ì˜ˆì¸¡
    sim_predictions = model.predict(simulation_df)

    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
    output_columns = ['Step Coverage (SC, %)', 'Depo Rate (nm/cycle)', 'C/H Impurity (at.%)', 'Particle Density (cm-3)']
    for i, col_name in enumerate(output_columns):
        simulation_df[f'predicted_{col_name}'] = sim_predictions[:, i]

    # ëª©í‘œ ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ì¡°ê±´ ì •ë ¬
    optimal_conditions = simulation_df.sort_values(by=f'predicted_{optimization_target}', ascending=False)
    
    print("\nâœ¨ AIê°€ ì˜ˆì¸¡í•œ ìµœì ì˜ ê³µì • ì¡°ê±´ âœ¨")
    print(f"(ëª©í‘œ: '{optimization_target}' ìµœëŒ€í™”)")
    print(optimal_conditions.head())

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    
    # 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° í´ë¦¬ë‹
    file_name = 'HCDS Data sample - HCDS Data Sample.csv'
    cleaned_df = load_and_clean_data(file_name)

    if cleaned_df is not None:
        # 2ë‹¨ê³„: AI ëª¨ë¸ í•™ìŠµ
        trained_model, X_data = train_model(cleaned_df)

        if trained_model is not None:
            # 3ë‹¨ê³„: ìµœì  ì¡°ê±´ íƒìƒ‰
            # ëª©í‘œ: 'Step Coverage (SC, %)'ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ì¡°ê±´ ì°¾ê¸°
            find_optimal_conditions(trained_model, X_data, optimization_target='Step Coverage (SC, %)')